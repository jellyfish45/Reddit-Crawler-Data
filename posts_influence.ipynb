{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c2bda71-c3b1-4019-9d1d-232c47b83285",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import praw \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "614a7236-8bb1-401e-99a8-a3138338c241",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=\"cXb7-vX9BLku6yVl6aKagg\",\n",
    "    client_secret=\"Pa1gMfMKKUqS_hyFllKzzY6d7__JPw\",\n",
    "    user_agent=\"jellyfish\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21450e1e-5ff2-4a8d-b505-c4251f937928",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>url</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>body</th>\n",
       "      <th>created</th>\n",
       "      <th>key_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5399</th>\n",
       "      <td>5400</td>\n",
       "      <td>to block the German autobahn</td>\n",
       "      <td>79603</td>\n",
       "      <td>zdc3sl</td>\n",
       "      <td>therewasanattempt</td>\n",
       "      <td>https://v.redd.it/zie3h567h54a1</td>\n",
       "      <td>4152</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.670259e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1614</th>\n",
       "      <td>1614</td>\n",
       "      <td>Gabby Petito Confirmed Dead by her Father on T...</td>\n",
       "      <td>75330</td>\n",
       "      <td>prift6</td>\n",
       "      <td>awfuleverything</td>\n",
       "      <td>https://i.redd.it/m3cc2ot6ijo71.png</td>\n",
       "      <td>11510</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.632092e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5884</th>\n",
       "      <td>5885</td>\n",
       "      <td>Print(\" suicidal thoughts\")</td>\n",
       "      <td>74839</td>\n",
       "      <td>e4golj</td>\n",
       "      <td>dankmemes</td>\n",
       "      <td>https://i.redd.it/4r4iepqfj0241.jpg</td>\n",
       "      <td>405</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.575202e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5376</th>\n",
       "      <td>5377</td>\n",
       "      <td>to make his death look like a suicide</td>\n",
       "      <td>61442</td>\n",
       "      <td>gd0bzc</td>\n",
       "      <td>therewasanattempt</td>\n",
       "      <td>https://i.redd.it/ujxthsw0umw41.jpg</td>\n",
       "      <td>832</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.588548e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5371</th>\n",
       "      <td>5372</td>\n",
       "      <td>to stop suicide</td>\n",
       "      <td>59371</td>\n",
       "      <td>iu8k8c</td>\n",
       "      <td>therewasanattempt</td>\n",
       "      <td>https://i.redd.it/2ez8ce09sln51.jpg</td>\n",
       "      <td>1212</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.600303e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5425</th>\n",
       "      <td>5426</td>\n",
       "      <td>To be edgy and poetic</td>\n",
       "      <td>56238</td>\n",
       "      <td>ceqsgz</td>\n",
       "      <td>therewasanattempt</td>\n",
       "      <td>https://i.redd.it/nnnp8d4oi1b31.jpg</td>\n",
       "      <td>497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.563446e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5378</th>\n",
       "      <td>5379</td>\n",
       "      <td>There was an attempt to \"shame\" someone who wa...</td>\n",
       "      <td>54025</td>\n",
       "      <td>freqp1</td>\n",
       "      <td>therewasanattempt</td>\n",
       "      <td>https://i.redd.it/4s7uadbduop41.jpg</td>\n",
       "      <td>627</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.585521e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5524</th>\n",
       "      <td>5525</td>\n",
       "      <td>To show China in a positive way</td>\n",
       "      <td>52816</td>\n",
       "      <td>fbu4hc</td>\n",
       "      <td>therewasanattempt</td>\n",
       "      <td>https://i.redd.it/67suqchmd2k41.jpg</td>\n",
       "      <td>547</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.583070e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5373</th>\n",
       "      <td>5374</td>\n",
       "      <td>..at reducing suicide rates..</td>\n",
       "      <td>46489</td>\n",
       "      <td>c371vx</td>\n",
       "      <td>therewasanattempt</td>\n",
       "      <td>https://i.imgur.com/eM27GT8.jpg</td>\n",
       "      <td>1352</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.561098e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6078</th>\n",
       "      <td>6079</td>\n",
       "      <td>West Taiwan really is a trainwreck.</td>\n",
       "      <td>45504</td>\n",
       "      <td>nnbcw3</td>\n",
       "      <td>dankmemes</td>\n",
       "      <td>https://i.redd.it/8lrwhca4dy171.jpg</td>\n",
       "      <td>885</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.622247e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5522</th>\n",
       "      <td>5523</td>\n",
       "      <td>To end it all</td>\n",
       "      <td>44298</td>\n",
       "      <td>ro90p2</td>\n",
       "      <td>therewasanattempt</td>\n",
       "      <td>https://v.redd.it/uqqtk0nmno781</td>\n",
       "      <td>428</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.640436e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5889</th>\n",
       "      <td>5890</td>\n",
       "      <td>suicide is badass!</td>\n",
       "      <td>44297</td>\n",
       "      <td>cuu7pt</td>\n",
       "      <td>dankmemes</td>\n",
       "      <td>https://i.redd.it/uilpub3isei31.jpg</td>\n",
       "      <td>141</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.566658e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6028</th>\n",
       "      <td>6029</td>\n",
       "      <td>My money is ready</td>\n",
       "      <td>43152</td>\n",
       "      <td>10vy9y4</td>\n",
       "      <td>dankmemes</td>\n",
       "      <td>https://i.redd.it/m3wsfvk08sga1.jpg</td>\n",
       "      <td>610</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.675764e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5411</th>\n",
       "      <td>5412</td>\n",
       "      <td>To walk on a glass bridge</td>\n",
       "      <td>42044</td>\n",
       "      <td>rba6s2</td>\n",
       "      <td>therewasanattempt</td>\n",
       "      <td>https://v.redd.it/8393q1g5w6481</td>\n",
       "      <td>919</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.638913e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5883</th>\n",
       "      <td>5884</td>\n",
       "      <td>Suicide</td>\n",
       "      <td>41737</td>\n",
       "      <td>dy3fy4</td>\n",
       "      <td>dankmemes</td>\n",
       "      <td>https://i.redd.it/dtk875ks9gz31.jpg</td>\n",
       "      <td>190</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.574085e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5419</th>\n",
       "      <td>5420</td>\n",
       "      <td>To commit a hate crime</td>\n",
       "      <td>39857</td>\n",
       "      <td>embek8</td>\n",
       "      <td>therewasanattempt</td>\n",
       "      <td>https://i.redd.it/qhoxrb115j931.jpg</td>\n",
       "      <td>795</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.578585e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6040</th>\n",
       "      <td>6041</td>\n",
       "      <td>A simple mistake</td>\n",
       "      <td>39259</td>\n",
       "      <td>12y9ai3</td>\n",
       "      <td>dankmemes</td>\n",
       "      <td>https://i.imgur.com/LljfW6B.gifv</td>\n",
       "      <td>324</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.682400e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5433</th>\n",
       "      <td>5434</td>\n",
       "      <td>To cheer up a commute</td>\n",
       "      <td>38236</td>\n",
       "      <td>ctxc0z</td>\n",
       "      <td>therewasanattempt</td>\n",
       "      <td>https://i.redd.it/ait5mctl30i31.jpg</td>\n",
       "      <td>329</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.566480e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5377</th>\n",
       "      <td>5378</td>\n",
       "      <td>There was an attempt to spread suicide awareness.</td>\n",
       "      <td>37438</td>\n",
       "      <td>fqnab6</td>\n",
       "      <td>therewasanattempt</td>\n",
       "      <td>https://i.redd.it/ape02kwewfp41.jpg</td>\n",
       "      <td>237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.585413e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6053</th>\n",
       "      <td>6054</td>\n",
       "      <td>I know the answer</td>\n",
       "      <td>37131</td>\n",
       "      <td>r5knt5</td>\n",
       "      <td>dankmemes</td>\n",
       "      <td>https://i.redd.it/a6zknoitpp281.jpg</td>\n",
       "      <td>2902</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.638269e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5370</th>\n",
       "      <td>5371</td>\n",
       "      <td>To Prevent Suicide</td>\n",
       "      <td>36846</td>\n",
       "      <td>16psiru</td>\n",
       "      <td>therewasanattempt</td>\n",
       "      <td>https://v.redd.it/xr7onn9u0xpb1</td>\n",
       "      <td>655</td>\n",
       "      <td>From @whitney.pyles on tiktok</td>\n",
       "      <td>1.695436e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5397</th>\n",
       "      <td>5398</td>\n",
       "      <td>To beat woman in front of public</td>\n",
       "      <td>34915</td>\n",
       "      <td>14h1nlt</td>\n",
       "      <td>therewasanattempt</td>\n",
       "      <td>https://v.redd.it/4tjxmg0ees7b1</td>\n",
       "      <td>1441</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.687534e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5961</th>\n",
       "      <td>5962</td>\n",
       "      <td>Etika has passed away.</td>\n",
       "      <td>34860</td>\n",
       "      <td>c5c77j</td>\n",
       "      <td>dankmemes</td>\n",
       "      <td>https://www.reddit.com/r/dankmemes/comments/c5...</td>\n",
       "      <td>1929</td>\n",
       "      <td>[NYPD has just confirmed](https://twitter.com/...</td>\n",
       "      <td>1.561484e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6102</th>\n",
       "      <td>6103</td>\n",
       "      <td>Waifu pillow's in the closet</td>\n",
       "      <td>34718</td>\n",
       "      <td>sdvqsb</td>\n",
       "      <td>dankmemes</td>\n",
       "      <td>https://i.redd.it/yxqlu9izn7e81.gif</td>\n",
       "      <td>355</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.643281e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6057</th>\n",
       "      <td>6058</td>\n",
       "      <td>uga buga UGA BUGA</td>\n",
       "      <td>34400</td>\n",
       "      <td>paxhie</td>\n",
       "      <td>dankmemes</td>\n",
       "      <td>https://i.redd.it/dw0vdnlxpdj71.jpg</td>\n",
       "      <td>3315</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.629843e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5527</th>\n",
       "      <td>5528</td>\n",
       "      <td>to make an inspirational quote</td>\n",
       "      <td>33054</td>\n",
       "      <td>ahwjz2</td>\n",
       "      <td>therewasanattempt</td>\n",
       "      <td>https://i.redd.it/96r47vk78kb21.jpg</td>\n",
       "      <td>206</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.547982e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5887</th>\n",
       "      <td>5888</td>\n",
       "      <td>No suicide for me!</td>\n",
       "      <td>31619</td>\n",
       "      <td>agz41x</td>\n",
       "      <td>dankmemes</td>\n",
       "      <td>https://i.redd.it/8lb0qpmc90b21.jpg</td>\n",
       "      <td>429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.547740e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6036</th>\n",
       "      <td>6037</td>\n",
       "      <td>It's just a meme !</td>\n",
       "      <td>31181</td>\n",
       "      <td>y6xxu5</td>\n",
       "      <td>dankmemes</td>\n",
       "      <td>https://i.redd.it/qh8m2hi03iu91.jpg</td>\n",
       "      <td>985</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.666071e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5528</th>\n",
       "      <td>5529</td>\n",
       "      <td>to be in the Christmas spirit.</td>\n",
       "      <td>30541</td>\n",
       "      <td>a6fuiu</td>\n",
       "      <td>therewasanattempt</td>\n",
       "      <td>https://i.redd.it/pvkgoee3jg421.jpg</td>\n",
       "      <td>141</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.544886e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6008</th>\n",
       "      <td>6009</td>\n",
       "      <td>Why can’t we have that here :(</td>\n",
       "      <td>30141</td>\n",
       "      <td>14w7ccl</td>\n",
       "      <td>dankmemes</td>\n",
       "      <td>https://i.redd.it/hqpd09lej7bb1.jpg</td>\n",
       "      <td>893</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.689025e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5915</th>\n",
       "      <td>5916</td>\n",
       "      <td>original meme from u/i_am_feeling_suicide, i c...</td>\n",
       "      <td>29962</td>\n",
       "      <td>bpahn5</td>\n",
       "      <td>dankmemes</td>\n",
       "      <td>https://i.redd.it/jks25k84njy21.png</td>\n",
       "      <td>246</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.558000e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5406</th>\n",
       "      <td>5407</td>\n",
       "      <td>To jump in a frozen lake</td>\n",
       "      <td>29640</td>\n",
       "      <td>wriy5i</td>\n",
       "      <td>therewasanattempt</td>\n",
       "      <td>https://v.redd.it/0b8j2mjezgi91</td>\n",
       "      <td>964</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.660828e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5888</th>\n",
       "      <td>5889</td>\n",
       "      <td>The real suicide squad</td>\n",
       "      <td>29472</td>\n",
       "      <td>r8okz7</td>\n",
       "      <td>dankmemes</td>\n",
       "      <td>https://i.redd.it/lu6pvaxdqi381.gif</td>\n",
       "      <td>133</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.638620e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6087</th>\n",
       "      <td>6088</td>\n",
       "      <td>Let’s get TF out of here</td>\n",
       "      <td>29042</td>\n",
       "      <td>qn5kiw</td>\n",
       "      <td>dankmemes</td>\n",
       "      <td>https://i.redd.it/ol86nhxu8qx71.gif</td>\n",
       "      <td>498</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.636096e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5375</th>\n",
       "      <td>5376</td>\n",
       "      <td>to prevent suicide</td>\n",
       "      <td>28920</td>\n",
       "      <td>9s2e44</td>\n",
       "      <td>therewasanattempt</td>\n",
       "      <td>https://i.redd.it/uxb7wnvqrwu11.jpg</td>\n",
       "      <td>291</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.540725e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5525</th>\n",
       "      <td>5526</td>\n",
       "      <td>to punish people</td>\n",
       "      <td>28854</td>\n",
       "      <td>by5ypf</td>\n",
       "      <td>therewasanattempt</td>\n",
       "      <td>https://i.redd.it/tobxmk0ai3331.jpg</td>\n",
       "      <td>372</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.559983e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5896</th>\n",
       "      <td>5897</td>\n",
       "      <td>It’s not suicide, it’s just murder</td>\n",
       "      <td>28321</td>\n",
       "      <td>ko46s2</td>\n",
       "      <td>dankmemes</td>\n",
       "      <td>https://i.redd.it/zwicm5bk4n861.jpg</td>\n",
       "      <td>155</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.609472e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6098</th>\n",
       "      <td>6099</td>\n",
       "      <td>I just don't want afghani people to lose their...</td>\n",
       "      <td>27713</td>\n",
       "      <td>p60ixh</td>\n",
       "      <td>dankmemes</td>\n",
       "      <td>https://i.redd.it/4zmbfnin2wh71.gif</td>\n",
       "      <td>494</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.629193e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5523</th>\n",
       "      <td>5524</td>\n",
       "      <td>To breakout of jail disguised as his 19 y/o da...</td>\n",
       "      <td>27631</td>\n",
       "      <td>qmpfws</td>\n",
       "      <td>therewasanattempt</td>\n",
       "      <td>https://v.redd.it/b8j35l1q2mx71</td>\n",
       "      <td>411</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.636046e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5904</th>\n",
       "      <td>5905</td>\n",
       "      <td>Suicide is badass</td>\n",
       "      <td>26685</td>\n",
       "      <td>f9ayxk</td>\n",
       "      <td>dankmemes</td>\n",
       "      <td>https://i.redd.it/1yg1lbnhx2j41.jpg</td>\n",
       "      <td>55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.582641e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6061</th>\n",
       "      <td>6062</td>\n",
       "      <td>apparently \"nt go next\" wasn't a good answer</td>\n",
       "      <td>25836</td>\n",
       "      <td>rsxeh4</td>\n",
       "      <td>dankmemes</td>\n",
       "      <td>https://i.redd.it/bhelwdvxlw881.png</td>\n",
       "      <td>1294</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.640968e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5898</th>\n",
       "      <td>5899</td>\n",
       "      <td>Suicide is badass</td>\n",
       "      <td>25759</td>\n",
       "      <td>el9vsw</td>\n",
       "      <td>dankmemes</td>\n",
       "      <td>https://i.redd.it/91ze9yr34c941.jpg</td>\n",
       "      <td>69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.578393e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5890</th>\n",
       "      <td>5891</td>\n",
       "      <td>Suicide is badass</td>\n",
       "      <td>24726</td>\n",
       "      <td>naako3</td>\n",
       "      <td>dankmemes</td>\n",
       "      <td>https://i.redd.it/3ahgzzdbqky61.jpg</td>\n",
       "      <td>80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.620775e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5403</th>\n",
       "      <td>5404</td>\n",
       "      <td>To intimidate</td>\n",
       "      <td>24482</td>\n",
       "      <td>11wgz4r</td>\n",
       "      <td>therewasanattempt</td>\n",
       "      <td>https://v.redd.it/l21zj4m0wvoa1</td>\n",
       "      <td>530</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.679314e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5430</th>\n",
       "      <td>5431</td>\n",
       "      <td>To force the cats to bathe</td>\n",
       "      <td>23992</td>\n",
       "      <td>7yls7h</td>\n",
       "      <td>therewasanattempt</td>\n",
       "      <td>https://i.imgur.com/6niKt6M.gif</td>\n",
       "      <td>711</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.519036e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5374</th>\n",
       "      <td>5375</td>\n",
       "      <td>To prevent suicide</td>\n",
       "      <td>23392</td>\n",
       "      <td>7athy4</td>\n",
       "      <td>therewasanattempt</td>\n",
       "      <td>https://i.redd.it/azj6811b31wz.jpg</td>\n",
       "      <td>397</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.509832e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5962</th>\n",
       "      <td>5963</td>\n",
       "      <td>maybe another time</td>\n",
       "      <td>21505</td>\n",
       "      <td>15oiuq4</td>\n",
       "      <td>dankmemes</td>\n",
       "      <td>https://i.redd.it/59027zskdjhb1.jpg</td>\n",
       "      <td>398</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.691784e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5925</th>\n",
       "      <td>5926</td>\n",
       "      <td>Every single time a man is held captive he's s...</td>\n",
       "      <td>20410</td>\n",
       "      <td>lku63r</td>\n",
       "      <td>dankmemes</td>\n",
       "      <td>https://i.redd.it/bz976x339rh61.gif</td>\n",
       "      <td>66</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.613444e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5905</th>\n",
       "      <td>5906</td>\n",
       "      <td>Suicide is sometimes the answer</td>\n",
       "      <td>20257</td>\n",
       "      <td>cndufs</td>\n",
       "      <td>dankmemes</td>\n",
       "      <td>https://i.redd.it/uq1bwu7j94f31.jpg</td>\n",
       "      <td>101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.565223e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5929</th>\n",
       "      <td>5930</td>\n",
       "      <td>I’d actually do it for free suicide</td>\n",
       "      <td>19576</td>\n",
       "      <td>ax67mf</td>\n",
       "      <td>dankmemes</td>\n",
       "      <td>https://i.redd.it/ucukd8lna3k21.jpg</td>\n",
       "      <td>41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.551700e+09</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                              title  score  \\\n",
       "5399        5400                       to block the German autobahn  79603   \n",
       "1614        1614  Gabby Petito Confirmed Dead by her Father on T...  75330   \n",
       "5884        5885                        Print(\" suicidal thoughts\")  74839   \n",
       "5376        5377              to make his death look like a suicide  61442   \n",
       "5371        5372                                    to stop suicide  59371   \n",
       "5425        5426                              To be edgy and poetic  56238   \n",
       "5378        5379  There was an attempt to \"shame\" someone who wa...  54025   \n",
       "5524        5525                    To show China in a positive way  52816   \n",
       "5373        5374                      ..at reducing suicide rates..  46489   \n",
       "6078        6079                West Taiwan really is a trainwreck.  45504   \n",
       "5522        5523                                      To end it all  44298   \n",
       "5889        5890                                 suicide is badass!  44297   \n",
       "6028        6029                                  My money is ready  43152   \n",
       "5411        5412                          To walk on a glass bridge  42044   \n",
       "5883        5884                                            Suicide  41737   \n",
       "5419        5420                             To commit a hate crime  39857   \n",
       "6040        6041                                   A simple mistake  39259   \n",
       "5433        5434                              To cheer up a commute  38236   \n",
       "5377        5378  There was an attempt to spread suicide awareness.  37438   \n",
       "6053        6054                                  I know the answer  37131   \n",
       "5370        5371                                 To Prevent Suicide  36846   \n",
       "5397        5398                   To beat woman in front of public  34915   \n",
       "5961        5962                             Etika has passed away.  34860   \n",
       "6102        6103                       Waifu pillow's in the closet  34718   \n",
       "6057        6058                                  uga buga UGA BUGA  34400   \n",
       "5527        5528                     to make an inspirational quote  33054   \n",
       "5887        5888                                 No suicide for me!  31619   \n",
       "6036        6037                                 It's just a meme !  31181   \n",
       "5528        5529                     to be in the Christmas spirit.  30541   \n",
       "6008        6009                     Why can’t we have that here :(  30141   \n",
       "5915        5916  original meme from u/i_am_feeling_suicide, i c...  29962   \n",
       "5406        5407                           To jump in a frozen lake  29640   \n",
       "5888        5889                             The real suicide squad  29472   \n",
       "6087        6088                           Let’s get TF out of here  29042   \n",
       "5375        5376                                 to prevent suicide  28920   \n",
       "5525        5526                                   to punish people  28854   \n",
       "5896        5897                 It’s not suicide, it’s just murder  28321   \n",
       "6098        6099  I just don't want afghani people to lose their...  27713   \n",
       "5523        5524  To breakout of jail disguised as his 19 y/o da...  27631   \n",
       "5904        5905                                  Suicide is badass  26685   \n",
       "6061        6062       apparently \"nt go next\" wasn't a good answer  25836   \n",
       "5898        5899                                  Suicide is badass  25759   \n",
       "5890        5891                                  Suicide is badass  24726   \n",
       "5403        5404                                      To intimidate  24482   \n",
       "5430        5431                         To force the cats to bathe  23992   \n",
       "5374        5375                                 To prevent suicide  23392   \n",
       "5962        5963                                 maybe another time  21505   \n",
       "5925        5926  Every single time a man is held captive he's s...  20410   \n",
       "5905        5906                    Suicide is sometimes the answer  20257   \n",
       "5929        5930                I’d actually do it for free suicide  19576   \n",
       "\n",
       "           id          subreddit  \\\n",
       "5399   zdc3sl  therewasanattempt   \n",
       "1614   prift6    awfuleverything   \n",
       "5884   e4golj          dankmemes   \n",
       "5376   gd0bzc  therewasanattempt   \n",
       "5371   iu8k8c  therewasanattempt   \n",
       "5425   ceqsgz  therewasanattempt   \n",
       "5378   freqp1  therewasanattempt   \n",
       "5524   fbu4hc  therewasanattempt   \n",
       "5373   c371vx  therewasanattempt   \n",
       "6078   nnbcw3          dankmemes   \n",
       "5522   ro90p2  therewasanattempt   \n",
       "5889   cuu7pt          dankmemes   \n",
       "6028  10vy9y4          dankmemes   \n",
       "5411   rba6s2  therewasanattempt   \n",
       "5883   dy3fy4          dankmemes   \n",
       "5419   embek8  therewasanattempt   \n",
       "6040  12y9ai3          dankmemes   \n",
       "5433   ctxc0z  therewasanattempt   \n",
       "5377   fqnab6  therewasanattempt   \n",
       "6053   r5knt5          dankmemes   \n",
       "5370  16psiru  therewasanattempt   \n",
       "5397  14h1nlt  therewasanattempt   \n",
       "5961   c5c77j          dankmemes   \n",
       "6102   sdvqsb          dankmemes   \n",
       "6057   paxhie          dankmemes   \n",
       "5527   ahwjz2  therewasanattempt   \n",
       "5887   agz41x          dankmemes   \n",
       "6036   y6xxu5          dankmemes   \n",
       "5528   a6fuiu  therewasanattempt   \n",
       "6008  14w7ccl          dankmemes   \n",
       "5915   bpahn5          dankmemes   \n",
       "5406   wriy5i  therewasanattempt   \n",
       "5888   r8okz7          dankmemes   \n",
       "6087   qn5kiw          dankmemes   \n",
       "5375   9s2e44  therewasanattempt   \n",
       "5525   by5ypf  therewasanattempt   \n",
       "5896   ko46s2          dankmemes   \n",
       "6098   p60ixh          dankmemes   \n",
       "5523   qmpfws  therewasanattempt   \n",
       "5904   f9ayxk          dankmemes   \n",
       "6061   rsxeh4          dankmemes   \n",
       "5898   el9vsw          dankmemes   \n",
       "5890   naako3          dankmemes   \n",
       "5403  11wgz4r  therewasanattempt   \n",
       "5430   7yls7h  therewasanattempt   \n",
       "5374   7athy4  therewasanattempt   \n",
       "5962  15oiuq4          dankmemes   \n",
       "5925   lku63r          dankmemes   \n",
       "5905   cndufs          dankmemes   \n",
       "5929   ax67mf          dankmemes   \n",
       "\n",
       "                                                    url  num_comments  \\\n",
       "5399                    https://v.redd.it/zie3h567h54a1          4152   \n",
       "1614                https://i.redd.it/m3cc2ot6ijo71.png         11510   \n",
       "5884                https://i.redd.it/4r4iepqfj0241.jpg           405   \n",
       "5376                https://i.redd.it/ujxthsw0umw41.jpg           832   \n",
       "5371                https://i.redd.it/2ez8ce09sln51.jpg          1212   \n",
       "5425                https://i.redd.it/nnnp8d4oi1b31.jpg           497   \n",
       "5378                https://i.redd.it/4s7uadbduop41.jpg           627   \n",
       "5524                https://i.redd.it/67suqchmd2k41.jpg           547   \n",
       "5373                    https://i.imgur.com/eM27GT8.jpg          1352   \n",
       "6078                https://i.redd.it/8lrwhca4dy171.jpg           885   \n",
       "5522                    https://v.redd.it/uqqtk0nmno781           428   \n",
       "5889                https://i.redd.it/uilpub3isei31.jpg           141   \n",
       "6028                https://i.redd.it/m3wsfvk08sga1.jpg           610   \n",
       "5411                    https://v.redd.it/8393q1g5w6481           919   \n",
       "5883                https://i.redd.it/dtk875ks9gz31.jpg           190   \n",
       "5419                https://i.redd.it/qhoxrb115j931.jpg           795   \n",
       "6040                   https://i.imgur.com/LljfW6B.gifv           324   \n",
       "5433                https://i.redd.it/ait5mctl30i31.jpg           329   \n",
       "5377                https://i.redd.it/ape02kwewfp41.jpg           237   \n",
       "6053                https://i.redd.it/a6zknoitpp281.jpg          2902   \n",
       "5370                    https://v.redd.it/xr7onn9u0xpb1           655   \n",
       "5397                    https://v.redd.it/4tjxmg0ees7b1          1441   \n",
       "5961  https://www.reddit.com/r/dankmemes/comments/c5...          1929   \n",
       "6102                https://i.redd.it/yxqlu9izn7e81.gif           355   \n",
       "6057                https://i.redd.it/dw0vdnlxpdj71.jpg          3315   \n",
       "5527                https://i.redd.it/96r47vk78kb21.jpg           206   \n",
       "5887                https://i.redd.it/8lb0qpmc90b21.jpg           429   \n",
       "6036                https://i.redd.it/qh8m2hi03iu91.jpg           985   \n",
       "5528                https://i.redd.it/pvkgoee3jg421.jpg           141   \n",
       "6008                https://i.redd.it/hqpd09lej7bb1.jpg           893   \n",
       "5915                https://i.redd.it/jks25k84njy21.png           246   \n",
       "5406                    https://v.redd.it/0b8j2mjezgi91           964   \n",
       "5888                https://i.redd.it/lu6pvaxdqi381.gif           133   \n",
       "6087                https://i.redd.it/ol86nhxu8qx71.gif           498   \n",
       "5375                https://i.redd.it/uxb7wnvqrwu11.jpg           291   \n",
       "5525                https://i.redd.it/tobxmk0ai3331.jpg           372   \n",
       "5896                https://i.redd.it/zwicm5bk4n861.jpg           155   \n",
       "6098                https://i.redd.it/4zmbfnin2wh71.gif           494   \n",
       "5523                    https://v.redd.it/b8j35l1q2mx71           411   \n",
       "5904                https://i.redd.it/1yg1lbnhx2j41.jpg            55   \n",
       "6061                https://i.redd.it/bhelwdvxlw881.png          1294   \n",
       "5898                https://i.redd.it/91ze9yr34c941.jpg            69   \n",
       "5890                https://i.redd.it/3ahgzzdbqky61.jpg            80   \n",
       "5403                    https://v.redd.it/l21zj4m0wvoa1           530   \n",
       "5430                    https://i.imgur.com/6niKt6M.gif           711   \n",
       "5374                 https://i.redd.it/azj6811b31wz.jpg           397   \n",
       "5962                https://i.redd.it/59027zskdjhb1.jpg           398   \n",
       "5925                https://i.redd.it/bz976x339rh61.gif            66   \n",
       "5905                https://i.redd.it/uq1bwu7j94f31.jpg           101   \n",
       "5929                https://i.redd.it/ucukd8lna3k21.jpg            41   \n",
       "\n",
       "                                                   body       created  \\\n",
       "5399                                                NaN  1.670259e+09   \n",
       "1614                                                NaN  1.632092e+09   \n",
       "5884                                                NaN  1.575202e+09   \n",
       "5376                                                NaN  1.588548e+09   \n",
       "5371                                                NaN  1.600303e+09   \n",
       "5425                                                NaN  1.563446e+09   \n",
       "5378                                                NaN  1.585521e+09   \n",
       "5524                                                NaN  1.583070e+09   \n",
       "5373                                                NaN  1.561098e+09   \n",
       "6078                                                NaN  1.622247e+09   \n",
       "5522                                                NaN  1.640436e+09   \n",
       "5889                                                NaN  1.566658e+09   \n",
       "6028                                                NaN  1.675764e+09   \n",
       "5411                                                NaN  1.638913e+09   \n",
       "5883                                                NaN  1.574085e+09   \n",
       "5419                                                NaN  1.578585e+09   \n",
       "6040                                                NaN  1.682400e+09   \n",
       "5433                                                NaN  1.566480e+09   \n",
       "5377                                                NaN  1.585413e+09   \n",
       "6053                                                NaN  1.638269e+09   \n",
       "5370                      From @whitney.pyles on tiktok  1.695436e+09   \n",
       "5397                                                NaN  1.687534e+09   \n",
       "5961  [NYPD has just confirmed](https://twitter.com/...  1.561484e+09   \n",
       "6102                                                NaN  1.643281e+09   \n",
       "6057                                                NaN  1.629843e+09   \n",
       "5527                                                NaN  1.547982e+09   \n",
       "5887                                                NaN  1.547740e+09   \n",
       "6036                                                NaN  1.666071e+09   \n",
       "5528                                                NaN  1.544886e+09   \n",
       "6008                                                NaN  1.689025e+09   \n",
       "5915                                                NaN  1.558000e+09   \n",
       "5406                                                NaN  1.660828e+09   \n",
       "5888                                                NaN  1.638620e+09   \n",
       "6087                                                NaN  1.636096e+09   \n",
       "5375                                                NaN  1.540725e+09   \n",
       "5525                                                NaN  1.559983e+09   \n",
       "5896                                                NaN  1.609472e+09   \n",
       "6098                                                NaN  1.629193e+09   \n",
       "5523                                                NaN  1.636046e+09   \n",
       "5904                                                NaN  1.582641e+09   \n",
       "6061                                                NaN  1.640968e+09   \n",
       "5898                                                NaN  1.578393e+09   \n",
       "5890                                                NaN  1.620775e+09   \n",
       "5403                                                NaN  1.679314e+09   \n",
       "5430                                                NaN  1.519036e+09   \n",
       "5374                                                NaN  1.509832e+09   \n",
       "5962                                                NaN  1.691784e+09   \n",
       "5925                                                NaN  1.613444e+09   \n",
       "5905                                                NaN  1.565223e+09   \n",
       "5929                                                NaN  1.551700e+09   \n",
       "\n",
       "     key_words  \n",
       "5399   suicide  \n",
       "1614   suicide  \n",
       "5884   suicide  \n",
       "5376   suicide  \n",
       "5371   suicide  \n",
       "5425   suicide  \n",
       "5378   suicide  \n",
       "5524   suicide  \n",
       "5373   suicide  \n",
       "6078   suicide  \n",
       "5522   suicide  \n",
       "5889   suicide  \n",
       "6028   suicide  \n",
       "5411   suicide  \n",
       "5883   suicide  \n",
       "5419   suicide  \n",
       "6040   suicide  \n",
       "5433   suicide  \n",
       "5377   suicide  \n",
       "6053   suicide  \n",
       "5370   suicide  \n",
       "5397   suicide  \n",
       "5961   suicide  \n",
       "6102   suicide  \n",
       "6057   suicide  \n",
       "5527   suicide  \n",
       "5887   suicide  \n",
       "6036   suicide  \n",
       "5528   suicide  \n",
       "6008   suicide  \n",
       "5915   suicide  \n",
       "5406   suicide  \n",
       "5888   suicide  \n",
       "6087   suicide  \n",
       "5375   suicide  \n",
       "5525   suicide  \n",
       "5896   suicide  \n",
       "6098   suicide  \n",
       "5523   suicide  \n",
       "5904   suicide  \n",
       "6061   suicide  \n",
       "5898   suicide  \n",
       "5890   suicide  \n",
       "5403   suicide  \n",
       "5430   suicide  \n",
       "5374   suicide  \n",
       "5962   suicide  \n",
       "5925   suicide  \n",
       "5905   suicide  \n",
       "5929   suicide  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts=pd.read_csv('suicide.csv')\n",
    "posts=posts.loc[(posts.key_words=='suicide') ]    #Only analyze 'suicide', narrow the range.\n",
    "posts=posts.sort_values(by='score', ascending=False)  #Score means upvoting numbers\n",
    "posts_sub=posts.loc[posts.subreddit=='SuicideWatch'].sort_values(by='score', ascending=False)\n",
    "posts_sub\n",
    "posts.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29ebb568-62ce-4d92-b9e7-fc2984790432",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/panjiayi/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk                                # Python library for NLP\n",
    "from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\n",
    "import matplotlib.pyplot as plt            # library for visualization\n",
    "import random \n",
    "import numpy as np\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cb4e9ca-27e1-4f5c-89af-7c016e667dd4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35622daf-0d14-4e88-8f47-05b5f93765cc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg \n",
    "test_x = test_pos + test_neg\n",
    "\n",
    "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5db2c5da-5abb-453f-9a0b-a02f4f0dd864",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f4ad96a-c065-420c-bf11-101099e317f3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    tweet2 = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    tweet2 = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet2)\n",
    "    tweet2 = re.sub(r'#', '', tweet2)\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet2)\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens: # Go through every word in your tokens list\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "            word not in string.punctuation):  # remove punctuation\n",
    "            tweets_clean.append(word)\n",
    "    stemmer = PorterStemmer() \n",
    "    # Create an empty list to store the stems\n",
    "    tweets_stem = [] \n",
    "    for word in tweets_clean:\n",
    "        stem_word = stemmer.stem(word)  # stemming word\n",
    "        tweets_stem.append(stem_word)  # append to the list\n",
    "    return tweets_stem\n",
    "\n",
    "def build_freqs(tweets, ys):\n",
    "    \"\"\"Build frequencies.\n",
    "    Input:\n",
    "        tweets: a list of tweets\n",
    "        ys: an m x 1 array with the sentiment label of each tweet\n",
    "            (either 0 or 1)\n",
    "    Output:\n",
    "        freqs: a dictionary mapping each (word, sentiment) pair to its\n",
    "        frequency\n",
    "    \"\"\"\n",
    "    # Convert np array to list since zip needs an iterable.\n",
    "    # The squeeze is necessary or the list ends up with one element.\n",
    "    # Also note that this is just a NOP if ys is already a list.\n",
    "    yslist = np.squeeze(ys).tolist()\n",
    "\n",
    "    # Start with an empty dictionary and populate it by looping over all tweets\n",
    "    # and over all processed words in each tweet.\n",
    "    freqs = {}\n",
    "    for y, tweet in zip(yslist, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, y)\n",
    "            if pair in freqs:\n",
    "                freqs[pair] += 1\n",
    "            else:\n",
    "                freqs[pair] = 1    \n",
    "    return freqs\n",
    "\n",
    "freqs = build_freqs(train_x, train_y)  #Build Frequancy\n",
    "\n",
    "def sigmoid(z): \n",
    "    '''\n",
    "    Input:\n",
    "        z: is the input (can be a scalar or an array)\n",
    "    Output:\n",
    "        h: the sigmoid of z\n",
    "    '''\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # calculate the sigmoid of z\n",
    "    h = 1/(1+np.exp(-z))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return h\n",
    "\n",
    "def gradientDescent(x, y, theta, alpha, num_iters):\n",
    "    '''\n",
    "    Input:\n",
    "        x: matrix of features which is (m,n+1)\n",
    "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
    "        theta: weight vector of dimension (n+1,1)\n",
    "        alpha: learning rate\n",
    "        num_iters: number of iterations you want to train your model for\n",
    "    Output:\n",
    "        J: the final cost\n",
    "        theta: your final weight vector\n",
    "    Hint: you might want to print the cost to make sure that it is going down.\n",
    "    '''\n",
    "    ### START CODE HERE ###\n",
    "    # get 'm', the number of rows in matrix x\n",
    "    m = x.shape[0]\n",
    "    for i in range(0, num_iters):\n",
    "        # get z, the dot product of x and theta\n",
    "        z = x@theta\n",
    "        # get the sigmoid of z\n",
    "        h = sigmoid(z)   #python自变量是标量还是向量无影响\n",
    "        # calculate the cost function\n",
    "        J = (-1/m)*(np.transpose(y)@np.log(h)+np.transpose((1-y))@np.log(1-h))  #向量运算要用@\n",
    "        # update the weights theta\n",
    "        theta = theta-alpha/m*(np.transpose(x)@(h-y))\n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "    J = float(J)\n",
    "    return J,theta\n",
    "\n",
    "\n",
    "def extract_features(tweet, freqs, process_tweet=process_tweet):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a list of words for one tweet\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "    Output: \n",
    "        x: a feature vector of dimension (1,3)\n",
    "    '''\n",
    "    # process_tweet tokenizes, stems, and removes stopwords\n",
    "    word_l = process_tweet(tweet)\n",
    "    \n",
    "    # 3 elements for [bias, positive, negative] counts\n",
    "    x = np.zeros(3) \n",
    "    \n",
    "    # bias term is set to 1\n",
    "    x[0] = 1 \n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # loop through each word in the list of words\n",
    "    for word in word_l:\n",
    "        if (word, 1) in freqs:\n",
    "            # increment the word count for the positive label 1\n",
    "            x[1] += freqs[(word,1)]\n",
    "        if (word, 0) in freqs:\n",
    "            # increment the word count for the negative label 0\n",
    "            x[2] += freqs[(word,0)]\n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    x = x[np.newaxis, :]  # adding batch dimension for further processing\n",
    "    assert(x.shape == (1, 3))\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81f01392-fdc6-40ea-916a-b0b0c03d42d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ss/sv50jmf54z5fg7h09y4ws6gc0000gn/T/ipykernel_32583/2023382314.py:93: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  J = float(J)\n"
     ]
    }
   ],
   "source": [
    "X = np.zeros((len(train_x), 3))\n",
    "for i in range(len(train_x)):\n",
    "    X[i, :]= extract_features(train_x[i], freqs)\n",
    "\n",
    "# training labels corresponding to X\n",
    "Y = train_y\n",
    "\n",
    "# Apply gradient descent\n",
    "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecc7dfdd-0be0-4355-ab99-38c19bb7f00b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_tweet(tweet, freqs, theta):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a string\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "        theta: (3,1) vector of weights\n",
    "    Output: \n",
    "        y_pred: the probability of a tweet being positive or negative\n",
    "    '''\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # extract the features of the tweet and store it into x\n",
    "    x = extract_features(tweet, freqs, process_tweet=process_tweet)\n",
    "    \n",
    "    # make the prediction using x and theta\n",
    "    y_pred = sigmoid(x@theta)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86b67909-e5a2-447d-bdf8-53d0f3517027",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_logistic_regression(test_x, test_y, freqs, theta, predict_tweet=predict_tweet):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        test_x: a list of tweets\n",
    "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
    "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
    "        theta: weight vector of dimension (3, 1)\n",
    "    Output: \n",
    "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # the list for storing predictions\n",
    "    y_hat = list()\n",
    "    \n",
    "    for tweet in test_x:\n",
    "        # get the label prediction for the tweet\n",
    "        z=extract_features(tweet, freqs, process_tweet=process_tweet)@theta\n",
    "        y_pred = sigmoid(z)\n",
    "        \n",
    "        if y_pred > 0.5:\n",
    "            # append 1.0 to the list\n",
    "            y_hat.append(1.0)\n",
    "        else:\n",
    "            # append 0 to the list\n",
    "            y_hat.append(0.0)\n",
    "\n",
    "    # With the above implementation, y_hat is a list, but test_y is (m,1) array\n",
    "    # convert both to one-dimensional arrays in order to compare them using the '==' operator\n",
    "    accuracy =np.sum(y_hat==test_y.flatten())/len(test_x)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55bd71d6-dc90-4e58-a534-f8bf478e69a9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression model's accuracy = 0.9950\n"
     ]
    }
   ],
   "source": [
    "tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
    "print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78071000-ae66-4c56-bb41-84ea84c52baf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Redditor(name='coderedninja'), Redditor(name='RedMirricat'), Redditor(name='hadToPutDickinCatHmm'), Redditor(name='Defy19'), Redditor(name='yennifluent'), Redditor(name='yeahwellokay'), Redditor(name='thistyviolin'), Redditor(name='ViciousNakedMoleRat'), Redditor(name='gsychopato'), Redditor(name='Deja__Vu__')]\n"
     ]
    }
   ],
   "source": [
    "#0  iloc[4]:\"to stop suicide\"\n",
    "submission = reddit.submission(url=\"https://www.reddit.com/r/therewasanattempt/comments/iu8k8c/to_stop_suicide/\")   #Find the first submission in 'suicide.csv' document                                                     \n",
    "submission.comments.replace_more(limit=None)  \n",
    "id=[]\n",
    "for comment in submission.comments.list():\n",
    "    id.append(comment.author)   #Find the author of comments of first submission    \n",
    "filtered_id = [item for item in id if item is not None]   #Some author log off so I cleared them.\n",
    "print(filtered_id[0:10])    #We are going to analyze 10 authors who were influenced by the first submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0244a9f5-dbba-4a71-92f9-b0ee264bd643",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping user (possible suspension): allthewrongwalls\n",
      "Skipping user (possible suspension): Etherius\n",
      "Skipping user (possible suspension): TimmyTesticles\n",
      "Skipping user (possible suspension): sylphyyyy\n",
      "Skipping user (possible suspension): Summamabitch\n",
      "Skipping user (possible suspension): RevengeOTheCaptnsPen\n",
      "Skipping user (possible suspension): Llanos31\n",
      "Skipping user (possible suspension): Lilytrap\n",
      "Skipping user (possible suspension): pujijik\n",
      "Skipping user (possible suspension): jesus_zombie_attack\n",
      "Skipping user (possible suspension): cocodecococo\n",
      "Skipping user (possible suspension): ZippZappZippty\n",
      "Skipping user (possible suspension): gmonkey143\n",
      "Skipping user (possible suspension): Same-Cartographer488\n",
      "Skipping user (possible suspension): LiquidMotion\n",
      "Skipping user (possible suspension): T1nkyWinky\n",
      "Skipping user (possible suspension): sheepeses\n",
      "Skipping user (possible suspension): Ibeprasin\n",
      "Skipping user (possible suspension): loveandcustard\n",
      "Skipping user (possible suspension): audion00ba\n",
      "Skipping user (possible suspension): jakethedumbmistake\n",
      "Skipping user (possible suspension): Spagot_Lord\n",
      "Skipping user (possible suspension): Overalls42\n",
      "Skipping user (possible suspension): egalroc\n",
      "Skipping user (possible suspension): anarchoposadist1\n",
      "Skipping user (possible suspension): Beingabummer\n",
      "Skipping user (possible suspension): sekhar1010\n",
      "Skipping user (possible suspension): jakethedumbmistake\n",
      "Skipping user (possible suspension): MakeYourselfS1ck\n",
      "Skipping user (possible suspension): bumblebritches57\n",
      "Skipping user (possible suspension): CatsWithAlmdudler\n",
      "Skipping user (possible suspension): Skyrocketxv\n",
      "Skipping user (possible suspension): landback2\n",
      "Skipping user (possible suspension): jesuzombieapocalypse\n",
      "Skipping user (possible suspension): Ltrfsn\n",
      "Skipping user (possible suspension): LifeForce33\n",
      "Skipping user (possible suspension): Bestboii\n",
      "Skipping user (possible suspension): -Rick_Sanchez_\n",
      "Skipping user (possible suspension): Baybob1\n",
      "Skipping user (possible suspension): arkain123\n",
      "Skipping user (possible suspension): InVirtuteElectionis\n",
      "Skipping user (possible suspension): drunk98\n",
      "Skipping user (possible suspension): BrattonCreedThoughts\n",
      "Skipping user (possible suspension): Cinammon-Sprinkler\n",
      "Skipping user (possible suspension): JudgementalPrick\n",
      "Skipping user (possible suspension): bokji\n",
      "Skipping user (possible suspension): MrDeckard\n",
      "Skipping user (possible suspension): cocodecococo\n",
      "Skipping user (possible suspension): CoreaCandy\n",
      "Skipping user (possible suspension): hippolyte_pixii\n",
      "Skipping user (possible suspension): NecessaryTurnip7\n",
      "Skipping user (possible suspension): IllyrioMoParties\n",
      "Skipping user (possible suspension): Extreme_Dingo\n",
      "Skipping user (possible suspension): KingFleaswallow\n",
      "Skipping user (possible suspension): bongblunt\n",
      "Skipping user (possible suspension): converter-bot\n",
      "Skipping user (possible suspension): DANKBOINUGGET\n",
      "Skipping user (possible suspension): DANKBOINUGGET\n",
      "Skipping user (possible suspension): Krypticore\n",
      "Skipping user (possible suspension): SuspendedNo2\n",
      "Skipping user (possible suspension): Blindfide\n",
      "Skipping user (possible suspension): ImmenseCock\n",
      "Skipping user (possible suspension): NoneHaveSufferedAsI\n",
      "Skipping user (possible suspension): QwertyKip\n",
      "Skipping user (possible suspension): Blindfide\n",
      "Skipping user (possible suspension): ScuffedJim\n",
      "Skipping user (possible suspension): alblks\n",
      "Skipping user (possible suspension): MugiwaraNoLuffytaro\n",
      "Skipping user (possible suspension): Martian_Shuriken\n",
      "Skipping user (possible suspension): arkain123\n",
      "Skipping user (possible suspension): GGuitarHero\n",
      "Skipping user (possible suspension): arkain123\n",
      "Skipping user (possible suspension): arkain123\n",
      "Skipping user (possible suspension): allthewrongwalls\n",
      "Skipping user (possible suspension): allthewrongwalls\n",
      "Skipping user (possible suspension): morerokk\n",
      "Skipping user (possible suspension): cocodecococo\n",
      "Skipping user (possible suspension): RevengeOTheCaptnsPen\n",
      "Skipping user (possible suspension): LifeForce33\n",
      "Skipping user (possible suspension): Martian_Shuriken\n",
      "Skipping user (possible suspension): raph772\n",
      "Skipping user (possible suspension): 7162628384736272837\n",
      "Skipping user (possible suspension): Martian_Shuriken\n",
      "Skipping user (possible suspension): InVirtuteElectionis\n",
      "Skipping user (possible suspension): MrPringles23\n",
      "Skipping user (possible suspension): Partially_Deaf\n",
      "Skipping user (possible suspension): kd5nrh\n",
      "Skipping user (possible suspension): AnastasiaTheSexy\n",
      "Skipping user (possible suspension): aguadovimeiro\n",
      "Skipping user (possible suspension): bokji\n",
      "Skipping user (possible suspension): alblks\n",
      "Skipping user (possible suspension): Martian_Shuriken\n",
      "Skipping user (possible suspension): morerokk\n",
      "Skipping user (possible suspension): CoreaCandy\n",
      "Skipping user (possible suspension): TheHeroicOnion\n",
      "Skipping user (possible suspension): bokji\n",
      "Skipping user (possible suspension): audion00ba\n",
      "Skipping user (possible suspension): converter-bot\n",
      "Skipping user (possible suspension): George-Floydtynal\n",
      "Skipping user (possible suspension): 46554B4E4348414453\n",
      "Skipping user (possible suspension): insertnamehere405\n",
      "Skipping user (possible suspension): major84\n",
      "Skipping user (possible suspension): throwmeaway562\n",
      "Skipping user (possible suspension): ImmenseCock\n",
      "Skipping user (possible suspension): Detriumph\n",
      "Skipping user (possible suspension): InVirtuteElectionis\n",
      "Skipping user (possible suspension): AffirmativeSZ\n",
      "Skipping user (possible suspension): diachi_revived\n",
      "Skipping user (possible suspension): big_ol_dad_dick\n",
      "Skipping user (possible suspension): Blindfide\n",
      "Skipping user (possible suspension): Sam-Culper\n",
      "Skipping user (possible suspension): Martian_Shuriken\n",
      "Skipping user (possible suspension): cocodecococo\n",
      "Skipping user (possible suspension): IllyrioMoParties\n",
      "Skipping user (possible suspension): Letscommenttogether\n",
      "Skipping user (possible suspension): arkain123\n",
      "Skipping user (possible suspension): Morbidly-A-Beast\n",
      "Skipping user (possible suspension): RabbidCupcakes\n",
      "Skipping user (possible suspension): morerokk\n",
      "Skipping user (possible suspension): RevengeOTheCaptnsPen\n",
      "Skipping user (possible suspension): DonDove\n",
      "Skipping user (possible suspension): TimeToRedditToday\n",
      "Skipping user (possible suspension): Zeebuoy\n",
      "Skipping user (possible suspension): MrPringles23\n",
      "Skipping user (possible suspension): aguadovimeiro\n",
      "Skipping user (possible suspension): Martian_Shuriken\n",
      "Skipping user (possible suspension): RobertThorn2022\n",
      "Skipping user (possible suspension): morerokk\n",
      "Skipping user (possible suspension): cocodecococo\n",
      "Skipping user (possible suspension): cocodecococo\n",
      "Skipping user (possible suspension): cocodecococo\n",
      "Skipping user (possible suspension): Detriumph\n",
      "Skipping user (possible suspension): Corazon95\n",
      "Skipping user (possible suspension): Blindfide\n",
      "Skipping user (possible suspension): Letscommenttogether\n",
      "Skipping user (possible suspension): Letscommenttogether\n",
      "Skipping user (possible suspension): arkain123\n",
      "Skipping user (possible suspension): RabbidCupcakes\n",
      "Skipping user (possible suspension): RevengeOTheCaptnsPen\n",
      "Skipping user (possible suspension): Martian_Shuriken\n",
      "Skipping user (possible suspension): brevitx\n",
      "Skipping user (possible suspension): CestLaVie1989\n"
     ]
    }
   ],
   "source": [
    "DIF0=[]\n",
    "Before0=[]\n",
    "After0=[]\n",
    "for i in range(0, 1000):\n",
    "    try:\n",
    "        # Try to get some information from the author\n",
    "        comment_count = reddit.redditor(filtered_id[i].name).comments.new(limit=1)\n",
    "        # If successful, going on\n",
    "        comment0 = []\n",
    "        for submission in reddit.redditor(filtered_id[i].name).submissions.new(limit=200):\n",
    "            comment0.append([submission.title, \n",
    "                             submission.created,\n",
    "                             i])\n",
    "    except Exception as e:\n",
    "        # If not, the author has suspended\n",
    "        print(f\"Skipping user (possible suspension): {filtered_id[i].name}\")\n",
    "        continue\n",
    "    comment0 = pd.DataFrame(comment0,\n",
    "                            columns=['title','created','Serial Number'])\n",
    "    #comment0 = comment0.sort_values(by='created', ascending=True)\n",
    "    split_time=posts['created'].iloc[4]\n",
    "    comment0_before=comment0.loc[comment0['created']<split_time]\n",
    "    comment0_after=comment0.loc[comment0['created']>split_time]\n",
    "    text0_before = ''.join(comment0_before.title.values)\n",
    "    y0_before=predict_tweet(text0_before, freqs, theta)\n",
    "    text0_after = ''.join(comment0_after.title.values)\n",
    "    y0_after=predict_tweet(text0_after, freqs, theta)\n",
    "    difference0=(y0_before-y0_after)\n",
    "    DIF0.append(difference0)\n",
    "    Before0.append(comment0_before)\n",
    "    After0.append(comment0_after)\n",
    "    #print(\"Positiveness before the first submission is\",y0_before,\"and after the first submission is\",y0_after,\".\\n\",\"The difference is\", difference0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9f17c5d-0372-4b6d-8c34-083da5ee7e3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "858"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(DIF0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2924f98e-f1de-42e9-b73a-3d273d5bf890",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#1  iloc[20]:\"to prevent suicide\"\n",
    "submission = reddit.submission(url=\"https://www.reddit.com/r/therewasanattempt/comments/16psiru/to_prevent_suicide/\")   #Find the second submission in 'suicide.csv' document     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c46b91fb-1205-46bc-a042-48c094f4b418",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission.comments.replace_more(limit=None)  \n",
    "id=[]\n",
    "for comment in submission.comments.list():\n",
    "    id.append(comment.author)   #Find the author of comments of second submission\n",
    "    \n",
    "filtered_id = [item for item in id if item is not None]   #Some author log off so I cleared them.\n",
    "#print(filtered_id[0:10])    #We are going to analyze 10 authors who were influenced by the second submission\n",
    "DIF1=[]\n",
    "Before1=[]\n",
    "After1=[]\n",
    "for i in range(0, 1000):\n",
    "    try:\n",
    "        # Try to get some information from the author\n",
    "        comment_count = reddit.redditor(filtered_id[i].name).comments.new(limit=1)\n",
    "        # If successful, going on\n",
    "        comment1 = []\n",
    "        for submission in reddit.redditor(filtered_id[i].name).submissions.new(limit=200):\n",
    "            comment1.append([submission.title, \n",
    "                             submission.created,\n",
    "                             i])\n",
    "    except Exception as e:\n",
    "        # If not, the author has suspended\n",
    "        #print(f\"Skipping user (possible suspension): {filtered_id[i].name}\")\n",
    "        continue\n",
    "        \n",
    "    comment1 = pd.DataFrame(comment1,\n",
    "                            columns=['title','created','Serial Number'])\n",
    "    #comment1 = comment1.sort_values(by='created', ascending=True)\n",
    "    split_time=posts['created'].iloc[20]\n",
    "    comment1_before=comment1.loc[comment1['created']<split_time]\n",
    "    comment1_after=comment1.loc[comment1['created']>split_time]\n",
    "    text1_before = ''.join(comment1_before.title.values)\n",
    "    y1_before=predict_tweet(text1_before, freqs, theta)\n",
    "    text1_after = ''.join(comment1_after.title.values)\n",
    "    y1_after=predict_tweet(text1_after, freqs, theta)\n",
    "    difference1=(y1_before-y1_after)\n",
    "    DIF1.append(difference1)\n",
    "    Before1.append(comment1_before)\n",
    "    After1.append(comment1_after)\n",
    "    #print(\"Positiveness before the second submission is\",y1_before,\"and after the second submission is\",y1_after,\".\\n\",\"The difference is\", difference1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "970219d1-e2e9-470f-a10d-2003ceb7fbdb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "490"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(DIF1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d19598e8-d669-412a-8d23-74ab917a1639",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#2  loc[47]:\"'If you were actually suicidal you'd have killed yourself already and not tell anybody' FUCK OFFFFFFF\"\n",
    "submission = reddit.submission(url=\"https://www.reddit.com/r/SuicideWatch/comments/jnrz57/if_you_were_actually_suicidal_youd_have_killed/\")   #Find the third submission in 'suicide.csv' document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "481b8e9d-1842-431c-b272-8a8a90c0f2c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission.comments.replace_more(limit=None)  \n",
    "id=[]\n",
    "for comment in submission.comments.list():\n",
    "    id.append(comment.author)   #Find the author of comments of third submission\n",
    "    \n",
    "filtered_id = [item for item in id if item is not None]   #Some author log off so I cleared them.\n",
    "#print(filtered_id[0:10])    \n",
    "DIF2=[]\n",
    "Before2=[]\n",
    "After2=[]\n",
    "for i in range(0, 1000):\n",
    "    try:\n",
    "        # Try to get some information from the author\n",
    "        comment_count = reddit.redditor(filtered_id[i].name).comments.new(limit=1)\n",
    "        # If successful, going on\n",
    "        comment2 = []\n",
    "        for submission in reddit.redditor(filtered_id[i].name).submissions.new(limit=200):\n",
    "            comment2.append([submission.title, \n",
    "                             submission.created,\n",
    "                             i])\n",
    "    except Exception as e:\n",
    "        # If not, the author has suspended\n",
    "        #print(f\"Skipping user (possible suspension): {filtered_id[i].name}\")\n",
    "        continue\n",
    "    comment2 = pd.DataFrame(comment2,\n",
    "                            columns=['title','created','Serial Number'])\n",
    "    #comment2 = comment2.sort_values(by='created', ascending=True)\n",
    "    split_time=posts_sub['created'].loc[47]\n",
    "    comment2_before=comment2.loc[comment2['created']<split_time]\n",
    "    comment2_after=comment2.loc[comment2['created']>split_time]\n",
    "    text2_before = ''.join(comment2_before.title.values)\n",
    "    y2_before=predict_tweet(text2_before, freqs, theta)\n",
    "    text2_after = ''.join(comment2_after.title.values)\n",
    "    y2_after=predict_tweet(text2_after, freqs, theta)\n",
    "    difference2=(y2_before-y2_after)\n",
    "    DIF2.append(difference2)\n",
    "    Before2.append(comment2_before)\n",
    "    After2.append(comment2_after)\n",
    "    #print(\"Positiveness before the third submission is\",y2_before,\"and after the third submission is\",y2_after,\".\\n\",\"The difference is\", difference2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25c28757-cc44-445a-8bb6-17fba49a86ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(DIF2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4044e6a2-3288-4bd0-811f-cd25a35b9aa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#3  loc[3]:\"I'm a fully-functioning suicidal.\"\n",
    "submission = reddit.submission(url=\"https://www.reddit.com/r/SuicideWatch/comments/ppjowo/im_a_fullyfunctioning_suicidal/\")   #Find the fourth submission in 'suicide.csv' document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9190db4-4cc4-4112-99d2-fc6aefb893ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission.comments.replace_more(limit=None)  \n",
    "id=[]\n",
    "for comment in submission.comments.list():\n",
    "    id.append(comment.author)   #Find the author of comments of fourth submission\n",
    "    \n",
    "filtered_id = [item for item in id if item is not None]   #Some author log off so I cleared them.\n",
    "#print(filtered_id[0:10])  \n",
    "Before3=[]\n",
    "After3=[]\n",
    "DIF3=[]\n",
    "for i in range(0, 1000):\n",
    "    try:\n",
    "        # Try to get some information from the author\n",
    "        comment_count = reddit.redditor(filtered_id[i].name).comments.new(limit=1)\n",
    "        # If successful, going on\n",
    "        comment3 = []\n",
    "        for submission in reddit.redditor(filtered_id[i].name).submissions.new(limit=200):\n",
    "            comment3.append([submission.title,  \n",
    "                             submission.created,\n",
    "                             i])\n",
    "    except Exception as e:\n",
    "        # If not, the author has suspended\n",
    "        #print(f\"Skipping user (possible suspension): {filtered_id[i].name}\")\n",
    "        continue\n",
    "    comment3 = pd.DataFrame(comment3,\n",
    "                            columns=['title','created','Serial Number'])\n",
    "    #comment3 = comment3.sort_values(by='created', ascending=True)\n",
    "    split_time=posts_sub['created'].loc[3]\n",
    "    comment3_before=comment3.loc[comment3['created']<split_time]\n",
    "    comment3_after=comment3.loc[comment3['created']>split_time]\n",
    "    text3_before = ''.join(comment3_before.title.values)\n",
    "    y3_before=predict_tweet(text3_before, freqs, theta)\n",
    "    text3_after = ''.join(comment3_after.title.values)\n",
    "    y3_after=predict_tweet(text3_after, freqs, theta)\n",
    "    difference3=(y3_before-y3_after)\n",
    "    DIF3.append(difference3)\n",
    "    Before3.append(comment3_before)\n",
    "    After3.append(comment3_after)\n",
    "    #print(\"Positiveness before the fourth submission is\",y3_before,\"and after the fourth submission is\",y3_after,\".\\n\",\"The difference is\", difference3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d5f713a-b535-41ad-8948-f3f9a727091f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(DIF3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f8210b1b-f2be-4785-84d6-bbc955ea0dc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_length = max(len(DIF0), len(DIF1), len(DIF2), len(DIF3))\n",
    "\n",
    "# Pad the lists with None or NaN to make them of equal length\n",
    "DIF0 += [None] * (max_length - len(DIF0))\n",
    "DIF1 += [None] * (max_length - len(DIF1))\n",
    "DIF2 += [None] * (max_length - len(DIF2))\n",
    "DIF3 += [None] * (max_length - len(DIF3))\n",
    "data={'DIF0':DIF0,\n",
    "      'DIF1':DIF1,\n",
    "      'DIF2':DIF2,\n",
    "      'DIF3':DIF3}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('Difference.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "04840143-29f3-4a8f-9f67-653993d36461",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DIF0</th>\n",
       "      <th>DIF1</th>\n",
       "      <th>DIF2</th>\n",
       "      <th>DIF3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[0.008745052811056364]]</td>\n",
       "      <td>[[-0.033839677468399776]]</td>\n",
       "      <td>[[0.0023252623631968006]]</td>\n",
       "      <td>[[-0.005110852959371859]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[-0.1026466422107003]]</td>\n",
       "      <td>[[-0.11223124551259583]]</td>\n",
       "      <td>[[0.179672007826861]]</td>\n",
       "      <td>[[-0.08411560641561405]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[-0.06888551598445142]]</td>\n",
       "      <td>[[-0.1312737284238924]]</td>\n",
       "      <td>[[-0.0206910387020246]]</td>\n",
       "      <td>[[-0.022560049469898036]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[0.04562360967435736]]</td>\n",
       "      <td>[[-0.018278326891021934]]</td>\n",
       "      <td>[[-0.0014545788500157641]]</td>\n",
       "      <td>[[0.463627223014264]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[-0.012557223564133413]]</td>\n",
       "      <td>[[-0.10143077476837875]]</td>\n",
       "      <td>[[0.007673561260071293]]</td>\n",
       "      <td>[[0.04525342956343281]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>[[0.04517484889126322]]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>[[-0.2557765473251503]]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>[[-0.24047055424481634]]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>[[0.25664047594108425]]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>[[0.4070801559242212]]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>858 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          DIF0                       DIF1  \\\n",
       "0     [[0.008745052811056364]]  [[-0.033839677468399776]]   \n",
       "1      [[-0.1026466422107003]]   [[-0.11223124551259583]]   \n",
       "2     [[-0.06888551598445142]]    [[-0.1312737284238924]]   \n",
       "3      [[0.04562360967435736]]  [[-0.018278326891021934]]   \n",
       "4    [[-0.012557223564133413]]   [[-0.10143077476837875]]   \n",
       "..                         ...                        ...   \n",
       "853    [[0.04517484889126322]]                       None   \n",
       "854    [[-0.2557765473251503]]                       None   \n",
       "855   [[-0.24047055424481634]]                       None   \n",
       "856    [[0.25664047594108425]]                       None   \n",
       "857     [[0.4070801559242212]]                       None   \n",
       "\n",
       "                           DIF2                       DIF3  \n",
       "0     [[0.0023252623631968006]]  [[-0.005110852959371859]]  \n",
       "1         [[0.179672007826861]]   [[-0.08411560641561405]]  \n",
       "2       [[-0.0206910387020246]]  [[-0.022560049469898036]]  \n",
       "3    [[-0.0014545788500157641]]      [[0.463627223014264]]  \n",
       "4      [[0.007673561260071293]]    [[0.04525342956343281]]  \n",
       "..                          ...                        ...  \n",
       "853                        None                       None  \n",
       "854                        None                       None  \n",
       "855                        None                       None  \n",
       "856                        None                       None  \n",
       "857                        None                       None  \n",
       "\n",
       "[858 rows x 4 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd404f6b-e204-4bfa-9391-4c4bc76c62ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72b32c2-5844-4793-bfbc-1ed0d5c4fd38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d665d8d3-0fb0-4935-b54b-51ee08ade9ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from empath import Empath\n",
    "lexicon = Empath()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "06e0e3e7-3f84-46d5-9050-51bfb1c56f5e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"~~people~~\", \"angry\", \"shouting\"]\n",
      "[\"sad\", \"&gt;_&gt\", \"kidding\", \"&gt;.&gt\", \"&gt;_&gt\", \"&lt;.&lt\"]\n",
      "[\"worried\", \"worried\", \"concerned\", \"apprehensive\", \"worry\", \"afraid\", \"scared\", \"worrying\", \"worries\", \"Worried\", \"hesitant\", \"nervous\", \"biggest_worry\", \"cautious\", \"concerned\", \"worrying\", \"paranoid\", \"worrisome\", \"wary\", \"leery\", \"worry\", \"upset\", \"big_deal\", \"upsetting\", \"biggest_concern\", \"main_concern\", \"stressing\", \"main_worry\", \"unsure\", \"bothering\", \"bad_sign\", \"big_concern\", \"terrified\", \"last_thing\", \"upset\", \"wondering\", \"only_concern\", \"real_concern\", \"thrilled\", \"hurt\", \"b/c\", \"hurting\", \"second_thoughts\", \"huge_deal\", \"good_sign\", \"only_worry\", \"terrified\", \"antsy\", \"bothered\", \"surprised\", \"major_concern\", \"scared\", \"biggest_fear\", \"fussed\", \"Worried\", \"too_big_a_deal\", \"curious\", \"bigger_concern\", \"wondering\", \"would't\", \"only_fear\", \"guessing\", \"huge_concern\", \"insistent\", \"that_big_a_deal\", \"stressed\", \"trouble\", \"main_fear\", \"risk\", \"bother\", \"careful\", \"how_much_trouble\", \"relieved\", \"reluctant\", \"keen\", \"only_real_concern\", \"peeved\", \"real_worry\", \"abut\", \"fussed\", \"alarmed\", \"big_issue\", \"honestly\", \"gunshy\", \"weary\", \"sure\", \"considering\", \"freaked\", \"panicking\", \"b/c\", \"annoyed\"]\n",
      "[\"frustrated\", \"frustrated\", \"annoyed\", \"angry\", \"upset\", \"frustrating\", \"frustrating\", \"disheartened\", \"impatient\", \"annoyed\", \"tired\", \"demotivated\", \"overwhelmed\", \"stressed\", \"discouraged\", \"disheartened\", \"flustered\", \"bored\", \"stressed\", \"antsy\", \"irritated\", \"unhappy\", \"pissy\", \"demoralizing\", \"irate\", \"disinterested\", \"agitated\", \"distracted\", \"resentful\", \"pissed\", \"littlest_things\", \"understandably\", \"tiring\", \"frustration\", \"uninterested\", \"panicky\", \"mad\", \"nervous\", \"frustrates\", \"infuriated\", \"aggravating\", \"disheartening\", \"infuriated\", \"exasperated\", \"anxious\", \"discouraging\", \"smallest_things\", \"overexcited\", \"hopeless\", \"aggravating\", \"embarrassed\", \"unsatisfied\", \"scared\", \"frazzled\", \"frustrate\", \"peeved\", \"hurt\", \"despondent\", \"nagging\", \"upset\", \"upset\", \"irrationally\", \"pissy\", \"bad_mood\", \"frustated\", \"fustrated\", \"dissatisfied\", \"annoying\", \"nagged\", \"furious\", \"depressed\", \"bothering\", \"maddening\", \"demoralized\", \"upsetting\", \"indecisive\", \"tiresome\", \"demotivated\", \"annoying\", \"embarassed\", \"panicked\", \"relieved\", \"ragey\", \"dejected\", \"disinterested\", \"upsetting\", \"exhausted\", \"disrespected\", \"unmotivated\", \"disgusted\", \"constant_complaining\", \"grumpy\", \"tiring\", \"sulky\", \"distracted\", \"unmotivated\", \"demotivating\", \"exhausting\", \"feeling\"]\n",
      "[\"anxious\", \"stressed\", \"panicky\", \"nervous\", \"depressed\", \"stressed\", \"irritable\", \"nervous_wreck\", \"anxious_person\", \"feeling\", \"fidgety\", \"anxiety_attacks\", \"anxious_people\", \"overstimulated\", \"agitated\", \"withdrawn\", \"uneasy\", \"really_bad_anxiety\", \"overwhelmed\", \"panic_attack\", \"distracted\", \"panic_attacks\", \"antsy\", \"relieved\", \"hypomanic\", \"depressed\", \"unsettled\", \"panicked\", \"anxiety_attack\", \"frazzled\", \"less_anxiety\", \"horrible_anxiety\", \"restless\", \"high_anxiety\", \"social_situations\", \"bad_anxiety\", \"nauseated\", \"manic\", \"more_anxiety\", \"depressed_state\", \"extreme_anxiety\", \"anxiety_issues\", \"uncomfortable\", \"anxiety\", \"anxious_feeling\", \"weepy\", \"nauseous\", \"paranoid\", \"terrified\", \"anxious_thoughts\", \"despondent\", \"introverted\", \"major_anxiety\", \"lonely\", \"anxiety\", \"moody\", \"scared\", \"terrible_anxiety\", \"exhausted\", \"crazy_anxiety\", \"sleepy\", \"so_much_anxiety\", \"normal_feeling\", \"suicidal\", \"forgetful\", \"lethargic\", \"unwell\", \"anxiousness\", \"trapped\", \"stressful\", \"intense_anxiety\", \"little_anxiety\", \"hypervigilant\", \"such_anxiety\", \"massive_anxiety\", \"overstimulated\", \"agoraphobic\", \"full_blown_panic_attack\", \"bad_social_anxiety\", \"depressive_state\", \"frustrated\", \"intrusive_thoughts\", \"neurotic\", \"shy_person\", \"frightened\", \"nervousness\", \"unnerved\", \"nauseated\", \"apprehensive\", \"grouchy\", \"cranky\", \"stressing\", \"hyperaware\", \"just_anxiety\", \"talkative\", \"terrible_mood\"]\n",
      "[\"nervous\", \"anxious\", \"panicky\", \"nervous_wreck\", \"apprehensive\", \"uneasy\", \"scared\", \"flustered\", \"antsy\", \"panicked\", \"embarrassed\", \"relieved\", \"embarassed\", \"freaked\", \"terrified\", \"shy\", \"unnerved\", \"nonchalant\", \"frazzled\", \"worried\", \"stressed\", \"uncomfortable\", \"fidgety\", \"worried\", \"awkward\", \"awkward\", \"panicking\", \"embarassed\", \"enthused\", \"stressed\", \"excited\", \"giddy\", \"excited/nervous\", \"unsettled\", \"intimidated\", \"agitated\", \"paranoid\", \"annoyed\", \"jumpy\", \"overexcited\", \"relaxed\", \"confident\", \"frustrated\", \"enthusiastic\", \"frightened\", \"upset\", \"irritated\", \"distracted\", \"flustered\", \"shaken\", \"weird_feeling\", \"weirded\", \"cautious\", \"sheepish\", \"afraid\", \"awkward\", \"confused\", \"akward\", \"bad_sign\", \"elated\", \"nervous/anxious\", \"reassuring\", \"alarmed\", \"just_nerves\", \"disheartened\", \"scared\", \"freaked\", \"nervous_person\", \"unnerving\", \"weepy\", \"nerve_racking\", \"disinterested\", \"huge_relief\", \"upset\", \"embarrased\", \"impatient\", \"panicking\", \"chatty\", \"rattled\", \"terrified\", \"nerve_wracking\", \"overwhelmed\", \"hesitant\", \"feeling\", \"thrilled\", \"reassured\", \"weird\", \"exasperated\", \"giggly\", \"frantic\", \"angry\", \"pushy\", \"embarrassing\", \"disconcerting\", \"nervousness\", \"embarrased\", \"talkative\", \"panic\"]\n",
      "[\"disappointed\", \"disappointed\", \"dissapointed\", \"dissappointed\", \"dissapointed\", \"disapointed\", \"disappointing\", \"surprised\", \"bummed\", \"underwhelmed\", \"bummed\", \"miffed\", \"unimpressed\", \"excited\", \"surprised\", \"disappointing\", \"intrigued\", \"impressed\", \"shocked\", \"excited\", \"shocked\", \"disapointed\", \"dissapointing\", \"saddened\", \"dismayed\", \"dissappointed\", \"thrilled\", \"peeved\", \"saddened\", \"ecstatic\", \"annoyed\", \"expecting\", \"pleased\", \"elated\", \"psyched\", \"stoked\", \"psyched\", \"baffled\", \"disheartened\", \"suprised\", \"perplexed\", \"hyped\", \"overjoyed\", \"suprised\", \"thrilled\", \"enthused\", \"sad\", \"irked\", \"relieved\", \"conflicted\", \"gonna_lie\", \"peeved\", \"wowed\", \"hyped\", \"intrigued\", \"confused\", \"floored\", \"liked\", \"jazzed\", \"disheartened\", \"huge_disappointment\", \"underwhelmed\", \"pleasantly\", \"infuriated\", \"impressed\", \"hoped\", \"annoyed\", \"heartbroken\", \"huge_letdown\", \"pleasant_surprise\", \"Disappointed\", \"perplexed\", \"amused\", \"jazzed\", \"upsetting\", \"upset\", \"baffled\", \"unsurprised\", \"big_disappointment\", \"giddy\", \"pissed\", \"mystified\", \"flabbergasted\", \"amazed\", \"Disappointing\", \"stoked\", \"irked\", \"overjoyed\", \"surpised\", \"cheated\", \"apprehensive\", \"wish\", \"puzzled\", \"crestfallen\", \"appalled\", \"astounded\", \"displeased\", \"spoiled\", \"perturbed\", \"amused\"]\n",
      "[\"gloomy\", \"dreary\", \"bleak\", \"cheery\", \"dark\", \"stormy\", \"somber\", \"rainy\", \"overcast\", \"melancholy\", \"melancholic\", \"foggy\", \"brooding\", \"grim\", \"morose\", \"depressing\", \"eerie\", \"wintery\", \"ominous\", \"colourful\", \"cheerful\", \"glum\", \"drab\", \"snowy\", \"wistful\", \"dour\", \"serene\", \"moody\", \"colorful\", \"foreboding\", \"cloudy\", \"vibrant\", \"dark\", \"whimsical\", \"depressing\", \"bright\", \"chilly\", \"grey_skies\", \"sullen\", \"lush\", \"windy\", \"sunny\", \"sombre\", \"blue_skies\", \"chilly\", \"surreal\", \"mopey\", \"cloudless\", \"melancholic\", \"lifeless\", \"forlorn\", \"cheery\", \"desolate\", \"mournful\", \"nighttime\", \"depressing\", \"uplifting\", \"upbeat\", \"vivid\", \"dreamlike\", \"foreboding\", \"dramatic\", \"bleaker\", \"dreamy\", \"dark\", \"vibrantly\", \"winter_wonderland\", \"sunnier\", \"joyful\", \"grey\", \"chipper\", \"eery\", \"pensive\", \"breezy\", \"cold\", \"tinged\", \"clear_sky\", \"blue_sky\", \"grumpy\", \"weepy\", \"hearted\", \"uninviting\", \"lively\", \"brightened\", \"little_dark\", \"joyless\", \"sappy\", \"carefree\", \"cozy\", \"muggy\", \"darker\"]\n",
      "[\"miserable\", \"depressed\", \"hopeless\", \"lonely\", \"unhappy\", \"depressed\", \"resentful\", \"lonely\", \"shitty_life\", \"alone\", \"trapped\", \"unmotivated\", \"stressed\", \"happy_person\", \"sick\", \"unloved\", \"feeling\", \"stressed\", \"stressful\", \"selfish\", \"exhausting\", \"miserable_existence\", \"miserable_life\", \"miserable_person\", \"mope\", \"shit_life\", \"friendless\", \"unmotivated\", \"exhausted\", \"shitty_feeling\", \"pretty_good_life\", \"emotional_mess\", \"unlovable\", \"despondent\", \"crappy_life\", \"shitty_relationship\", \"unaccomplished\", \"resent\", \"emotional_wreck\", \"exhausting\", \"hellish\", \"unsupportive\", \"moping\", \"suicidal\", \"mopey\", \"friendless\", \"complete_wreck\", \"dead-end_job\", \"dread\", \"suffocating\", \"crabby\", \"miserable_situation\", \"stressful_job\", \"selfish_asshole\", \"deprived\", \"bad_relationship\", \"unfulfilled\", \"resenting\", \"horrible_feeling\", \"happiest\", \"lazy_slob\", \"mediocre_life\", \"absolute_hell\", \"constant_struggle\", \"unhappy_relationship\", \"shitty_time\", \"unfulfilling\", \"torturous\", \"heartbroken\", \"just_life\", \"living_nightmare\", \"good_girlfriend\", \"bad_life\", \"shitty_mood\", \"miserable_people\", \"depressing\", \"constant_worry\", \"devastated\", \"terrified\", \"own_personal_hell\", \"awesome_life\", \"happier_person\", \"thankful\", \"sad\", \"cranky\", \"depressing\", \"horrible_person\", \"jobless\", \"ashamed\", \"pitying\", \"constant_anxiety\", \"grumpy\", \"joyless\", \"shitty\", \"misery\", \"terrible_life\"]\n",
      "[\"lonely\", \"lonely\", \"mopey\", \"boring_person\"]\n",
      "[\"happy\"]\n",
      "[\"loved\", \"liked\", \"adored\", \"hated\", \"enjoyed\", \"Loved\", \"Hated\", \"LOVED\", \"looooved\", \"adore\", \"disliked\", \"loathed\", \"LOVED\", \"despised\", \"loving\", \"loooved\", \"HATED\", \"huge_fan\", \"loooooved\", \"HUGE_fan\", \"biggest_disappointment\", \"biggest_fan\", \"enthralled\", \"detested\", \"captivated\", \"massive_fan\", \"Liked\", \"enthralled\", \"love(d\", \"raved\", \"amazing\", \"same_feeling\", \"huge_disappointment\", \"wished\", \"watched\", \"reminds\", \"soft_spot\", \"best_part\", \"those_years_ago\", \"favorite_story\", \"adoring\", \"love/hate_relationship\", \"favorite_parts\", \"fond_memories\", \"cared\", \"fantastic\", \"first_introduction\", \"enjoyed\", \"wowed\", \"AMAZING.\", \"pained\", \"huge_fans\", \"first_times\", \"first_time\", \"freakin\", \"absolute_favorite\", \"floored\", \"excited\", \"very_first_time\", \"favorite\", \"thrilled\", \"absolute_love\", \"favourite\", \"first_experience\", \"big_fan\", \"thrilled\", \"kick-ass\", \"amazed\", \"felt\", \"huge*_fan\", \"LOVED\", \"time_favorite\", \"AMAZING\", \"very_fond_memories\", \"favorite_part\", \"sucked\", \"disappointed\", \"adores\", \"enamored\", \"such_high_hopes\", \"wee_lad\", \"Huge_fan\", \"least_favorite\", \"only_good_thing\", \"favorite_thing\", \"best_parts\", \"best_moments\", \"cried\", \"favourite_parts\", \"remembered\", \"teared\", \"heartbroken\", \"such_fond_memories\", \"enamored\", \"major_fan\"]\n",
      "[\"joyful\", \"joyous\", \"uplifting\", \"cheerful\", \"loving\", \"carefree\", \"cheery\", \"sorrowful\", \"indescribably\", \"melancholy\", \"somber\", \"comforting\", \"comforting\", \"blissful\", \"jubilant\", \"contemplative\", \"heartwarming\", \"serene\", \"mournful\", \"invigorating\", \"tearful\", \"contented\", \"wistful\", \"exhilarated\", \"heartfelt\", \"caring\", \"comforted\", \"wonderful\", \"exuberant\", \"joy\", \"humbling\", \"triumphant\", \"despairing\", \"sorrowful\", \"morose\", \"loved\", \"despondent\", \"happy_person\", \"elated\", \"loving\", \"genuine_happiness\", \"cathartic\", \"touching\", \"heart_warming\", \"dignified\", \"compassionate\", \"wondrous\", \"poignant\", \"upbeat\", \"blessed\", \"melancholic\", \"stoic\", \"bittersweet\", \"pained\", \"fleeting\", \"pleasant\", \"jovial\", \"true_joy\", \"moving\", \"uplifted\", \"liberating\", \"longing\", \"inspiring\", \"comforting\", \"calming\", \"go-lucky\", \"thoughtful\", \"consoling\", \"gratefulness\", \"pure_happiness\", \"profound\", \"delight\", \"pure_love\", \"glum\", \"torturous\", \"joyfully\", \"humbling\", \"alluring\", \"outwardly\", \"edifying\", \"indescribable\", \"happy_times\", \"whimsical\", \"feeling\", \"exhilarating\", \"heartwarming\", \"wholesome\", \"inspirational\", \"selfless\", \"regretful\", \"such_happiness\", \"reverent\", \"pensive\", \"such_joy\", \"courageous\", \"endearing\", \"beautiful\"]\n",
      "[\"content\", \"content\", \"relevant_content\", \"said_content\", \"own_content\", \"legitimate_content\", \"actual_content\", \"content_people\", \"content_creator\", \"real_content\", \"good_content\", \"only_content\", \"worthwhile_content\", \"specific_content\", \"unique_content\", \"shitty_content\", \"low_quality_content\", \"other_content\", \"paid_content\", \"content\", \"meaningful_content\", \"reddit_content\", \"other_content_creators\", \"certain_content\", \"popular_content\", \"interesting_content\", \"poor_content\", \"better_content\", \"creative_content\", \"subreddit\", \"game_content\", \"further_content\", \"low_effort_content\", \"content_creators\", \"pirated_material\", \"own_forum\", \"free_content\", \"paid_users\", \"pirated_content\", \"user_submissions\", \"valuable_content\", \"regular_content\", \"ad_revenue\", \"own_original_content\", \"best_content\"]\n",
      "[\"pleased\", \"impressed\", \"delighted\", \"thrilled\", \"unimpressed\", \"thrilled\", \"pleased\", \"overjoyed\", \"disappointed\", \"displeased\", \"surprised\", \"disappointed\", \"overjoyed\", \"impressed\", \"dismayed\", \"ecstatic\", \"chuffed\", \"appreciative\", \"elated\", \"delighted\", \"enthused\", \"enthused\", \"intrigued\", \"excited\", \"surprised\", \"astonished\", \"excited\", \"stoked\", \"miffed\", \"saddened\", \"amused\", \"underwhelmed\", \"satisfied\", \"humbled\", \"thankful\", \"shocked\", \"honoured\", \"glad\", \"perplexed\", \"shocked\", \"perplexed\", \"dissapointed\", \"dissapointed\", \"pleasantly\", \"dissappointed\", \"eager\", \"grateful\", \"honored\", \"heartened\", \"disheartened\", \"chuffed\", \"relieved\", \"wowed\", \"keen\", \"saddened\", \"hoped\", \"appalled\", \"intrigued\", \"displeased\", \"Impressed\", \"amazed\", \"hopeful\", \"enthusiastic\", \"elated\", \"smitten\", \"stoked\", \"awed\", \"amused\", \"puzzled\", \"unimpressed\", \"proud\", \"happy\", \"disappointing\", \"gracious\", \"apprehensive\", \"disapointed\", \"puzzled\", \"astonished\", \"perturbed\", \"psyched\", \"psyched\", \"envious\", \"gobsmacked\", \"floored\", \"astounded\", \"loving\", \"pleasant_surprise\", \"great_surprise\", \"commended\", \"baffled\", \"congratulated\", \"peeved\", \"underwhelmed\", \"greatful\", \"gratified\", \"dissatisfied\", \"reassured\", \"bemused\"]\n",
      "[\"grateful\", \"thankful\", \"greatful\", \"appreciative\", \"fortunate\", \"blessed\", \"kind_person\", \"overjoyed\", \"thrilled\", \"glad\", \"proud\", \"wonderful_person\", \"humbled\", \"loved\", \"appreciate\", \"delighted\", \"amazing_gift\", \"relieved\", \"understanding\", \"honored\", \"happy\", \"envious\", \"wonderful_people\", \"thrilled\", \"gratitude\", \"reassured\", \"supportive\", \"Thankful\", \"good_soul\", \"amazing_person\", \"gracious\", \"elated\", \"moved\", \"generous_gift\", \"huge_relief\", \"honored\", \"comforted\", \"I/we\", \"wonderful_experience\", \"great_gift\", \"wonderful_gift\", \"humbled\", \"wished\", \"pleased\", \"disheartened\", \"great_person\", \"thankfull\", \"cherish\", \"awesome_person\", \"thankful\", \"eager\", \"lovely_person\", \"ungrateful\", \"good_fortune\", \"so_much_love\", \"kind_people\", \"patient\", \"sincerely\", \"obliged\", \"overjoyed\", \"wonderful\", \"generosity\", \"caring\", \"pleased\", \"lovely_people\", \"generous\", \"reassured\", \"ecstatic\", \"commiserate\", \"honoured\", \"generous_person\", \"honoured\", \"gratefulness\", \"little_reminder\", \"appreciate\", \"great_friend\", \"wonderful_advice\", \"thanked\", \"ashamed\", \"promise\", \"humbling\", \"welcomed\", \"truly\", \"kind_gesture\", \"saddened\", \"reassuring\", \"well_wishes\", \"selfless\", \"dearly\", \"gratefull\", \"kind_thoughts\", \"thanking\", \"reassuring\", \"supportive_words\"]\n",
      "[\"relieved\", \"relieved\", \"elated\", \"terrified\", \"heartbroken\", \"devastated\", \"huge_relief\", \"reassured\", \"nervous\", \"regretful\", \"distraught\", \"embarrassed\", \"panicked\", \"comforting\", \"freaked\", \"saddened\", \"horrified\", \"overjoyed\", \"nervous_wreck\", \"unnerved\", \"comforted\", \"reassuring\", \"shocked\", \"mortified\", \"anxious\", \"thankful\", \"feeling\", \"traumatized\", \"reassured\", \"upsetting\", \"sad\", \"conflicted\", \"wishing\", \"strange_feeling\", \"thrilled\", \"big_relief\", \"glad\", \"unsettled\", \"relived\", \"frightened\", \"defeated\", \"uneasy\", \"comforting\", \"scared\", \"bewildered\", \"disheartened\", \"despondent\", \"disgusted\", \"disappointed\", \"embarassed\", \"mortified\", \"felt\", \"reassuring\", \"pained\", \"stressed\", \"depressed\", \"dejected\", \"panicky\", \"upset\", \"horrible_feeling\", \"amused\", \"giddy\", \"bummed\", \"perplexed\", \"shaken\", \"confused\", \"frazzled\", \"puzzled\", \"terrible_feeling\", \"shocked\", \"stressed\", \"annoyed\", \"distressed\", \"dismayed\", \"freaked\", \"hopeful\", \"tearful\", \"weird_feeling\", \"bummed\", \"overwhelmed\", \"embarassed\", \"disturbed\", \"grateful\", \"elated\", \"apprehensive\", \"speechless\", \"flabbergasted\", \"dreading\", \"weepy\", \"total_shock\", \"Relieved\", \"worried\", \"disheartened\", \"moved\", \"ecstatic\", \"perturbed\", \"great_relief\", \"feeling\", \"lonely\"]\n",
      "[\"optimistic\", \"pessimistic\", \"hopeful\", \"optomistic\", \"optimistically\", \"skeptical\", \"cynical\", \"pessimistically\", \"sceptical\", \"confident\", \"pessimist\", \"optimistic_side\", \"worrisome\", \"optimist\", \"jaded\", \"thrilled\", \"uncertain\", \"ambitious\", \"enthused\", \"cautious\", \"excited\", \"worrying\", \"disheartened\", \"honest\", \"disappointing\", \"apprehensive\", \"reassuring\", \"bullish\", \"bleak\", \"great_sign\", \"good_sign\", \"cynic\", \"bodes\", \"optimism\", \"disheartened\", \"doubtful\", \"ecstatic\", \"committed\", \"enthusiastic\", \"cautiously\", \"disappointed\", \"disappointing\", \"optimistic_outlook\", \"naive\", \"high_hopes\", \"idealistic\", \"long_term_future\", \"na\\u00efve\", \"next_few_years\", \"concerned\", \"very_good_sign\", \"weary\", \"skeptic\", \"conflicted\", \"fatalistic\", \"realistic\", \"good_news\", \"enthused\", \"more_hope\", \"forthcoming\", \"Optimistic\", \"pipe_dream\", \"immediate_future\", \"shortsighted\", \"heartening\", \"exciting\", \"nervous\", \"worried\", \"optimistic_person\", \"cautious_optimism\", \"disappointed\", \"confidant\", \"encouraging\", \"long_haul\", \"informed\", \"surprising\", \"reassured\", \"complacent\", \"foolish\", \"at_least_hope\", \"thankful\", \"huge_surprise\", \"dismal\", \"promising\", \"eternal_optimist\", \"very_little_hope\", \"successful\", \"premature\", \"bright_future\", \"disheartening\", \"dismayed\", \"convinced\", \"unexcited\", \"apathetic\", \"bad_sign\", \"real_possibility\", \"next_couple_years\"]\n",
      "[\"satisfied\", \"unsatisfied\", \"dissatisfied\", \"satisified\", \"content\", \"satisfactory\", \"satisfied\", \"dissatisfied\", \"pleased\", \"unhappy\", \"desire\", \"satisfy\", \"unsatisfactory\", \"pleased\", \"thrilled\", \"confident\", \"happier\", \"disappointed\", \"determined\", \"thrilled\", \"sated\", \"unfulfilled\", \"I/we\", \"pressured\", \"desired\", \"ecstatic\", \"elated\", \"obligated\", \"satisfies\", \"gratified\", \"committed\", \"enjoying\", \"compelled\", \"indifferent\", \"disheartened\", \"desired\", \"reassured\", \"contempt\", \"satisfying\", \"thankful\", \"grateful\", \"happy\", \"satisfaction\", \"positive\", \"longing\", \"unsure\", \"comfortable\", \"obliged\", \"relieved\", \"overjoyed\", \"feeling\", \"tempted\", \"continue\", \"unimpressed\", \"disappointed\", \"cheated\", \"compelled\", \"loving\", \"certain\", \"okay\", \"desire\", \"honest\", \"wanting\", \"it-\"]\n",
      "[\"excited\", \"excited\", \"psyched\", \"stoked\", \"psyched\", \"stoked\", \"pumped\", \"thrilled\", \"jazzed\", \"giddy\", \"ecstatic\", \"bummed\", \"bummed\", \"enthused\", \"Excited\", \"hyped\", \"Excited\", \"hyped\", \"disappointed\", \"jazzed\", \"intrigued\", \"thrilled\", \"hopeful\", \"exciting\", \"elated\", \"amped\", \"overjoyed\", \"disappointed\", \"nervous\", \"dissapointed\", \"excited/nervous\", \"exciting\", \"awesome\", \"hoping\", \"apprehensive\", \"dissapointed\", \"dreading\", \"disheartened\", \"disappointing\", \"underwhelmed\", \"disapointed\", \"impressed\", \"pleased\", \"disappointing\", \"unexcited\", \"glad\", \"pleasant_surprise\", \"Stoked\", \"antsy\", \"high_hopes\", \"pumped\", \"dissappointed\", \"nice_surprise\", \"hoping\", \"enthusiastic\", \"relieved\", \"estatic\", \"delighted\", \"little_bummed\", \"surprised\", \"intrigued\", \"surprised\", \"exciting_news\", \"awesome_surprise\", \"really_good_feeling\", \"wowed\", \"good_feeling\", \"conflicted\", \"optimistic\", \"gonna_lie\", \"disapointed\", \"enthused\", \"really_good_time\", \"real_treat\", \"super_hype\", \"amazing\", \"happy\", \"tempted\", \"unimpressed\", \"big_plans\", \"great_surprise\", \"keen\", \"so_hype\", \"thankful\", \"heartbroken\", \"amazed\", \"shocked\", \"exicted\", \"eager\", \"awesome_news\", \"enthralled\", \"very_high_hopes\", \"amazing_news\"]\n"
     ]
    }
   ],
   "source": [
    "#negative\n",
    "lexicon.create_category(\"angry\", [\"angry\"], model=\"reddit\")\n",
    "lexicon.create_category(\"sad\", [\"sad\"], model=\"reddit\")\n",
    "lexicon.create_category(\"worried\", [\"worried\"], model=\"reddit\")\n",
    "lexicon.create_category(\"frustrated\", [\"frustrated\"], model=\"reddit\")\n",
    "lexicon.create_category(\"anxious\", [\"anxious\"], model=\"reddit\")\n",
    "lexicon.create_category(\"nervous\", [\"nervous\"], model=\"reddit\")\n",
    "lexicon.create_category(\"disappointed\", [\"disappointed\"], model=\"reddit\")\n",
    "lexicon.create_category(\"gloomy\", [\"gloomy\"], model=\"reddit\")\n",
    "lexicon.create_category(\"miserable\", [\"miserable\"], model=\"reddit\")\n",
    "lexicon.create_category(\"lonely\", [\"lonely\"], model=\"reddit\")\n",
    "#positive\n",
    "lexicon.create_category(\"happy\", [\"happy\"], model=\"reddit\")\n",
    "lexicon.create_category(\"loved\", [\"loved\"], model=\"reddit\")\n",
    "lexicon.create_category(\"joyful\", [\"joyful\"], model=\"reddit\")\n",
    "lexicon.create_category(\"content\", [\"content\"], model=\"reddit\")\n",
    "lexicon.create_category(\"pleased\", [\"pleased\"], model=\"reddit\")\n",
    "lexicon.create_category(\"grateful\", [\"grateful\"], model=\"reddit\")\n",
    "lexicon.create_category(\"relieved\", [\"relieved\"], model=\"reddit\")\n",
    "lexicon.create_category(\"optimistic\", [\"optimistic\"], model=\"reddit\")\n",
    "lexicon.create_category(\"satisfied\", [\"satisfied\"], model=\"reddit\")\n",
    "lexicon.create_category(\"excited\", [\"excited\"], model=\"reddit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3efc54f9-0e9b-4fe5-9041-580298b8c469",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['before','after','difference']\n",
    "index=[\"angry\",\"sad\",\"worried\",\n",
    "       \"frustrated\",\"anxious\",\n",
    "        \"nervous\",\"disappointed\",\n",
    "      \"gloomy\",\"miserable\",\"longly\",\n",
    "       \"happy\", \"loved\", \"joyful\",\n",
    "        \"content\", \"pleased\",\n",
    "      \"grateful\", \"relieved\",\n",
    "      \"optimistic\", \"satisfied\",\n",
    "        \"excited\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "550bdffe-3fe8-4d5f-b9a1-6fa08b50e540",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dic0={}\n",
    "for i in range(0,len(Before0)):\n",
    "    df0=pd.DataFrame(np.zeros((20,3)),columns=columns,index=index)\n",
    "    for text in Before0[i].title:\n",
    "        negative_before0 = lexicon.analyze(text, categories=[\"angry\",\"sad\",\"worried\",\n",
    "                                                    \"frustrated\",\"anxious\",\n",
    "                                                    \"nervous\",\"disappointed\",\n",
    "                                                    \"gloomy\",\"miserable\",\"longly\"])\n",
    "        positive_before0 = lexicon.analyze(text, categories=[\"happy\", \"loved\", \"joyful\",\n",
    "                                                     \"content\", \"pleased\",\n",
    "                                                     \"grateful\", \"relieved\",\n",
    "                                                     \"optimistic\", \"satisfied\",\n",
    "                                                     \"excited\", \"loved\"])\n",
    "        for key, value in positive_before0.items():\n",
    "            df0.loc[key,'before']+=value\n",
    "        for key, value in negative_before0.items():\n",
    "            df0.loc[key,'before']+=value\n",
    "    for text in After0[i].title:\n",
    "        negative_after0 = lexicon.analyze(text, categories=[\"angry\", \"sad\", \"worried\",\n",
    "                                                             \"frustrated\", \"anxious\",\n",
    "                                                             \"nervous\", \"disappointed\",\n",
    "                                                             \"gloomy\", \"miserable\", \"longly\"])\n",
    "        positive_after0 = lexicon.analyze(text, categories=[\"happy\", \"loved\", \"joyful\",\n",
    "                                                             \"content\", \"pleased\",\n",
    "                                                             \"grateful\", \"relieved\",\n",
    "                                                             \"optimistic\", \"satisfied\",\n",
    "                                                             \"excited\"])\n",
    "\n",
    "        for key, value in positive_after0.items():\n",
    "            df0.loc[key,'after']+=value\n",
    "        for key, value in negative_after0.items():\n",
    "            df0.loc[key,'after']+=value\n",
    "    df0.difference=df0.before-df0.after\n",
    "    dic0[i]=df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8f4256e-a080-4170-bd25-671593b9fdd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dic0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dic0[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dic0' is not defined"
     ]
    }
   ],
   "source": [
    "dic0[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "46247264-f26c-4278-962a-a725139a4607",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dic1={}\n",
    "for i in range(0,len(Before1)):\n",
    "    df1=pd.DataFrame(np.zeros((20,3)),columns=columns,index=index)\n",
    "    for text in Before1[i].title:\n",
    "        negative_before1 = lexicon.analyze(text, categories=[\"angry\",\"sad\",\"worried\",\n",
    "                                                    \"frustrated\",\"anxious\",\n",
    "                                                    \"nervous\",\"disappointed\",\n",
    "                                                    \"gloomy\",\"miserable\",\"longly\"])\n",
    "        positive_before1 = lexicon.analyze(text, categories=[\"happy\", \"loved\", \"joyful\",\n",
    "                                                     \"content\", \"pleased\",\n",
    "                                                     \"grateful\", \"relieved\",\n",
    "                                                     \"optimistic\", \"satisfied\",\n",
    "                                                     \"excited\", \"loved\"])\n",
    "        for key, value in positive_before1.items():\n",
    "            df1.loc[key,'before']+=value\n",
    "        for key, value in negative_before1.items():\n",
    "            df1.loc[key,'before']+=value\n",
    "    for text in After1[i].title:\n",
    "        negative_after1 = lexicon.analyze(text, categories=[\"angry\", \"sad\", \"worried\",\n",
    "                                                             \"frustrated\", \"anxious\",\n",
    "                                                             \"nervous\", \"disappointed\",\n",
    "                                                             \"gloomy\", \"miserable\", \"longly\"])\n",
    "        positive_after1 = lexicon.analyze(text, categories=[\"happy\", \"loved\", \"joyful\",\n",
    "                                                             \"content\", \"pleased\",\n",
    "                                                             \"grateful\", \"relieved\",\n",
    "                                                             \"optimistic\", \"satisfied\",\n",
    "                                                             \"excited\"])\n",
    "\n",
    "        for key, value in positive_after1.items():\n",
    "            df1.loc[key,'after']+=value\n",
    "        for key, value in negative_after1.items():\n",
    "            df1.loc[key,'after']+=value\n",
    "    df1.difference=df1.before-df1.after\n",
    "    dic1[i]=df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "62f1ed13-e31c-4659-be92-457686b327fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "      <th>difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>angry</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sad</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worried</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frustrated</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anxious</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nervous</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointed</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gloomy</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>miserable</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>longly</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>happy</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loved</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joyful</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>content</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pleased</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grateful</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relieved</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimistic</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>satisfied</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excited</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              before  after  difference\n",
       "angry            0.0    0.0         0.0\n",
       "sad              0.0    0.0         0.0\n",
       "worried          0.0    0.0         0.0\n",
       "frustrated       0.0    0.0         0.0\n",
       "anxious          0.0    0.0         0.0\n",
       "nervous          0.0    0.0         0.0\n",
       "disappointed     0.0    0.0         0.0\n",
       "gloomy           0.0    0.0         0.0\n",
       "miserable        0.0    0.0         0.0\n",
       "longly           0.0    0.0         0.0\n",
       "happy            0.0    0.0         0.0\n",
       "loved            0.0    1.0        -1.0\n",
       "joyful           0.0    0.0         0.0\n",
       "content          0.0    2.0        -2.0\n",
       "pleased          0.0    0.0         0.0\n",
       "grateful         0.0    0.0         0.0\n",
       "relieved         0.0    0.0         0.0\n",
       "optimistic       0.0    0.0         0.0\n",
       "satisfied        0.0    2.0        -2.0\n",
       "excited          0.0    1.0        -1.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6048f418-ceee-4278-aaac-5e4ff86bca21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dic2={}\n",
    "for i in range(0,len(Before2)):\n",
    "    df2=pd.DataFrame(np.zeros((20,3)),columns=columns,index=index)\n",
    "    for text in Before2[i].title:\n",
    "        negative_before2 = lexicon.analyze(text, categories=[\"angry\",\"sad\",\"worried\",\n",
    "                                                    \"frustrated\",\"anxious\",\n",
    "                                                    \"nervous\",\"disappointed\",\n",
    "                                                    \"gloomy\",\"miserable\",\"longly\"])\n",
    "        positive_before2 = lexicon.analyze(text, categories=[\"happy\", \"loved\", \"joyful\",\n",
    "                                                     \"content\", \"pleased\",\n",
    "                                                     \"grateful\", \"relieved\",\n",
    "                                                     \"optimistic\", \"satisfied\",\n",
    "                                                     \"excited\", \"loved\"])\n",
    "        for key, value in positive_before2.items():\n",
    "            df2.loc[key,'before']+=value\n",
    "        for key, value in negative_before2.items():\n",
    "            df2.loc[key,'before']+=value\n",
    "    for text in After2[i].title:\n",
    "        negative_after2 = lexicon.analyze(text, categories=[\"angry\", \"sad\", \"worried\",\n",
    "                                                             \"frustrated\", \"anxious\",\n",
    "                                                             \"nervous\", \"disappointed\",\n",
    "                                                             \"gloomy\", \"miserable\", \"longly\"])\n",
    "        positive_after2 = lexicon.analyze(text, categories=[\"happy\", \"loved\", \"joyful\",\n",
    "                                                             \"content\", \"pleased\",\n",
    "                                                             \"grateful\", \"relieved\",\n",
    "                                                             \"optimistic\", \"satisfied\",\n",
    "                                                             \"excited\"])\n",
    "\n",
    "        for key, value in positive_after2.items():\n",
    "            df2.loc[key,'after']+=value\n",
    "        for key, value in negative_after2.items():\n",
    "            df2.loc[key,'after']+=value\n",
    "    df2.difference=df2.before-df2.after\n",
    "    dic2[i]=df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "664a6fe2-7228-4037-a064-d12e2af51595",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "      <th>difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>angry</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sad</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worried</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frustrated</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anxious</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nervous</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointed</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gloomy</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>miserable</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>longly</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>happy</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loved</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joyful</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>content</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pleased</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grateful</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relieved</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimistic</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>satisfied</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excited</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              before  after  difference\n",
       "angry            0.0    0.0         0.0\n",
       "sad              0.0    0.0         0.0\n",
       "worried          1.0    2.0        -1.0\n",
       "frustrated       0.0    1.0        -1.0\n",
       "anxious          0.0    0.0         0.0\n",
       "nervous          0.0    2.0        -2.0\n",
       "disappointed     3.0    0.0         3.0\n",
       "gloomy           1.0    0.0         1.0\n",
       "miserable        1.0    0.0         1.0\n",
       "longly           0.0    0.0         0.0\n",
       "happy            0.0    0.0         0.0\n",
       "loved            0.0    0.0         0.0\n",
       "joyful           2.0    0.0         2.0\n",
       "content          1.0    1.0         0.0\n",
       "pleased          1.0    0.0         1.0\n",
       "grateful         1.0    0.0         1.0\n",
       "relieved         0.0    1.0        -1.0\n",
       "optimistic       1.0    1.0         0.0\n",
       "satisfied        0.0    0.0         0.0\n",
       "excited          1.0    0.0         1.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2d0aeb04-a8b6-46f3-aa54-6045eadcc708",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dic3={}\n",
    "for i in range(0,len(Before3)):\n",
    "    df3=pd.DataFrame(np.zeros((20,3)),columns=columns,index=index)\n",
    "    for text in Before3[i].title:\n",
    "        negative_before3 = lexicon.analyze(text, categories=[\"angry\",\"sad\",\"worried\",\n",
    "                                                    \"frustrated\",\"anxious\",\n",
    "                                                    \"nervous\",\"disappointed\",\n",
    "                                                    \"gloomy\",\"miserable\",\"longly\"])\n",
    "        positive_before3 = lexicon.analyze(text, categories=[\"happy\", \"loved\", \"joyful\",\n",
    "                                                     \"content\", \"pleased\",\n",
    "                                                     \"grateful\", \"relieved\",\n",
    "                                                     \"optimistic\", \"satisfied\",\n",
    "                                                     \"excited\", \"loved\"])\n",
    "        for key, value in positive_before3.items():\n",
    "            df3.loc[key,'before']+=value\n",
    "        for key, value in negative_before3.items():\n",
    "            df3.loc[key,'before']+=value\n",
    "    for text in After3[i].title:\n",
    "        negative_after3 = lexicon.analyze(text, categories=[\"angry\", \"sad\", \"worried\",\n",
    "                                                             \"frustrated\", \"anxious\",\n",
    "                                                             \"nervous\", \"disappointed\",\n",
    "                                                             \"gloomy\", \"miserable\", \"longly\"])\n",
    "        positive_after3 = lexicon.analyze(text, categories=[\"happy\", \"loved\", \"joyful\",\n",
    "                                                             \"content\", \"pleased\",\n",
    "                                                             \"grateful\", \"relieved\",\n",
    "                                                             \"optimistic\", \"satisfied\",\n",
    "                                                             \"excited\"])\n",
    "\n",
    "        for key, value in positive_after3.items():\n",
    "            df3.loc[key,'after']+=value\n",
    "        for key, value in negative_after3.items():\n",
    "            df3.loc[key,'after']+=value\n",
    "    df3.difference=df3.before-df3.after\n",
    "    dic3[i]=df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ee73cb58-0392-4024-bb8a-376f7c17dcda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "      <th>difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>angry</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sad</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worried</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frustrated</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anxious</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nervous</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointed</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gloomy</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>miserable</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>longly</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>happy</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loved</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joyful</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>content</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pleased</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grateful</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relieved</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimistic</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>satisfied</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excited</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              before  after  difference\n",
       "angry            0.0    0.0         0.0\n",
       "sad              0.0    0.0         0.0\n",
       "worried          1.0    0.0         1.0\n",
       "frustrated       1.0    0.0         1.0\n",
       "anxious          4.0    2.0         2.0\n",
       "nervous          0.0    1.0        -1.0\n",
       "disappointed     1.0    0.0         1.0\n",
       "gloomy           0.0    0.0         0.0\n",
       "miserable        4.0    0.0         4.0\n",
       "longly           0.0    0.0         0.0\n",
       "happy            0.0    0.0         0.0\n",
       "loved            0.0    0.0         0.0\n",
       "joyful           0.0    0.0         0.0\n",
       "content          0.0    0.0         0.0\n",
       "pleased          0.0    0.0         0.0\n",
       "grateful         0.0    0.0         0.0\n",
       "relieved         1.0    0.0         1.0\n",
       "optimistic       0.0    0.0         0.0\n",
       "satisfied        1.0    0.0         1.0\n",
       "excited          0.0    0.0         0.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic3[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1511a9a3-80ad-4ba5-a4f2-4749cf5697c5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dic0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m converted_dic0 \u001b[38;5;241m=\u001b[39m {key: df\u001b[38;5;241m.\u001b[39mto_dict(orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m key, df \u001b[38;5;129;01min\u001b[39;00m dic0\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdic0.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[1;32m      4\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(converted_dic0, json_file)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dic0' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "converted_dic0 = {key: df.to_dict(orient='split') for key, df in dic0.items()}\n",
    "with open('dic0.json', 'w') as json_file:\n",
    "    json.dump(converted_dic0, json_file)\n",
    "converted_dic1 = {key: df.to_dict(orient='split') for key, df in dic1.items()}\n",
    "with open('dic1.json', 'w') as json_file:\n",
    "    json.dump(converted_dic0, json_file)\n",
    "converted_dic2 = {key: df.to_dict(orient='split') for key, df in dic2.items()}\n",
    "with open('dic2.json', 'w') as json_file:\n",
    "    json.dump(converted_dic0, json_file)\n",
    "converted_dic3 = {key: df.to_dict(orient='split') for key, df in dic3.items()}\n",
    "with open('dic3.json', 'w') as json_file:\n",
    "    json.dump(converted_dic0, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d8203bc-c62e-4f3f-bd01-289ba9299259",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('dic0.json', 'r') as json_file:\n",
    "    dic = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9513c4f1-9287-49b9-b1e4-da924db37074",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': ['angry',\n",
       "  'sad',\n",
       "  'worried',\n",
       "  'frustrated',\n",
       "  'anxious',\n",
       "  'nervous',\n",
       "  'disappointed',\n",
       "  'gloomy',\n",
       "  'miserable',\n",
       "  'longly',\n",
       "  'happy',\n",
       "  'loved',\n",
       "  'joyful',\n",
       "  'content',\n",
       "  'pleased',\n",
       "  'grateful',\n",
       "  'relieved',\n",
       "  'optimistic',\n",
       "  'satisfied',\n",
       "  'excited'],\n",
       " 'columns': ['before', 'after', 'difference'],\n",
       " 'data': [[0.0, 0.0, 0.0],\n",
       "  [1.0, 0.0, 1.0],\n",
       "  [0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0],\n",
       "  [0.0, 1.0, -1.0],\n",
       "  [1.0, 1.0, 0.0],\n",
       "  [0.0, 0.0, 0.0],\n",
       "  [1.0, 0.0, 1.0],\n",
       "  [0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0],\n",
       "  [0.0, 1.0, -1.0],\n",
       "  [0.0, 0.0, 0.0],\n",
       "  [0.0, 1.0, -1.0],\n",
       "  [0.0, 1.0, -1.0],\n",
       "  [0.0, 0.0, 0.0],\n",
       "  [1.0, 0.0, 1.0],\n",
       "  [0.0, 1.0, -1.0],\n",
       "  [0.0, 0.0, 0.0],\n",
       "  [0.0, 1.0, -1.0]]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic['2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "276e3c9d-3251-4343-9e4f-409fc9e14022",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data0=pd.DataFrame(np.zeros((20,3)),columns=columns,index=index)\n",
    "data1=pd.DataFrame(np.zeros((20,3)),columns=columns,index=index)\n",
    "data2=pd.DataFrame(np.zeros((20,3)),columns=columns,index=index)\n",
    "data3=pd.DataFrame(np.zeros((20,3)),columns=columns,index=index)\n",
    "for key in dic0:\n",
    "    data0+=dic0[key]\n",
    "for key in dic1:\n",
    "    data1+=dic1[key]\n",
    "for key in dic2:\n",
    "    data2+=dic2[key]\n",
    "for key in dic3:\n",
    "    data3+=dic3[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cbc8a116-b78c-41c5-9ad3-7a828d60191b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "      <th>difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>angry</th>\n",
       "      <td>13.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sad</th>\n",
       "      <td>41.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worried</th>\n",
       "      <td>390.0</td>\n",
       "      <td>430.0</td>\n",
       "      <td>-40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frustrated</th>\n",
       "      <td>230.0</td>\n",
       "      <td>266.0</td>\n",
       "      <td>-36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anxious</th>\n",
       "      <td>144.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nervous</th>\n",
       "      <td>294.0</td>\n",
       "      <td>334.0</td>\n",
       "      <td>-40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointed</th>\n",
       "      <td>288.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>-7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gloomy</th>\n",
       "      <td>98.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>-32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>miserable</th>\n",
       "      <td>185.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>-18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>longly</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>happy</th>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loved</th>\n",
       "      <td>784.0</td>\n",
       "      <td>533.0</td>\n",
       "      <td>251.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joyful</th>\n",
       "      <td>203.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>-8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>content</th>\n",
       "      <td>96.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pleased</th>\n",
       "      <td>230.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>-81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grateful</th>\n",
       "      <td>285.0</td>\n",
       "      <td>366.0</td>\n",
       "      <td>-81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relieved</th>\n",
       "      <td>270.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>-8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimistic</th>\n",
       "      <td>112.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>-66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>satisfied</th>\n",
       "      <td>387.0</td>\n",
       "      <td>443.0</td>\n",
       "      <td>-56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excited</th>\n",
       "      <td>268.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>-26.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              before  after  difference\n",
       "angry           13.0   30.0       -17.0\n",
       "sad             41.0   34.0         7.0\n",
       "worried        390.0  430.0       -40.0\n",
       "frustrated     230.0  266.0       -36.0\n",
       "anxious        144.0  140.0         4.0\n",
       "nervous        294.0  334.0       -40.0\n",
       "disappointed   288.0  295.0        -7.0\n",
       "gloomy          98.0  130.0       -32.0\n",
       "miserable      185.0  203.0       -18.0\n",
       "longly           0.0    0.0         0.0\n",
       "happy           80.0  100.0       -20.0\n",
       "loved          784.0  533.0       251.0\n",
       "joyful         203.0  211.0        -8.0\n",
       "content         96.0   83.0        13.0\n",
       "pleased        230.0  311.0       -81.0\n",
       "grateful       285.0  366.0       -81.0\n",
       "relieved       270.0  278.0        -8.0\n",
       "optimistic     112.0  178.0       -66.0\n",
       "satisfied      387.0  443.0       -56.0\n",
       "excited        268.0  294.0       -26.0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "48d66533-30a3-4313-8f9b-e94a94260211",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('influence.xlsx') as writer:\n",
    "    data0.to_excel(writer, sheet_name='Sheet0')\n",
    "    data1.to_excel(writer, sheet_name='Sheet1')\n",
    "    data2.to_excel(writer, sheet_name='Sheet2')\n",
    "    data3.to_excel(writer, sheet_name='Sheet3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acbc467-ed15-44e2-9789-c16b8c4fc91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
