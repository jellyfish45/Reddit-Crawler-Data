{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of What raw_data Outputs Look Like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put into this a list of usernames to get the data for \n",
    "users_get_data_for = [\"Pokechu22\", \"13steinj\"]\n",
    "\n",
    "#if you are using this to gather data for a group of people above set this to true. Otherwise it will scrape random users data to use for matching.\n",
    "just_get_for_them = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset description\n",
    "\"\"\"\n",
    "Redditor Instantiation\n",
    "- load all data into memory first (static) \n",
    "\n",
    "get_base_data(): return redditor[data]\n",
    "get_comment_data()\n",
    "get_submission_data() \n",
    "//some kind of store value w/ state (seperate dict)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "//all should contain stuff like a date scraped\n",
    "Redditor_Dict:\n",
    "Username ->  {\n",
    "    \"data\": properties + labels in dict \n",
    "    \"comments\": [\n",
    "        (comment date, {\"subreddit_id\" : subreddit_id, \"submission_id\": submission_id, \"comment_id\": comment_id})\n",
    "        # (comment date, subreddit_id, .submission.id, {dictionary of all info})\n",
    "        //dict of all info should be initialized seperately\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "[properties + labels below, comments: {relationship data + [\"post_id\" :id of post commented on [comment.subreddit_id]]}]\n",
    "#what relevant edge data do we need?\n",
    "\n",
    "\n",
    "SubmissionDict\n",
    "{\n",
    "    submission.id:  {\n",
    "        \"created_utc\"\n",
    "        \"distinguished\" \n",
    "        \"is_original_content\" \n",
    "        \"over_18\" \n",
    "        \"score\" \n",
    "        \"title\"\n",
    "        \"upvote_ratio\" \n",
    "        \"users_commented_on\": submission.comments.replace_more(None).list() | None\n",
    "        \"comment_data\": {\"user\": comment.author.id, \"id\": comment.id} | None\n",
    "    }\n",
    "}\n",
    "\n",
    "CommentDict {\n",
    "    submission_id + \"--------\" + comment_id : { \n",
    "        \"data\": {\n",
    "            \"is_edited\": comment.edited,\n",
    "            \"link_title\": comment.link_title, \n",
    "            \"num_replies\": len(comment.replies.list()),\n",
    "            \"score\": comment.score, \n",
    "            \"score_is_hidden\": comment.score_hidden,\n",
    "            \"total_awards\": comment.total_awards_received, \n",
    "            \"num_ups\": comment.ups, \n",
    "            \"num_downs\": comment.downs, \n",
    "            \"body\": comment.body,\n",
    "            \"date\": comment.created_utc,\n",
    "            \"is_submitter\": comment.is_submitter, \n",
    "            \"stickied\": comment.stickied\n",
    "            \n",
    "        },\n",
    "        \"meta\": {\n",
    "                \"ITERATION_NUM\": ITERATION_NUM, \n",
    "                \"TIME\": int(time.time()),\n",
    "                \"submission_id\": submission_id, \n",
    "                \"comment_id\": comment.id,\n",
    "                \"author\": redditor.name\n",
    "        }\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw \n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"CL5xizTtujKZc4HOuFF_5w\",\n",
    "    client_secret=\"mD9JbzEVNFB8xopQKE_2WTcyIaz0hg\",\n",
    "    user_agent=\"logan-vaz\"\n",
    ")\n",
    "\n",
    "#reddit secret mD9JbzEVNFB8xopQKE_2WTcyIaz0hg, user_agen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERATION_NUM = 1\n",
    "import time \n",
    "\n",
    "def get_redditor_data(redditor: praw.reddit):\n",
    "    \"\"\"get all data from a praw redditor object\"\"\"\n",
    "\n",
    "    submission_ids = list() \n",
    "\n",
    "    to_ret = dict()\n",
    "    to_ret[\"username\"] = redditor.name\n",
    "    to_ret[\"comment_karma\"] = redditor.comment_karma\n",
    "    # to_ret[\"num_comments\"] = len(redditor.comments.top())\n",
    "    to_ret[\"time_creation\"] = redditor.created_utc\n",
    "    to_ret[\"verified_email\"] = redditor.has_verified_email\n",
    "    to_ret[\"is_employee\"] = redditor.is_employee\n",
    "    to_ret[\"is_mod\"] = redditor.is_mod\n",
    "    to_ret[\"is_gold\"] = redditor.is_gold \n",
    "    to_ret[\"is_suspended\"] = False\n",
    "    to_ret[\"link_karma\"] = redditor.link_karma\n",
    "    to_ret[\"num_moderated\"] = len(redditor.moderated())\n",
    "    to_ret[\"num_multireddits\"] = len(redditor.multireddits())\n",
    "    to_ret[\"num_trophies\"] = len(redditor.trophies())\n",
    "\n",
    "    if (redditor.subreddit):\n",
    "        to_ret[\"has_subreddit\"] = True\n",
    "        to_ret[\"over_18\"] = redditor.subreddit.over_18#redditor.subreddit[\"over_18\"]\n",
    "        to_ret[\"num_subscribers\"] = redditor.subreddit.subscribers#redditor.subreddit[\"subscribers\"]\n",
    "\n",
    "        #this is the only one added that rlly shouldn't be\n",
    "        to_ret[\"public_description\"] = redditor.subreddit.public_description\n",
    "    else: to_ret[\"has_subreddit\"] = False  \n",
    "    \n",
    "    to_ret = {\"data\": to_ret} \n",
    "\n",
    "    comment_data_user = list() \n",
    "    comment_data_dict = dict()\n",
    "\n",
    "    comment_cnt = 0 \n",
    "    for comment in redditor.comments.new():\n",
    "        print(\"comment is\", comment_cnt)\n",
    "        comment_cnt += 1\n",
    "        submission_id = comment.link_id \n",
    "        submission_ids.append(submission_id)\n",
    "        submission_id =comment.submission.id\n",
    "\n",
    "        # print(\"Created date is \" , comment.created_utc)\n",
    "        comment_data_user.append([int(comment.created_utc), {\"subreddit_id\" : comment.subreddit_id, \"submission_id\": submission_id, \"comment_id\": comment.id}])\n",
    "\n",
    "        comment_key = submission_id + \"--------\" + comment.id\n",
    "        comment_data = {\n",
    "\n",
    "            \"data\": {\n",
    "                \"is_edited\": comment.edited,\n",
    "                \"link_title\": comment.link_title, \n",
    "                \"num_replies\": len(comment.replies.list()),\n",
    "                \"score\": comment.score, \n",
    "                \"score_is_hidden\": comment.score_hidden,\n",
    "                \"total_awards\": comment.total_awards_received, \n",
    "                \"num_ups\": comment.ups, \n",
    "                \"num_downs\": comment.downs, \n",
    "                \"body\": comment.body,\n",
    "                \"date\": comment.created_utc,\n",
    "                \"is_submitter\": comment.is_submitter, \n",
    "                \"stickied\": comment.stickied\n",
    "                \n",
    "            },\n",
    "            \"meta\": {\n",
    "                 \"ITERATION_NUM\": ITERATION_NUM, \n",
    "                 \"TIME\": int(time.time()),\n",
    "                 \"submission_id\": submission_id, \n",
    "                 \"comment_id\": comment.id,\n",
    "                 \"author\": redditor.name\n",
    "            }\n",
    "            \n",
    "        }\n",
    "        comment_data_dict[comment_key] = comment_data\n",
    "    \n",
    "    to_ret[\"comments\"] = comment_data_user \n",
    "    to_ret[\"meta\"] = {\n",
    "        \"ITERATION_NUM\": ITERATION_NUM, \n",
    "        \"TIME\": int(time.time())\n",
    "    }\n",
    "    print(\"returning triplet\")\n",
    "    return to_ret, comment_data_dict, submission_ids\n",
    "\n",
    "\n",
    "\n",
    "# def get_submission_data():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prawcore #required for exception handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(listed):\n",
    "    \"\"\"from a list of morecomments and comments extract comments, implements backoff\"\"\"\n",
    "    to_ret = list()\n",
    "    for l in listed:\n",
    "        if (isinstance(l, praw.models.MoreComments)):\n",
    "            time.sleep(0.05)\n",
    "            try:\n",
    "                \n",
    "                base_comments = get_comments(l.comments())\n",
    "            except prawcore.exceptions.TooManyRequests as e:\n",
    "                print(\"backoff 1\")\n",
    "                time.sleep(10)\n",
    "                try:\n",
    "                    base_comments = get_comments(l.comments())\n",
    "                except prawcore.exceptions.TooManyRequests as e:\n",
    "                    print(\"backoff 2\")\n",
    "                    time.sleep(30)\n",
    "                    base_comments = get_comments(l.comments())\n",
    "            to_ret += base_comments \n",
    "        else:\n",
    "            to_ret.append(l)\n",
    "    return to_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/842557/how-to-prevent-a-block-of-code-from-being-interrupted-by-keyboardinterrupt-in-py\n",
    "#interrupt handler\n",
    "import signal\n",
    "import logging\n",
    "\n",
    "class DelayedKeyboardInterrupt:\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.signal_received = False\n",
    "        self.old_handler = signal.signal(signal.SIGINT, self.handler)\n",
    "                \n",
    "    def handler(self, sig, frame):\n",
    "        self.signal_received = (sig, frame)\n",
    "        logging.debug('SIGINT received. Delaying KeyboardInterrupt.')\n",
    "    \n",
    "    def __exit__(self, type, value, traceback):\n",
    "        signal.signal(signal.SIGINT, self.old_handler)\n",
    "        if self.signal_received:\n",
    "            self.old_handler(*self.signal_received)\n",
    "\n",
    "\n",
    "new_authors = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hottest_submissions = list()\n",
    "# for submission in reddit.subreddit(\"depression\").hot():\n",
    "#     # print(dir(submission))\n",
    "#     # print(\"subreddit_subscribers\", submission.subreddit_subscribers)\n",
    "#     # print(\"comments \", submission.comments)\n",
    "#     # print(\"author_fullname\", submission.author_fullname)\n",
    "#     # print(\"author\", submission.author)\n",
    "\n",
    "#     # break\n",
    "#     hottest_submissions.append(submission)\n",
    "\n",
    "# new_authors = [h.author.name for h in hottest_submissions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import os\n",
    "REDDITOR_FILE = \"raw_data/redditor_dict.json\"\n",
    "SUBMISSION_FILE = \"raw_data/submission_dict.json\"\n",
    "COMMENT_FILE = \"raw_data/comment_dict.json\"\n",
    "\n",
    "def save_get_json(file_name):\n",
    "    if (os.path.exists(file_name)):\n",
    "        with open(file_name, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return dict()\n",
    "\n",
    "\n",
    "redditor_dict = save_get_json(REDDITOR_FILE)\n",
    "submission_dict = save_get_json(SUBMISSION_FILE)\n",
    "comment_dict = save_get_json(COMMENT_FILE)\n",
    "\n",
    "print(\"heads up this takes a while\")\n",
    "\n",
    "print(\"len of redditor_dict is \", len(redditor_dict))\n",
    "print(\"len of submission_dict is \", len(submission_dict))\n",
    "print(\"len of comment_dict is \", len(comment_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of ppl to try is  300315\n",
      "chunk len is  0\n",
      "chunk len is  2\n",
      "group is  control\n",
      "USER IS  Pokechu22\n",
      "comment is 0\n",
      "comment is 1\n",
      "comment is 2\n",
      "comment is 3\n",
      "comment is 4\n",
      "comment is 5\n",
      "comment is 6\n",
      "comment is 7\n",
      "comment is 8\n",
      "comment is 9\n",
      "comment is 10\n",
      "comment is 11\n",
      "comment is 12\n",
      "comment is 13\n",
      "comment is 14\n",
      "comment is 15\n",
      "comment is 16\n",
      "comment is 17\n",
      "comment is 18\n",
      "comment is 19\n",
      "comment is 20\n",
      "comment is 21\n",
      "comment is 22\n",
      "comment is 23\n",
      "comment is 24\n",
      "comment is 25\n",
      "comment is 26\n",
      "comment is 27\n",
      "comment is 28\n",
      "comment is 29\n",
      "comment is 30\n",
      "comment is 31\n",
      "comment is 32\n",
      "comment is 33\n",
      "comment is 34\n",
      "comment is 35\n",
      "comment is 36\n",
      "comment is 37\n",
      "comment is 38\n",
      "comment is 39\n",
      "comment is 40\n",
      "comment is 41\n",
      "comment is 42\n",
      "comment is 43\n",
      "comment is 44\n",
      "comment is 45\n",
      "comment is 46\n",
      "comment is 47\n",
      "comment is 48\n",
      "comment is 49\n",
      "comment is 50\n",
      "comment is 51\n",
      "comment is 52\n",
      "comment is 53\n",
      "comment is 54\n",
      "comment is 55\n",
      "comment is 56\n",
      "comment is 57\n",
      "comment is 58\n",
      "comment is 59\n",
      "comment is 60\n",
      "comment is 61\n",
      "comment is 62\n",
      "comment is 63\n",
      "comment is 64\n",
      "comment is 65\n",
      "comment is 66\n",
      "comment is 67\n",
      "comment is 68\n",
      "comment is 69\n",
      "comment is 70\n",
      "comment is 71\n",
      "comment is 72\n",
      "comment is 73\n",
      "comment is 74\n",
      "comment is 75\n",
      "comment is 76\n",
      "comment is 77\n",
      "comment is 78\n",
      "comment is 79\n",
      "comment is 80\n",
      "comment is 81\n",
      "comment is 82\n",
      "comment is 83\n",
      "comment is 84\n",
      "comment is 85\n",
      "comment is 86\n",
      "comment is 87\n",
      "comment is 88\n",
      "comment is 89\n",
      "comment is 90\n",
      "comment is 91\n",
      "comment is 92\n",
      "comment is 93\n",
      "comment is 94\n",
      "comment is 95\n",
      "comment is 96\n",
      "comment is 97\n",
      "comment is 98\n",
      "comment is 99\n",
      "returning triplet\n",
      "matched id\n",
      "matched id\n",
      "sub cnt is 10\n",
      "matched id\n",
      "matched id\n",
      "sub cnt is 20\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "sub cnt is 30\n",
      "matched id\n",
      "matched id\n",
      "sub cnt is 40\n",
      "matched id\n",
      "matched id\n",
      "sub cnt is 50\n",
      "matched id\n",
      "matched id\n",
      "sub cnt is 60\n",
      "matched id\n",
      "sub cnt is 70\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "sub cnt is 80\n",
      "matched id\n",
      "matched id\n",
      "sub cnt is 90\n",
      "sub cnt is 100\n",
      "updating dict\n",
      "group is  control\n",
      "USER IS  13steinj\n",
      "comment is 0\n",
      "comment is 1\n",
      "comment is 2\n",
      "comment is 3\n",
      "comment is 4\n",
      "comment is 5\n",
      "comment is 6\n",
      "comment is 7\n",
      "comment is 8\n",
      "comment is 9\n",
      "comment is 10\n",
      "comment is 11\n",
      "comment is 12\n",
      "comment is 13\n",
      "comment is 14\n",
      "comment is 15\n",
      "comment is 16\n",
      "comment is 17\n",
      "comment is 18\n",
      "comment is 19\n",
      "comment is 20\n",
      "comment is 21\n",
      "comment is 22\n",
      "comment is 23\n",
      "comment is 24\n",
      "comment is 25\n",
      "comment is 26\n",
      "comment is 27\n",
      "comment is 28\n",
      "comment is 29\n",
      "comment is 30\n",
      "comment is 31\n",
      "comment is 32\n",
      "comment is 33\n",
      "comment is 34\n",
      "comment is 35\n",
      "comment is 36\n",
      "comment is 37\n",
      "comment is 38\n",
      "comment is 39\n",
      "comment is 40\n",
      "comment is 41\n",
      "comment is 42\n",
      "comment is 43\n",
      "comment is 44\n",
      "comment is 45\n",
      "comment is 46\n",
      "comment is 47\n",
      "comment is 48\n",
      "comment is 49\n",
      "comment is 50\n",
      "comment is 51\n",
      "comment is 52\n",
      "comment is 53\n",
      "comment is 54\n",
      "comment is 55\n",
      "comment is 56\n",
      "comment is 57\n",
      "comment is 58\n",
      "comment is 59\n",
      "comment is 60\n",
      "comment is 61\n",
      "comment is 62\n",
      "comment is 63\n",
      "comment is 64\n",
      "comment is 65\n",
      "comment is 66\n",
      "comment is 67\n",
      "comment is 68\n",
      "comment is 69\n",
      "comment is 70\n",
      "comment is 71\n",
      "comment is 72\n",
      "comment is 73\n",
      "comment is 74\n",
      "comment is 75\n",
      "comment is 76\n",
      "comment is 77\n",
      "comment is 78\n",
      "comment is 79\n",
      "comment is 80\n",
      "comment is 81\n",
      "comment is 82\n",
      "comment is 83\n",
      "comment is 84\n",
      "comment is 85\n",
      "comment is 86\n",
      "comment is 87\n",
      "comment is 88\n",
      "comment is 89\n",
      "comment is 90\n",
      "comment is 91\n",
      "comment is 92\n",
      "comment is 93\n",
      "comment is 94\n",
      "comment is 95\n",
      "comment is 96\n",
      "comment is 97\n",
      "comment is 98\n",
      "comment is 99\n",
      "returning triplet\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "sub cnt is 10\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "sub cnt is 20\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "sub cnt is 30\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "sub cnt is 40\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "sub cnt is 50\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "sub cnt is 60\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "sub cnt is 70\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "sub cnt is 80\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "sub cnt is 90\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "matched id\n",
      "sub cnt is 100\n",
      "matched id\n",
      "updating dict\n",
      "dont interrupt\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[WinError 5] Access is denied.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\logan\\OneDrive\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\Dataset Expansion\\data_collection.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/logan/OneDrive/Documents/Research/Mental_Health/Reddit_Data_Scraping/Reddit-Crawler-Data/Dataset%20Expansion/data_collection.ipynb#X13sZmlsZQ%3D%3D?line=158'>159</a>\u001b[0m time\u001b[39m.\u001b[39msleep(\u001b[39m0.125\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/logan/OneDrive/Documents/Research/Mental_Health/Reddit_Data_Scraping/Reddit-Crawler-Data/Dataset%20Expansion/data_collection.ipynb#X13sZmlsZQ%3D%3D?line=159'>160</a>\u001b[0m \u001b[39m#with open(SUBMISSION_FILE, 'w') as submission_f:\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/logan/OneDrive/Documents/Research/Mental_Health/Reddit_Data_Scraping/Reddit-Crawler-Data/Dataset%20Expansion/data_collection.ipynb#X13sZmlsZQ%3D%3D?line=160'>161</a>\u001b[0m \u001b[39mwith\u001b[39;49;00m atomic_write(SUBMISSION_FILE, overwrite\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m) \u001b[39mas\u001b[39;49;00m submission_f:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/logan/OneDrive/Documents/Research/Mental_Health/Reddit_Data_Scraping/Reddit-Crawler-Data/Dataset%20Expansion/data_collection.ipynb#X13sZmlsZQ%3D%3D?line=161'>162</a>\u001b[0m     json\u001b[39m.\u001b[39;49mdump(submission_dict, submission_f) \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/logan/OneDrive/Documents/Research/Mental_Health/Reddit_Data_Scraping/Reddit-Crawler-Data/Dataset%20Expansion/data_collection.ipynb#X13sZmlsZQ%3D%3D?line=162'>163</a>\u001b[0m \u001b[39m# with open(COMMENT_FILE, 'w') as comment_f:\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\contextlib.py:144\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[39mif\u001b[39;00m typ \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m         \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgen)\n\u001b[0;32m    145\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\logan\\OneDrive\\Documents\\Research\\Mental_Health\\.venv\\Lib\\site-packages\\atomicwrites\\__init__.py:172\u001b[0m, in \u001b[0;36mAtomicWriter._open\u001b[1;34m(self, get_fileobject)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[39myield\u001b[39;00m f\n\u001b[0;32m    171\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msync(f)\n\u001b[1;32m--> 172\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcommit(f)\n\u001b[0;32m    173\u001b[0m     success \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\logan\\OneDrive\\Documents\\Research\\Mental_Health\\.venv\\Lib\\site-packages\\atomicwrites\\__init__.py:205\u001b[0m, in \u001b[0;36mAtomicWriter.commit\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''Move the temporary file to the target location.'''\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_overwrite:\n\u001b[1;32m--> 205\u001b[0m     replace_atomic(f\u001b[39m.\u001b[39;49mname, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_path)\n\u001b[0;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     move_atomic(f\u001b[39m.\u001b[39mname, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path)\n",
      "File \u001b[1;32mc:\\Users\\logan\\OneDrive\\Documents\\Research\\Mental_Health\\.venv\\Lib\\site-packages\\atomicwrites\\__init__.py:99\u001b[0m, in \u001b[0;36mreplace_atomic\u001b[1;34m(src, dst)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreplace_atomic\u001b[39m(src, dst):\n\u001b[0;32m     92\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[39m    Move ``src`` to ``dst``. If ``dst`` exists, it will be silently\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[39m    overwritten.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[39m    atomic.\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m     \u001b[39mreturn\u001b[39;00m _replace_atomic(src, dst)\n",
      "File \u001b[1;32mc:\\Users\\logan\\OneDrive\\Documents\\Research\\Mental_Health\\.venv\\Lib\\site-packages\\atomicwrites\\__init__.py:79\u001b[0m, in \u001b[0;36m_replace_atomic\u001b[1;34m(src, dst)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_replace_atomic\u001b[39m(src, dst):\n\u001b[1;32m---> 79\u001b[0m     _handle_errors(windll\u001b[39m.\u001b[39;49mkernel32\u001b[39m.\u001b[39;49mMoveFileExW(\n\u001b[0;32m     80\u001b[0m         _path_to_unicode(src), _path_to_unicode(dst),\n\u001b[0;32m     81\u001b[0m         _windows_default_flags \u001b[39m|\u001b[39;49m _MOVEFILE_REPLACE_EXISTING\n\u001b[0;32m     82\u001b[0m     ))\n",
      "File \u001b[1;32mc:\\Users\\logan\\OneDrive\\Documents\\Research\\Mental_Health\\.venv\\Lib\\site-packages\\atomicwrites\\__init__.py:76\u001b[0m, in \u001b[0;36m_handle_errors\u001b[1;34m(rv)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_handle_errors\u001b[39m(rv):\n\u001b[0;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m rv:\n\u001b[1;32m---> 76\u001b[0m         \u001b[39mraise\u001b[39;00m WinError()\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 5] Access is denied."
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "import time    \n",
    "import random\n",
    "from atomicwrites import atomic_write\n",
    "\n",
    "import numpy as np \n",
    "epoch_time = int(time.time())\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "#first, let's just fill with the treatment group - by name\n",
    "treatment_group = ['MoosieGoose', 'JollyK9', 'Southern_Ad3032', 'bduwowy272habbw', 'Late_Introduction203', 'kapster68', 'TheApertureMonkey', 'talemoon22', 'sebagolindenwald', 'spicyranchplzz', 'TheFloorMayBeLava_02', 'rxtten_flesh', 'greenblooded395', 'greenblooded395', 'DrakenJosh98', 'WhichUsernameIsBest', 'FStahp2', 'Pongpianskul', 'Kanashimi515', 'eviuwu', 'Kattheloner_22', 'Reeze2911', 'Sac20000', 'RanpoWasTaken', 'jlynny1811', 'Playful-Fail4778', 'GarageOk8109', 'katandcats', 'holyredemption', 'jifpeanutbutter420', 'Timely_Inflation1000', 'Erica_Peanut']\n",
    "\n",
    "# treatment_group += new_authors\n",
    "#get potential treatment groups from those that comment on at least one of the same submission\n",
    "control_explore = [comment_data[\"user\"] for submission in submission_dict.values() if submission[\"comment_data\"] for comment_data in submission[\"comment_data\"]]\n",
    "control_explore = [c for c in control_explore if not c in redditor_dict]\n",
    "random.seed(24601)\n",
    "random.shuffle(control_explore)\n",
    "\n",
    "print(\"length of ppl to try is \", len(control_explore))\n",
    "\n",
    "\n",
    "if (just_get_for_them):\n",
    "    treatment_group = list()\n",
    "    control_explore = users_get_data_for\n",
    "\n",
    "\n",
    "users_and_groups = [(v, \"control\") for v in control_explore]\n",
    "users_and_groups_chunks = np.array_split([(v, \"treatment\") for v in treatment_group], max(int(len(treatment_group)/16), 1)) +  np.array_split(users_and_groups, max(int(len(users_and_groups)/256), 1))\n",
    "\n",
    "\n",
    "#was trying to batch requests - doesn't really appear possible so we're instead doing this to hopefully avoid rate limits\n",
    "for user_group_chunk in users_and_groups_chunks:\n",
    "    print(\"chunk len is \" ,len(user_group_chunk) )\n",
    "    \n",
    "    something_changed = False\n",
    "    try:\n",
    "        #get the basic user information \n",
    "        \n",
    "        for user_name, group in user_group_chunk:\n",
    "            time.sleep(.05)\n",
    "            print(\"group is \", group)\n",
    "            print(\"USER IS \", user_name)\n",
    "            if (user_name in redditor_dict.keys()): \n",
    "                print(\"skipping\")\n",
    "                \n",
    "                continue\n",
    "            # print(\"continuing\")\n",
    "            redditor =  reddit.redditor(user_name)\n",
    "            try:\n",
    "                user_data, comment_data, submission_ids = get_redditor_data(redditor) \n",
    "            except AttributeError as E:\n",
    "                print(\"Missing is \", E)\n",
    "                continue\n",
    "\n",
    "            something_changed = True\n",
    "\n",
    "            #time to get all relevant submission data (TODO - this is how we'll add users - begging the question of the order to add them in but we'll deal w/ that later)\n",
    "            this_submission_data = dict()\n",
    "            cnt = 0 \n",
    "            submissions_fetch = [sub_id for sub_id in submission_ids if not (sub_id in submission_dict or sub_id in this_submission_data)]\n",
    "            submissions = reddit.info(fullnames=submissions_fetch)\n",
    "            for submission in submissions:\n",
    "                cnt += 1\n",
    "                if (cnt % 10 ==0): print(f'sub cnt is {cnt}') \n",
    "                submission_id = submission.id\n",
    "                if (submission_id in submission_dict or submission_id in this_submission_data): \n",
    "                    print(\"matched id\")\n",
    "                    continue \n",
    "\n",
    "                #get comment data for this submission - if treatment we want to extract more data\n",
    "                if (group == \"treatment\"):\n",
    "                    commentors = dict() \n",
    "                    sub_comment_data = list()\n",
    "                    time.sleep(0.125/2)\n",
    "                    try:\n",
    "                        all_comments = submission.comments.list()\n",
    "                    except prawcore.exceptions.TooManyRequests as E:\n",
    "                        print(\"backoff 1\")\n",
    "                        time.sleep(10)\n",
    "                        try:\n",
    "                            all_comments = submission.comments.list()\n",
    "                        except prawcore.exceptions.TooManyRequests as E:\n",
    "                            print(\"backoff 2\")\n",
    "                            time.sleep(30)\n",
    "                            all_comments = submission.comments.list()\n",
    "\n",
    "                    original_comments = [c for c in all_comments if not isinstance(c, praw.models.MoreComments)]\n",
    "                    layered_comments = get_comments([c for c in all_comments if isinstance(c, praw.models.MoreComments)])\n",
    "\n",
    "                    for comment in original_comments:\n",
    "                        if (comment.author and comment.author not in [\"None\", \"[removed]\", \"[deleted]\"] and comment.author.name):\n",
    "                            commentors[comment.author.name] = True\n",
    "                            sub_comment_data.append( {\n",
    "                                \"user\": comment.author.name,\n",
    "                                \"id\": comment.id,\n",
    "                                \"original_reply\": True\n",
    "                            })\n",
    "                    for comment in layered_comments:\n",
    "                        # submission.author != \"None\" or \"[removed]\" or \"[deleted]\"\n",
    "                        print(\"comment author is \", comment.author)\n",
    "                        if (comment.author and comment.author not in [\"None\", \"[removed]\", \"[deleted]\"] and comment.author.name):\n",
    "                            commentors[comment.author.name] = True\n",
    "                            sub_comment_data.append( {\n",
    "                                \"user\": comment.author.name,\n",
    "                                \"id\": comment.id,\n",
    "                                \"original_reply\": False\n",
    "                            })\n",
    "                    this_submission_data[submission_id] = {\n",
    "                        \"created_utc\": int(submission.created_utc),\n",
    "                        \"distinguished\": submission.distinguished,\n",
    "                        \"is_original_content\" : submission.is_original_content,\n",
    "                        \"over_18\": submission.over_18,\n",
    "                        \"score\": submission.score, \n",
    "                        \"title\": submission.title,\n",
    "                        \"edited\": submission.edited, \n",
    "                        \"selftext\": submission.selftext,\n",
    "                        \"upvote_ratio\": submission.upvote_ratio,\n",
    "                        \"users_commented_on\": list(commentors.keys()),\n",
    "                        \"comment_data\":sub_comment_data\n",
    "                    }\n",
    "                else:\n",
    "                    this_submission_data[submission_id] = {\n",
    "                        \"created_utc\": int(submission.created_utc),\n",
    "                        \"distinguished\": submission.distinguished,\n",
    "                        \"is_original_content\" : submission.is_original_content,\n",
    "                        \"over_18\": submission.over_18,\n",
    "                        \"score\": submission.score, \n",
    "                        \"title\": submission.title,\n",
    "                        \"edited\": submission.edited, \n",
    "                        \"selftext\": submission.selftext,\n",
    "                        \"upvote_ratio\": submission.upvote_ratio,\n",
    "                        \"users_commented_on\": None,\n",
    "                        \"comment_data\":None\n",
    "                    }\n",
    "\n",
    "            print(\"updating dict\")\n",
    "            redditor_dict.update({user_name: user_data})\n",
    "            submission_dict.update(this_submission_data)\n",
    "            comment_dict.update(comment_data)\n",
    "    \n",
    "    except KeyboardInterrupt as E:\n",
    "        #if it's keyboard let's still try n save what we have\n",
    "        print(\"trying to save what have\")\n",
    "        \n",
    "    except Exception as E:\n",
    "        traceback.print_exc()\n",
    "        print(\"Exception is \", E)\n",
    "        time.sleep(32) \n",
    "    if (something_changed):\n",
    "        with DelayedKeyboardInterrupt():\n",
    "            print(\"dont interrupt - heads up this can take a minute\")\n",
    "            #with open(SUBMISSION_FILE, 'w') as submission_f:\n",
    "            with atomic_write(SUBMISSION_FILE, overwrite=True) as submission_f:\n",
    "                json.dump(submission_dict, submission_f) \n",
    "            # with open(COMMENT_FILE, 'w') as comment_f:\n",
    "            with atomic_write(COMMENT_FILE, overwrite=True) as comment_f:\n",
    "                json.dump(comment_dict,comment_f )\n",
    "            # with open(REDDITOR_FILE, 'w') as redditor_f:\n",
    "            with atomic_write(REDDITOR_FILE, overwrite=True) as redditor_f:\n",
    "                json.dump(redditor_dict,redditor_f )\n",
    "\n",
    "        print(\"Saved files\")\n",
    "        # time.sleep(0.5)\n",
    "        time.sleep(random.random()*3+.5)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "end\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'redditor_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\logan\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\Dataset Expansion\\data_collection.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/logan/Documents/Research/Mental_Health/Reddit_Data_Scraping/Reddit-Crawler-Data/Dataset%20Expansion/data_collection.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m redditor_dict\n",
      "\u001b[1;31mNameError\u001b[0m: name 'redditor_dict' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ignore - No Longer an Issue If Don't Kill Kernel Correction Code - Basically Before I Implemented Atomic Rights Kernel Crashed During File Writes Casuing Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - add code to ensure all dependencies are met bc the kernel crashed before I added atomic operations\n",
    "#IE for each user check that all comments exists (and fetch them if not)\n",
    "#for each comment check that all submissions exist (and fetch them if not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE - will need to rerun submissions for treatment group seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "734606\n",
      "752455\n"
     ]
    }
   ],
   "source": [
    "# #checking state\n",
    "# comments_get = list()\n",
    "# subs_get = dict()\n",
    "# for user, user_data in redditor_dict.items():\n",
    "#     for (_, comment) in user_data[\"comments\"]:\n",
    "#         sub_id = comment[\"submission_id\"]\n",
    "#         com_id = comment[\"comment_id\"]\n",
    "#         _key = sub_id + \"--------\" + com_id \n",
    "#         if not _key in comment_dict:\n",
    "#             comments_get.append(_key)\n",
    "#         if not sub_id in subs_get:\n",
    "#             subs_get[sub_id] = True\n",
    "\n",
    "# #checking state (each comment implies a valid submission)\n",
    "\n",
    "# for k, v in comment_dict.items():\n",
    "#     sub_id = v[\"meta\"][\"submission_id\"]\n",
    "#     if (not sub_id in submission_dict):\n",
    "#         # subs_get.append(sub_id)\n",
    "#         subs_get[sub_id] = True\n",
    "\n",
    "# print(len(comments_get))\n",
    "# print(len(subs_get))\n",
    "# print(len(submission_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #fixing submission data bc aparently that got cut off\n",
    "# def get_sub_ids(ids):\n",
    "#     return [i if i.startswith('t3_') else f't3_{i}' for i in ids]\n",
    "\n",
    "# list_of_subs = get_sub_ids(list(subs_get.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\logan\\AppData\\Local\\Temp\\ipykernel_39188\\508471449.py:2: DeprecationWarning: Positional arguments for 'Reddit.info' will no longer be supported in PRAW 8.\n",
      "Call this function with 'fullnames' as a keyword argument.\n",
      "  for submission in reddit.info(list_of_subs):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n",
      "260\n",
      "280\n",
      "300\n",
      "320\n",
      "340\n",
      "360\n",
      "380\n",
      "400\n",
      "420\n",
      "440\n",
      "460\n",
      "480\n",
      "500\n",
      "520\n",
      "540\n",
      "560\n",
      "580\n",
      "600\n",
      "620\n",
      "640\n",
      "660\n",
      "680\n",
      "700\n",
      "720\n",
      "740\n",
      "760\n",
      "780\n",
      "800\n",
      "820\n",
      "840\n",
      "860\n",
      "880\n",
      "900\n",
      "920\n",
      "940\n",
      "960\n",
      "980\n",
      "1000\n",
      "1020\n",
      "1040\n",
      "1060\n",
      "1080\n",
      "1100\n",
      "1120\n",
      "1140\n",
      "1160\n",
      "1180\n",
      "1200\n",
      "1220\n",
      "1240\n",
      "1260\n",
      "1280\n",
      "1300\n",
      "1320\n",
      "1340\n",
      "1360\n",
      "1380\n",
      "1400\n",
      "1420\n",
      "1440\n",
      "1460\n",
      "1480\n",
      "1500\n",
      "1520\n",
      "1540\n",
      "1560\n",
      "1580\n",
      "1600\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\logan\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\Dataset Expansion\\experiment.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/logan/Documents/Research/Mental_Health/Reddit_Data_Scraping/Reddit-Crawler-Data/Dataset%20Expansion/experiment.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m cnt \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/logan/Documents/Research/Mental_Health/Reddit_Data_Scraping/Reddit-Crawler-Data/Dataset%20Expansion/experiment.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m submission \u001b[39min\u001b[39;00m reddit\u001b[39m.\u001b[39minfo(list_of_subs):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/logan/Documents/Research/Mental_Health/Reddit_Data_Scraping/Reddit-Crawler-Data/Dataset%20Expansion/experiment.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     submission_dict[submission\u001b[39m.\u001b[39mid] \u001b[39m=\u001b[39m {\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/logan/Documents/Research/Mental_Health/Reddit_Data_Scraping/Reddit-Crawler-Data/Dataset%20Expansion/experiment.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mcreated_utc\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mint\u001b[39m(submission\u001b[39m.\u001b[39mcreated_utc),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/logan/Documents/Research/Mental_Health/Reddit_Data_Scraping/Reddit-Crawler-Data/Dataset%20Expansion/experiment.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mdistinguished\u001b[39m\u001b[39m\"\u001b[39m: submission\u001b[39m.\u001b[39mdistinguished,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/logan/Documents/Research/Mental_Health/Reddit_Data_Scraping/Reddit-Crawler-Data/Dataset%20Expansion/experiment.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mcomment_data\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/logan/Documents/Research/Mental_Health/Reddit_Data_Scraping/Reddit-Crawler-Data/Dataset%20Expansion/experiment.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         }\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/logan/Documents/Research/Mental_Health/Reddit_Data_Scraping/Reddit-Crawler-Data/Dataset%20Expansion/experiment.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     cnt \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\logan\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\.venv\\lib\\site-packages\\praw\\reddit.py:779\u001b[0m, in \u001b[0;36mReddit.info.<locals>.generator\u001b[1;34m(names)\u001b[0m\n\u001b[0;32m    777\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    778\u001b[0m params \u001b[39m=\u001b[39m {api_parameter_name: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(chunk)}\n\u001b[1;32m--> 779\u001b[0m \u001b[39mfor\u001b[39;00m result \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget(API_PATH[\u001b[39m\"\u001b[39;49m\u001b[39minfo\u001b[39;49m\u001b[39m\"\u001b[39;49m], params\u001b[39m=\u001b[39;49mparams):\n\u001b[0;32m    780\u001b[0m     \u001b[39myield\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\logan\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\.venv\\lib\\site-packages\\praw\\util\\deprecate_args.py:43\u001b[0m, in \u001b[0;36m_deprecate_args.<locals>.wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m     arg_string \u001b[39m=\u001b[39m _generate_arg_string(_old_args[: \u001b[39mlen\u001b[39m(args)])\n\u001b[0;32m     37\u001b[0m     warn(\n\u001b[0;32m     38\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPositional arguments for \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m!r}\u001b[39;00m\u001b[39m will no longer be\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     39\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m supported in PRAW 8.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mCall this function with \u001b[39m\u001b[39m{\u001b[39;00marg_string\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     40\u001b[0m         \u001b[39mDeprecationWarning\u001b[39;00m,\n\u001b[0;32m     41\u001b[0m         stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m     42\u001b[0m     )\n\u001b[1;32m---> 43\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(_old_args, args)), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\logan\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\.venv\\lib\\site-packages\\praw\\reddit.py:712\u001b[0m, in \u001b[0;36mReddit.get\u001b[1;34m(self, path, params)\u001b[0m\n\u001b[0;32m    699\u001b[0m \u001b[39m@_deprecate_args\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    700\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\n\u001b[0;32m    701\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    704\u001b[0m     params: Optional[Union[\u001b[39mstr\u001b[39m, Dict[\u001b[39mstr\u001b[39m, Union[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m]]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    705\u001b[0m ):\n\u001b[0;32m    706\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return parsed objects returned from a GET request to ``path``.\u001b[39;00m\n\u001b[0;32m    707\u001b[0m \n\u001b[0;32m    708\u001b[0m \u001b[39m    :param path: The path to fetch.\u001b[39;00m\n\u001b[0;32m    709\u001b[0m \u001b[39m    :param params: The query parameters to add to the request (default: ``None``).\u001b[39;00m\n\u001b[0;32m    710\u001b[0m \n\u001b[0;32m    711\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 712\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_objectify_request(method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mGET\u001b[39;49m\u001b[39m\"\u001b[39;49m, params\u001b[39m=\u001b[39;49mparams, path\u001b[39m=\u001b[39;49mpath)\n",
      "File \u001b[1;32mc:\\Users\\logan\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\.venv\\lib\\site-packages\\praw\\reddit.py:517\u001b[0m, in \u001b[0;36mReddit._objectify_request\u001b[1;34m(self, data, files, json, method, params, path)\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_objectify_request\u001b[39m(\n\u001b[0;32m    492\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    493\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    499\u001b[0m     path: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    500\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    501\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Run a request through the ``Objector``.\u001b[39;00m\n\u001b[0;32m    502\u001b[0m \n\u001b[0;32m    503\u001b[0m \u001b[39m    :param data: Dictionary, bytes, or file-like object to send in the body of the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    514\u001b[0m \n\u001b[0;32m    515\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_objector\u001b[39m.\u001b[39mobjectify(\n\u001b[1;32m--> 517\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    518\u001b[0m             data\u001b[39m=\u001b[39;49mdata,\n\u001b[0;32m    519\u001b[0m             files\u001b[39m=\u001b[39;49mfiles,\n\u001b[0;32m    520\u001b[0m             json\u001b[39m=\u001b[39;49mjson,\n\u001b[0;32m    521\u001b[0m             method\u001b[39m=\u001b[39;49mmethod,\n\u001b[0;32m    522\u001b[0m             params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    523\u001b[0m             path\u001b[39m=\u001b[39;49mpath,\n\u001b[0;32m    524\u001b[0m         )\n\u001b[0;32m    525\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\logan\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\.venv\\lib\\site-packages\\praw\\util\\deprecate_args.py:43\u001b[0m, in \u001b[0;36m_deprecate_args.<locals>.wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m     arg_string \u001b[39m=\u001b[39m _generate_arg_string(_old_args[: \u001b[39mlen\u001b[39m(args)])\n\u001b[0;32m     37\u001b[0m     warn(\n\u001b[0;32m     38\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPositional arguments for \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m!r}\u001b[39;00m\u001b[39m will no longer be\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     39\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m supported in PRAW 8.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mCall this function with \u001b[39m\u001b[39m{\u001b[39;00marg_string\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     40\u001b[0m         \u001b[39mDeprecationWarning\u001b[39;00m,\n\u001b[0;32m     41\u001b[0m         stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m     42\u001b[0m     )\n\u001b[1;32m---> 43\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(_old_args, args)), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\logan\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\.venv\\lib\\site-packages\\praw\\reddit.py:941\u001b[0m, in \u001b[0;36mReddit.request\u001b[1;34m(self, data, files, json, method, params, path)\u001b[0m\n\u001b[0;32m    939\u001b[0m     \u001b[39mraise\u001b[39;00m ClientException(\u001b[39m\"\u001b[39m\u001b[39mAt most one of \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mjson\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is supported.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    940\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 941\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_core\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    942\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[0;32m    943\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[0;32m    944\u001b[0m         json\u001b[39m=\u001b[39;49mjson,\n\u001b[0;32m    945\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[0;32m    946\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    947\u001b[0m         path\u001b[39m=\u001b[39;49mpath,\n\u001b[0;32m    948\u001b[0m     )\n\u001b[0;32m    949\u001b[0m \u001b[39mexcept\u001b[39;00m BadRequest \u001b[39mas\u001b[39;00m exception:\n\u001b[0;32m    950\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\logan\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\.venv\\lib\\site-packages\\prawcore\\sessions.py:328\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, path, data, files, json, params, timeout)\u001b[0m\n\u001b[0;32m    326\u001b[0m     json[\u001b[39m\"\u001b[39m\u001b[39mapi_type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mjson\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    327\u001b[0m url \u001b[39m=\u001b[39m urljoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_requestor\u001b[39m.\u001b[39moauth_url, path)\n\u001b[1;32m--> 328\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request_with_retries(\n\u001b[0;32m    329\u001b[0m     data\u001b[39m=\u001b[39;49mdata,\n\u001b[0;32m    330\u001b[0m     files\u001b[39m=\u001b[39;49mfiles,\n\u001b[0;32m    331\u001b[0m     json\u001b[39m=\u001b[39;49mjson,\n\u001b[0;32m    332\u001b[0m     method\u001b[39m=\u001b[39;49mmethod,\n\u001b[0;32m    333\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    334\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    335\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    336\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\logan\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\.venv\\lib\\site-packages\\prawcore\\sessions.py:234\u001b[0m, in \u001b[0;36mSession._request_with_retries\u001b[1;34m(self, data, files, json, method, params, timeout, url, retry_strategy_state)\u001b[0m\n\u001b[0;32m    232\u001b[0m retry_strategy_state\u001b[39m.\u001b[39msleep()\n\u001b[0;32m    233\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_request(data, method, params, url)\n\u001b[1;32m--> 234\u001b[0m response, saved_exception \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    235\u001b[0m     data,\n\u001b[0;32m    236\u001b[0m     files,\n\u001b[0;32m    237\u001b[0m     json,\n\u001b[0;32m    238\u001b[0m     method,\n\u001b[0;32m    239\u001b[0m     params,\n\u001b[0;32m    240\u001b[0m     retry_strategy_state,\n\u001b[0;32m    241\u001b[0m     timeout,\n\u001b[0;32m    242\u001b[0m     url,\n\u001b[0;32m    243\u001b[0m )\n\u001b[0;32m    245\u001b[0m do_retry \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m response \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m codes[\u001b[39m\"\u001b[39m\u001b[39munauthorized\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\logan\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\.venv\\lib\\site-packages\\prawcore\\sessions.py:186\u001b[0m, in \u001b[0;36mSession._make_request\u001b[1;34m(self, data, files, json, method, params, retry_strategy_state, timeout, url)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_make_request\u001b[39m(\n\u001b[0;32m    175\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    176\u001b[0m     data: \u001b[39mlist\u001b[39m[\u001b[39mtuple\u001b[39m[\u001b[39mstr\u001b[39m, Any]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    183\u001b[0m     url: \u001b[39mstr\u001b[39m,\n\u001b[0;32m    184\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[Response, \u001b[39mNone\u001b[39;00m] \u001b[39m|\u001b[39m \u001b[39mtuple\u001b[39m[\u001b[39mNone\u001b[39;00m, \u001b[39mException\u001b[39;00m]:\n\u001b[0;32m    185\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 186\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_rate_limiter\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m    187\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_requestor\u001b[39m.\u001b[39;49mrequest,\n\u001b[0;32m    188\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_header_callback,\n\u001b[0;32m    189\u001b[0m             method,\n\u001b[0;32m    190\u001b[0m             url,\n\u001b[0;32m    191\u001b[0m             allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    192\u001b[0m             data\u001b[39m=\u001b[39;49mdata,\n\u001b[0;32m    193\u001b[0m             files\u001b[39m=\u001b[39;49mfiles,\n\u001b[0;32m    194\u001b[0m             json\u001b[39m=\u001b[39;49mjson,\n\u001b[0;32m    195\u001b[0m             params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    196\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    197\u001b[0m         )\n\u001b[0;32m    198\u001b[0m         log\u001b[39m.\u001b[39mdebug(\n\u001b[0;32m    199\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mResponse: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m bytes) (rst-\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m:rem-\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m:used-\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m ratelimit) at \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    200\u001b[0m             response\u001b[39m.\u001b[39mstatus_code,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m             time\u001b[39m.\u001b[39mtime(),\n\u001b[0;32m    206\u001b[0m         )\n\u001b[0;32m    207\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\logan\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\.venv\\lib\\site-packages\\prawcore\\rate_limit.py:47\u001b[0m, in \u001b[0;36mRateLimiter.call\u001b[1;34m(self, request_function, set_header_callback, *args, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelay()\n\u001b[0;32m     46\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m set_header_callback()\n\u001b[1;32m---> 47\u001b[0m response \u001b[39m=\u001b[39m request_function(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     48\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate(response\u001b[39m.\u001b[39mheaders)\n\u001b[0;32m     49\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\logan\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\.venv\\lib\\site-packages\\prawcore\\requestor.py:68\u001b[0m, in \u001b[0;36mRequestor.request\u001b[1;34m(self, timeout, *args, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Issue the HTTP request capturing any errors that may occur.\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 68\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_http\u001b[39m.\u001b[39mrequest(\u001b[39m*\u001b[39margs, timeout\u001b[39m=\u001b[39mtimeout \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     69\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:  \u001b[39m# noqa: BLE001\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     \u001b[39mraise\u001b[39;00m RequestException(exc, args, kwargs) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\logan\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\.venv\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\logan\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\.venv\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39msend(request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\logan\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\.venv\\lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\logan\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\.venv\\lib\\site-packages\\urllib3\\connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    787\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    789\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_request(\n\u001b[0;32m    791\u001b[0m     conn,\n\u001b[0;32m    792\u001b[0m     method,\n\u001b[0;32m    793\u001b[0m     url,\n\u001b[0;32m    794\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout_obj,\n\u001b[0;32m    795\u001b[0m     body\u001b[39m=\u001b[39mbody,\n\u001b[0;32m    796\u001b[0m     headers\u001b[39m=\u001b[39mheaders,\n\u001b[0;32m    797\u001b[0m     chunked\u001b[39m=\u001b[39mchunked,\n\u001b[0;32m    798\u001b[0m     retries\u001b[39m=\u001b[39mretries,\n\u001b[0;32m    799\u001b[0m     response_conn\u001b[39m=\u001b[39mresponse_conn,\n\u001b[0;32m    800\u001b[0m     preload_content\u001b[39m=\u001b[39mpreload_content,\n\u001b[0;32m    801\u001b[0m     decode_content\u001b[39m=\u001b[39mdecode_content,\n\u001b[0;32m    802\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mresponse_kw,\n\u001b[0;32m    803\u001b[0m )\n\u001b[0;32m    805\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n\u001b[0;32m    806\u001b[0m clean_exit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\logan\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\.venv\\lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[39m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    537\u001b[0m \u001b[39mexcept\u001b[39;00m (BaseSSLError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    538\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Users\\logan\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\.venv\\lib\\site-packages\\urllib3\\connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mresponse\u001b[39;00m \u001b[39mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    460\u001b[0m \u001b[39m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 461\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    463\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    464\u001b[0m     assert_header_parsing(httplib_response\u001b[39m.\u001b[39mmsg)\n",
      "File \u001b[1;32mC:\\Python310\\lib\\http\\client.py:1368\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1366\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1367\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1368\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[0;32m   1369\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[0;32m   1370\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mC:\\Python310\\lib\\http\\client.py:317\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 317\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[0;32m    318\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[0;32m    319\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python310\\lib\\http\\client.py:278\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 278\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    279\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[0;32m    280\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Python310\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python310\\lib\\ssl.py:1273\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1269\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1270\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1271\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1272\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1273\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1274\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1275\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mC:\\Python310\\lib\\ssl.py:1129\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1127\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1128\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1129\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1130\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1131\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# cnt = 0 \n",
    "# for submission in reddit.info(list_of_subs):\n",
    "#     submission_dict[submission.id] = {\n",
    "#             \"created_utc\": int(submission.created_utc),\n",
    "#             \"distinguished\": submission.distinguished,\n",
    "#             \"is_original_content\" : submission.is_original_content,\n",
    "#             \"over_18\": submission.over_18,\n",
    "#             \"score\": submission.score, \n",
    "#             \"title\": submission.title,\n",
    "#             \"edited\": submission.edited, \n",
    "#             \"selftext\": submission.selftext,\n",
    "#             \"upvote_ratio\": submission.upvote_ratio,\n",
    "#             \"users_commented_on\": None,\n",
    "#             \"comment_data\":None\n",
    "#         }\n",
    "#     cnt += 1\n",
    "#     if (cnt % 20 == 0): print(cnt)\n",
    "#     if (cnt % 500 == 50):\n",
    "#         with DelayedKeyboardInterrupt():\n",
    "#             with atomic_write(SUBMISSION_FILE, overwrite=True) as submission_f:\n",
    "#                 json.dump(submission_dict, submission_f) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
