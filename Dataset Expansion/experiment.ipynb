{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw \n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"CL5xizTtujKZc4HOuFF_5w\",\n",
    "    client_secret=\"mD9JbzEVNFB8xopQKE_2WTcyIaz0hg\",\n",
    "    user_agent=\"logan-vaz\"\n",
    ")\n",
    "\n",
    "#reddit secret mD9JbzEVNFB8xopQKE_2WTcyIaz0hg, user_agen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - I misnamed control/treatment in here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERATION_NUM = 1\n",
    "\n",
    "import time \n",
    "def get_redditor_data(redditor: praw.reddit):\n",
    "\n",
    "    submission_ids = list() \n",
    "\n",
    "    to_ret = dict()\n",
    "    to_ret[\"username\"] = redditor.name\n",
    "    to_ret[\"comment_karma\"] = redditor.comment_karma\n",
    "    # to_ret[\"num_comments\"] = len(redditor.comments.top())\n",
    "    to_ret[\"time_creation\"] = redditor.created_utc\n",
    "    to_ret[\"verified_email\"] = redditor.has_verified_email\n",
    "    to_ret[\"is_employee\"] = redditor.is_employee\n",
    "    to_ret[\"is_mod\"] = redditor.is_mod\n",
    "    to_ret[\"is_gold\"] = redditor.is_gold \n",
    "    to_ret[\"is_suspended\"] = False\n",
    "    to_ret[\"link_karma\"] = redditor.link_karma\n",
    "    to_ret[\"num_moderated\"] = len(redditor.moderated())\n",
    "    to_ret[\"num_multireddits\"] = len(redditor.multireddits())\n",
    "    to_ret[\"num_trophies\"] = len(redditor.trophies())\n",
    "\n",
    "    if (redditor.subreddit):\n",
    "        to_ret[\"has_subreddit\"] = True\n",
    "        to_ret[\"over_18\"] = redditor.subreddit.over_18#redditor.subreddit[\"over_18\"]\n",
    "        to_ret[\"num_subscribers\"] = redditor.subreddit.subscribers#redditor.subreddit[\"subscribers\"]\n",
    "\n",
    "        #this is the only one added that rlly shouldn't be\n",
    "        to_ret[\"public_description\"] = redditor.subreddit.public_description\n",
    "    else: to_ret[\"has_subreddit\"] = False  \n",
    "    \n",
    "    to_ret = {\"data\": to_ret} \n",
    "\n",
    "    comment_data_user = list() \n",
    "    comment_data_dict = dict()\n",
    "\n",
    "    comment_cnt = 0 \n",
    "    for comment in redditor.comments.new():\n",
    "        print(\"comment is\", comment_cnt)\n",
    "        comment_cnt += 1\n",
    "        submission_id = comment.link_id \n",
    "        submission_ids.append(submission_id)\n",
    "        submission_id =comment.submission.id\n",
    "\n",
    "        # print(\"Created date is \" , comment.created_utc)\n",
    "        comment_data_user.append([int(comment.created_utc), {\"subreddit_id\" : comment.subreddit_id, \"submission_id\": submission_id, \"comment_id\": comment.id}])\n",
    "\n",
    "        comment_key = submission_id + \"--------\" + comment.id\n",
    "        comment_data = {\n",
    "\n",
    "            \"data\": {\n",
    "                \"is_edited\": comment.edited,\n",
    "                \"link_title\": comment.link_title, \n",
    "                \"num_replies\": len(comment.replies.list()),\n",
    "                \"score\": comment.score, \n",
    "                \"score_is_hidden\": comment.score_hidden,\n",
    "                \"total_awards\": comment.total_awards_received, \n",
    "                \"num_ups\": comment.ups, \n",
    "                \"num_downs\": comment.downs, \n",
    "                \"body\": comment.body,\n",
    "                \"date\": comment.created_utc,\n",
    "                \"is_submitter\": comment.is_submitter, \n",
    "                \"stickied\": comment.stickied\n",
    "                \n",
    "            },\n",
    "            \"meta\": {\n",
    "                 \"ITERATION_NUM\": ITERATION_NUM, \n",
    "                 \"TIME\": int(time.time()),\n",
    "                 \"submission_id\": submission_id, \n",
    "                 \"comment_id\": comment.id,\n",
    "                 \"author\": redditor.name\n",
    "            }\n",
    "            \n",
    "        }\n",
    "        comment_data_dict[comment_key] = comment_data\n",
    "    \n",
    "    to_ret[\"comments\"] = comment_data_user \n",
    "    to_ret[\"meta\"] = {\n",
    "        \"ITERATION_NUM\": ITERATION_NUM, \n",
    "        \"TIME\": int(time.time())\n",
    "    }\n",
    "    print(\"returning triplet\")\n",
    "    return to_ret, comment_data_dict, submission_ids\n",
    "\n",
    "\n",
    "\n",
    "# def get_submission_data():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prawcore #required for exception handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(listed):\n",
    "    \"\"\"from a list of morecomments and comments extract comments\"\"\"\n",
    "    to_ret = list()\n",
    "    for l in listed:\n",
    "        if (isinstance(l, praw.models.MoreComments)):\n",
    "            time.sleep(0.05)\n",
    "            try:\n",
    "                \n",
    "                base_comments = get_comments(l.comments())\n",
    "            except prawcore.exceptions.TooManyRequests as e:\n",
    "                print(\"backoff 1\")\n",
    "                time.sleep(10)\n",
    "                try:\n",
    "                    base_comments = get_comments(l.comments())\n",
    "                except prawcore.exceptions.TooManyRequests as e:\n",
    "                    print(\"backoff 2\")\n",
    "                    time.sleep(30)\n",
    "                    base_comments = get_comments(l.comments())\n",
    "            to_ret += base_comments \n",
    "        else:\n",
    "            to_ret.append(l)\n",
    "    return to_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/842557/how-to-prevent-a-block-of-code-from-being-interrupted-by-keyboardinterrupt-in-py\n",
    "#interrupt handler\n",
    "import signal\n",
    "import logging\n",
    "\n",
    "class DelayedKeyboardInterrupt:\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.signal_received = False\n",
    "        self.old_handler = signal.signal(signal.SIGINT, self.handler)\n",
    "                \n",
    "    def handler(self, sig, frame):\n",
    "        self.signal_received = (sig, frame)\n",
    "        logging.debug('SIGINT received. Delaying KeyboardInterrupt.')\n",
    "    \n",
    "    def __exit__(self, type, value, traceback):\n",
    "        signal.signal(signal.SIGINT, self.old_handler)\n",
    "        if self.signal_received:\n",
    "            self.old_handler(*self.signal_received)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of redditor_dict is  5961\n",
      "len of submission_dict is  388268\n",
      "len of comment_dict is  596948\n",
      "length of ppl to try is  176883\n",
      "chunk len is  11\n",
      "group is  control\n",
      "USER IS  MoosieGoose\n",
      "skipping\n",
      "group is  control\n",
      "USER IS  JollyK9\n",
      "skipping\n",
      "group is  control\n",
      "USER IS  Southern_Ad3032\n",
      "skipping\n",
      "group is  control\n",
      "USER IS  bduwowy272habbw\n",
      "skipping\n",
      "group is  control\n",
      "USER IS  Late_Introduction203\n",
      "skipping\n",
      "group is  control\n",
      "USER IS  kapster68\n",
      "skipping\n",
      "group is  control\n",
      "USER IS  TheApertureMonkey\n",
      "skipping\n",
      "group is  control\n",
      "USER IS  talemoon22\n",
      "skipping\n",
      "group is  control\n",
      "USER IS  sebagolindenwald\n",
      "skipping\n",
      "group is  control\n",
      "USER IS  spicyranchplzz\n",
      "skipping\n",
      "group is  control\n",
      "USER IS  TheFloorMayBeLava_02\n",
      "skipping\n",
      "chunk len is  11\n",
      "group is  control\n",
      "USER IS  rxtten_flesh\n",
      "skipping\n",
      "group is  control\n",
      "USER IS  greenblooded395\n",
      "skipping\n",
      "group is  control\n",
      "USER IS  greenblooded395\n",
      "skipping\n",
      "group is  control\n",
      "USER IS  DrakenJosh98\n",
      "skipping\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\logan\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\Dataset Expansion\\experiment.ipynb Cell 6\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/logan/Documents/Research/Mental_Health/Reddit_Data_Scraping/Reddit-Crawler-Data/Dataset%20Expansion/experiment.ipynb#W5sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/logan/Documents/Research/Mental_Health/Reddit_Data_Scraping/Reddit-Crawler-Data/Dataset%20Expansion/experiment.ipynb#W5sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     \u001b[39m#get the basic user information \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/logan/Documents/Research/Mental_Health/Reddit_Data_Scraping/Reddit-Crawler-Data/Dataset%20Expansion/experiment.ipynb#W5sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     \u001b[39mfor\u001b[39;00m user_name, group \u001b[39min\u001b[39;00m user_group_chunk:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/logan/Documents/Research/Mental_Health/Reddit_Data_Scraping/Reddit-Crawler-Data/Dataset%20Expansion/experiment.ipynb#W5sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m         time\u001b[39m.\u001b[39;49msleep(\u001b[39m.05\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/logan/Documents/Research/Mental_Health/Reddit_Data_Scraping/Reddit-Crawler-Data/Dataset%20Expansion/experiment.ipynb#W5sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mgroup is \u001b[39m\u001b[39m\"\u001b[39m, group)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/logan/Documents/Research/Mental_Health/Reddit_Data_Scraping/Reddit-Crawler-Data/Dataset%20Expansion/experiment.ipynb#W5sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mUSER IS \u001b[39m\u001b[39m\"\u001b[39m, user_name)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json \n",
    "import os\n",
    "\n",
    "import traceback\n",
    "import time    \n",
    "import random\n",
    "from atomicwrites import atomic_write\n",
    "\n",
    "import numpy as np \n",
    "epoch_time = int(time.time())\n",
    "\n",
    "\n",
    "REDDITOR_FILE = \"raw_data/redditor_dict.json\"\n",
    "SUBMISSION_FILE = \"raw_data/submission_dict.json\"\n",
    "COMMENT_FILE = \"raw_data/comment_dict.json\"\n",
    "\n",
    "def save_get_json(file_name):\n",
    "    if (os.path.exists(file_name)):\n",
    "        with open(file_name, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return dict()\n",
    "\n",
    "\n",
    "redditor_dict = save_get_json(REDDITOR_FILE)\n",
    "submission_dict = save_get_json(SUBMISSION_FILE)\n",
    "comment_dict = save_get_json(COMMENT_FILE)\n",
    "\n",
    "print(\"len of redditor_dict is \", len(redditor_dict))\n",
    "print(\"len of submission_dict is \", len(submission_dict))\n",
    "print(\"len of comment_dict is \", len(comment_dict))\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "#first, let's just fill with the control group - by name\n",
    "control_group = ['MoosieGoose', 'JollyK9', 'Southern_Ad3032', 'bduwowy272habbw', 'Late_Introduction203', 'kapster68', 'TheApertureMonkey', 'talemoon22', 'sebagolindenwald', 'spicyranchplzz', 'TheFloorMayBeLava_02', 'rxtten_flesh', 'greenblooded395', 'greenblooded395', 'DrakenJosh98', 'WhichUsernameIsBest', 'FStahp2', 'Pongpianskul', 'Kanashimi515', 'eviuwu', 'Kattheloner_22', 'Reeze2911', 'Sac20000', 'RanpoWasTaken', 'jlynny1811', 'Playful-Fail4778', 'GarageOk8109', 'katandcats', 'holyredemption', 'jifpeanutbutter420', 'Timely_Inflation1000', 'Erica_Peanut']\n",
    "\n",
    "#get potential treatment groups from those that comment on at least one of the same submission\n",
    "treatment_explore = [comment_data[\"user\"] for submission in submission_dict.values() if submission[\"comment_data\"] for comment_data in submission[\"comment_data\"]]\n",
    "treatment_explore = [t for t in treatment_explore if not t in redditor_dict]\n",
    "random.seed(24601)\n",
    "random.shuffle(treatment_explore)\n",
    "\n",
    "print(\"length of ppl to try is \", len(treatment_explore))\n",
    "\n",
    "\n",
    "users_and_groups = [(v, \"control\") for v in control_group] + [(v, \"treat\") for v in treatment_explore]\n",
    "users_and_groups_chunks = np.array_split(users_and_groups, max(int(len(users_and_groups)/10), 1))\n",
    "\n",
    "#was trying to batch requests - doesn't really appear possible so we're instead doing this to hopefully avoid rate limits\n",
    "for user_group_chunk in users_and_groups_chunks:\n",
    "    print(\"chunk len is \" ,len(user_group_chunk) )\n",
    "    \n",
    "    something_changed = False\n",
    "    try:\n",
    "        #get the basic user information \n",
    "        \n",
    "        for user_name, group in user_group_chunk:\n",
    "            time.sleep(.05)\n",
    "            print(\"group is \", group)\n",
    "            print(\"USER IS \", user_name)\n",
    "            if (user_name in redditor_dict.keys()): \n",
    "                print(\"skipping\")\n",
    "                \n",
    "                continue\n",
    "            # print(\"continuing\")\n",
    "            redditor =  reddit.redditor(user_name)\n",
    "            try:\n",
    "                user_data, comment_data, submission_ids = get_redditor_data(redditor) \n",
    "            except AttributeError as E:\n",
    "                print(\"Missing is \", E)\n",
    "                continue\n",
    "\n",
    "            something_changed = True\n",
    "\n",
    "            #time to get all relevant submission data (TODO - this is how we'll add users - begging the question of the order to add them in but we'll deal w/ that later)\n",
    "            this_submission_data = dict()\n",
    "            cnt = 0 \n",
    "            submissions_fetch = [sub_id for sub_id in submission_ids if not (sub_id in submission_dict or sub_id in this_submission_data)]\n",
    "            submissions = reddit.info(fullnames=submissions_fetch)\n",
    "            for submission in submissions:\n",
    "                cnt += 1\n",
    "                if (cnt % 10 ==0): print(f'sub cnt is {cnt}') \n",
    "                submission_id = submission.id\n",
    "                if (submission_id in submission_dict or submission_id in this_submission_data): \n",
    "                    print(\"matched id\")\n",
    "                    continue \n",
    "\n",
    "                #get comment data for this submission - if control we want to extract more data\n",
    "                if (group == \"control\"):\n",
    "                    commentors = dict() \n",
    "                    sub_comment_data = list()\n",
    "                    time.sleep(0.125)\n",
    "                    try:\n",
    "                        all_comments = submission.comments.list()\n",
    "                    except prawcore.exceptions.TooManyRequests as E:\n",
    "                        print(\"backoff 1\")\n",
    "                        time.sleep(10)\n",
    "                        try:\n",
    "                            all_comments = submission.comments.list()\n",
    "                        except prawcore.exceptions.TooManyRequests as E:\n",
    "                            print(\"backoff 2\")\n",
    "                            time.sleep(30)\n",
    "                            all_comments = submission.comments.list()\n",
    "\n",
    "                    original_comments = [c for c in all_comments if not isinstance(c, praw.models.MoreComments)]\n",
    "                    layered_comments = get_comments([c for c in all_comments if isinstance(c, praw.models.MoreComments)])\n",
    "\n",
    "                    for comment in original_comments:\n",
    "                        if (comment.author and comment.author not in [\"None\", \"[removed]\", \"[deleted]\"] and comment.author.name):\n",
    "                            commentors[comment.author.name] = True\n",
    "                            sub_comment_data.append( {\n",
    "                                \"user\": comment.author.name,\n",
    "                                \"id\": comment.id,\n",
    "                                \"original_reply\": True\n",
    "                            })\n",
    "                    for comment in layered_comments:\n",
    "                        # submission.author != \"None\" or \"[removed]\" or \"[deleted]\"\n",
    "                        print(\"comment author is \", comment.author)\n",
    "                        if (comment.author and comment.author not in [\"None\", \"[removed]\", \"[deleted]\"] and comment.author.name):\n",
    "                            commentors[comment.author.name] = True\n",
    "                            sub_comment_data.append( {\n",
    "                                \"user\": comment.author.name,\n",
    "                                \"id\": comment.id,\n",
    "                                \"original_reply\": False\n",
    "                            })\n",
    "                    this_submission_data[submission_id] = {\n",
    "                        \"created_utc\": int(submission.created_utc),\n",
    "                        \"distinguished\": submission.distinguished,\n",
    "                        \"is_original_content\" : submission.is_original_content,\n",
    "                        \"over_18\": submission.over_18,\n",
    "                        \"score\": submission.score, \n",
    "                        \"title\": submission.title,\n",
    "                        \"edited\": submission.edited, \n",
    "                        \"selftext\": submission.selftext,\n",
    "                        \"upvote_ratio\": submission.upvote_ratio,\n",
    "                        \"users_commented_on\": list(commentors.keys()),\n",
    "                        \"comment_data\":sub_comment_data\n",
    "                    }\n",
    "                else:\n",
    "                    this_submission_data[submission_id] = {\n",
    "                        \"created_utc\": int(submission.created_utc),\n",
    "                        \"distinguished\": submission.distinguished,\n",
    "                        \"is_original_content\" : submission.is_original_content,\n",
    "                        \"over_18\": submission.over_18,\n",
    "                        \"score\": submission.score, \n",
    "                        \"title\": submission.title,\n",
    "                        \"edited\": submission.edited, \n",
    "                        \"selftext\": submission.selftext,\n",
    "                        \"upvote_ratio\": submission.upvote_ratio,\n",
    "                        \"users_commented_on\": None,\n",
    "                        \"comment_data\":None\n",
    "                    }\n",
    "\n",
    "            print(\"updating dict\")\n",
    "            redditor_dict.update({user_name: user_data})\n",
    "            submission_dict.update(this_submission_data)\n",
    "            comment_dict.update(comment_data)\n",
    "    \n",
    "    except Exception as E:\n",
    "        traceback.print_exc()\n",
    "        print(\"Exception is \", E)\n",
    "        time.sleep(32) \n",
    "    if (something_changed):\n",
    "        with DelayedKeyboardInterrupt():\n",
    "            #with open(SUBMISSION_FILE, 'w') as submission_f:\n",
    "            with atomic_write(SUBMISSION_FILE, overwrite=True) as submission_f:\n",
    "                json.dump(submission_dict, submission_f) \n",
    "            # with open(COMMENT_FILE, 'w') as comment_f:\n",
    "            with atomic_write(COMMENT_FILE, overwrite=True) as comment_f:\n",
    "                json.dump(comment_dict,comment_f )\n",
    "            # with open(REDDITOR_FILE, 'w') as redditor_f:\n",
    "            with atomic_write(REDDITOR_FILE, overwrite=True) as redditor_f:\n",
    "                json.dump(redditor_dict,redditor_f )\n",
    "\n",
    "        print(\"Saved files\")\n",
    "        time.sleep(3)\n",
    "        time.sleep(random.random()*3+.5)\n",
    "\n",
    "\"\"\"\n",
    "Redditor Instantiation\n",
    "- load all data into memory first (static) \n",
    "\n",
    "get_base_data(): return redditor[data]\n",
    "get_comment_data()\n",
    "get_submission_data() \n",
    "//some kind of store value w/ state (seperate dict)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "//all should contain stuff like a date scraped\n",
    "Redditor_Dict:\n",
    "Username ->  {\n",
    "    \"data\": properties + labels in dict \n",
    "    \"comments\": [\n",
    "        (comment date, {\"subreddit_id\" : subreddit_id, \"submission_id\": submission_id, \"comment_id\": comment_id})\n",
    "        # (comment date, subreddit_id, .submission.id, {dictionary of all info})\n",
    "        //dict of all info should be initialized seperately\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "[properties + labels below, comments: {relationship data + [\"post_id\" :id of post commented on [comment.subreddit_id]]}]\n",
    "#what relevant edge data do we need?\n",
    "\n",
    "\n",
    "SubmissionDict\n",
    "{\n",
    "    submission.id:  {\n",
    "        \"created_utc\"\n",
    "        \"distinguished\" \n",
    "        \"is_original_content\" \n",
    "        \"over_18\" \n",
    "        \"score\" \n",
    "        \"title\"\n",
    "        \"upvote_ratio\" \n",
    "        \"users_commented_on\": submission.comments.replace_more(None).list() | None\n",
    "        \"comment_data\": {\"user\": comment.author.id, \"id\": comment.id} | None\n",
    "    }\n",
    "}\n",
    "\n",
    "CommentDict {\n",
    "    submission_id + \"--------\" + comment_id : { \n",
    "        \"data\": {\n",
    "            \"is_edited\": comment.edited,\n",
    "            \"link_title\": comment.link_title, \n",
    "            \"num_replies\": len(comment.replies.list()),\n",
    "            \"score\": comment.score, \n",
    "            \"score_is_hidden\": comment.score_hidden,\n",
    "            \"total_awards\": comment.total_awards_received, \n",
    "            \"num_ups\": comment.ups, \n",
    "            \"num_downs\": comment.downs, \n",
    "            \"body\": comment.body,\n",
    "            \"date\": comment.created_utc,\n",
    "            \"is_submitter\": comment.is_submitter, \n",
    "            \"stickied\": comment.stickied\n",
    "            \n",
    "        },\n",
    "        \"meta\": {\n",
    "                \"ITERATION_NUM\": ITERATION_NUM, \n",
    "                \"TIME\": int(time.time()),\n",
    "                \"submission_id\": submission_id, \n",
    "                \"comment_id\": comment.id,\n",
    "                \"author\": redditor.name\n",
    "        }\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "end\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correction Code - Basically Before I Implemented Atomic Rights Kernel Crashed During File Writes Casuing Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - add code to ensure all dependencies are met bc the kernel crashed before I added atomic operations\n",
    "#IE for each user check that all comments exists (and fetch them if not)\n",
    "#for each comment check that all submissions exist (and fetch them if not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE - will need to rerun submissions for control group seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "369269\n",
      "388629\n"
     ]
    }
   ],
   "source": [
    "#checking state\n",
    "comments_get = list()\n",
    "subs_get = dict()\n",
    "for user, user_data in redditor_dict.items():\n",
    "    for (_, comment) in user_data[\"comments\"]:\n",
    "        sub_id = comment[\"submission_id\"]\n",
    "        com_id = comment[\"comment_id\"]\n",
    "        _key = sub_id + \"--------\" + com_id \n",
    "        if not _key in comment_dict:\n",
    "            comments_get.append(_key)\n",
    "        if not sub_id in subs_get:\n",
    "            subs_get[sub_id] = True\n",
    "\n",
    "#checking state (each comment implies a valid submission)\n",
    "\n",
    "for k, v in comment_dict.items():\n",
    "    sub_id = v[\"meta\"][\"submission_id\"]\n",
    "    if (not sub_id in submission_dict):\n",
    "        # subs_get.append(sub_id)\n",
    "        subs_get[sub_id] = True\n",
    "\n",
    "print(len(comments_get))\n",
    "print(len(subs_get))\n",
    "print(len(submission_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixing submission data bc aparently that got cut off\n",
    "def get_sub_ids(ids):\n",
    "    return [i if i.startswith('t3_') else f't3_{i}' for i in ids]\n",
    "\n",
    "list_of_subs = get_sub_ids(list(subs_get.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\logan\\AppData\\Local\\Temp\\ipykernel_39188\\508471449.py:2: DeprecationWarning: Positional arguments for 'Reddit.info' will no longer be supported in PRAW 8.\n",
      "Call this function with 'fullnames' as a keyword argument.\n",
      "  for submission in reddit.info(list_of_subs):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n",
      "260\n",
      "280\n",
      "300\n",
      "320\n",
      "340\n",
      "360\n",
      "380\n",
      "400\n",
      "420\n",
      "440\n",
      "460\n",
      "480\n",
      "500\n",
      "520\n",
      "540\n",
      "560\n",
      "580\n",
      "600\n",
      "620\n",
      "640\n",
      "660\n",
      "680\n",
      "700\n",
      "720\n",
      "740\n",
      "760\n",
      "780\n",
      "800\n",
      "820\n",
      "840\n",
      "860\n",
      "880\n",
      "900\n",
      "920\n",
      "940\n",
      "960\n",
      "980\n",
      "1000\n",
      "1020\n",
      "1040\n",
      "1060\n",
      "1080\n",
      "1100\n",
      "1120\n",
      "1140\n",
      "1160\n",
      "1180\n",
      "1200\n",
      "1220\n",
      "1240\n",
      "1260\n",
      "1280\n",
      "1300\n",
      "1320\n",
      "1340\n",
      "1360\n",
      "1380\n",
      "1400\n",
      "1420\n",
      "1440\n",
      "1460\n",
      "1480\n",
      "1500\n",
      "1520\n",
      "1540\n",
      "1560\n",
      "1580\n",
      "1600\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\logan\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\Dataset Expansion\\experiment.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/logan/Documents/Research/Mental_Health/Reddit_Data_Scraping/Reddit-Crawler-Data/Dataset%20Expansion/experiment.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m cnt \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/logan/Documents/Research/Mental_Health/Reddit_Data_Scraping/Reddit-Crawler-Data/Dataset%20Expansion/experiment.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m submission \u001b[39min\u001b[39;00m reddit\u001b[39m.\u001b[39minfo(list_of_subs):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/logan/Documents/Research/Mental_Health/Reddit_Data_Scraping/Reddit-Crawler-Data/Dataset%20Expansion/experiment.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     submission_dict[submission\u001b[39m.\u001b[39mid] \u001b[39m=\u001b[39m {\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/logan/Documents/Research/Mental_Health/Reddit_Data_Scraping/Reddit-Crawler-Data/Dataset%20Expansion/experiment.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mcreated_utc\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mint\u001b[39m(submission\u001b[39m.\u001b[39mcreated_utc),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/logan/Documents/Research/Mental_Health/Reddit_Data_Scraping/Reddit-Crawler-Data/Dataset%20Expansion/experiment.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mdistinguished\u001b[39m\u001b[39m\"\u001b[39m: submission\u001b[39m.\u001b[39mdistinguished,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/logan/Documents/Research/Mental_Health/Reddit_Data_Scraping/Reddit-Crawler-Data/Dataset%20Expansion/experiment.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mcomment_data\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/logan/Documents/Research/Mental_Health/Reddit_Data_Scraping/Reddit-Crawler-Data/Dataset%20Expansion/experiment.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         }\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/logan/Documents/Research/Mental_Health/Reddit_Data_Scraping/Reddit-Crawler-Data/Dataset%20Expansion/experiment.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     cnt \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\logan\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\.venv\\lib\\site-packages\\praw\\reddit.py:779\u001b[0m, in \u001b[0;36mReddit.info.<locals>.generator\u001b[1;34m(names)\u001b[0m\n\u001b[0;32m    777\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    778\u001b[0m params \u001b[39m=\u001b[39m {api_parameter_name: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(chunk)}\n\u001b[1;32m--> 779\u001b[0m \u001b[39mfor\u001b[39;00m result \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget(API_PATH[\u001b[39m\"\u001b[39;49m\u001b[39minfo\u001b[39;49m\u001b[39m\"\u001b[39;49m], params\u001b[39m=\u001b[39;49mparams):\n\u001b[0;32m    780\u001b[0m     \u001b[39myield\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\logan\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\.venv\\lib\\site-packages\\praw\\util\\deprecate_args.py:43\u001b[0m, in \u001b[0;36m_deprecate_args.<locals>.wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m     arg_string \u001b[39m=\u001b[39m _generate_arg_string(_old_args[: \u001b[39mlen\u001b[39m(args)])\n\u001b[0;32m     37\u001b[0m     warn(\n\u001b[0;32m     38\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPositional arguments for \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m!r}\u001b[39;00m\u001b[39m will no longer be\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     39\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m supported in PRAW 8.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mCall this function with \u001b[39m\u001b[39m{\u001b[39;00marg_string\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     40\u001b[0m         \u001b[39mDeprecationWarning\u001b[39;00m,\n\u001b[0;32m     41\u001b[0m         stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m     42\u001b[0m     )\n\u001b[1;32m---> 43\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(_old_args, args)), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\logan\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\.venv\\lib\\site-packages\\praw\\reddit.py:712\u001b[0m, in \u001b[0;36mReddit.get\u001b[1;34m(self, path, params)\u001b[0m\n\u001b[0;32m    699\u001b[0m \u001b[39m@_deprecate_args\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    700\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\n\u001b[0;32m    701\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    704\u001b[0m     params: Optional[Union[\u001b[39mstr\u001b[39m, Dict[\u001b[39mstr\u001b[39m, Union[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m]]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    705\u001b[0m ):\n\u001b[0;32m    706\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return parsed objects returned from a GET request to ``path``.\u001b[39;00m\n\u001b[0;32m    707\u001b[0m \n\u001b[0;32m    708\u001b[0m \u001b[39m    :param path: The path to fetch.\u001b[39;00m\n\u001b[0;32m    709\u001b[0m \u001b[39m    :param params: The query parameters to add to the request (default: ``None``).\u001b[39;00m\n\u001b[0;32m    710\u001b[0m \n\u001b[0;32m    711\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 712\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_objectify_request(method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mGET\u001b[39;49m\u001b[39m\"\u001b[39;49m, params\u001b[39m=\u001b[39;49mparams, path\u001b[39m=\u001b[39;49mpath)\n",
      "File \u001b[1;32mc:\\Users\\logan\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\.venv\\lib\\site-packages\\praw\\reddit.py:517\u001b[0m, in \u001b[0;36mReddit._objectify_request\u001b[1;34m(self, data, files, json, method, params, path)\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_objectify_request\u001b[39m(\n\u001b[0;32m    492\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    493\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    499\u001b[0m     path: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    500\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    501\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Run a request through the ``Objector``.\u001b[39;00m\n\u001b[0;32m    502\u001b[0m \n\u001b[0;32m    503\u001b[0m \u001b[39m    :param data: Dictionary, bytes, or file-like object to send in the body of the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    514\u001b[0m \n\u001b[0;32m    515\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_objector\u001b[39m.\u001b[39mobjectify(\n\u001b[1;32m--> 517\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    518\u001b[0m             data\u001b[39m=\u001b[39;49mdata,\n\u001b[0;32m    519\u001b[0m             files\u001b[39m=\u001b[39;49mfiles,\n\u001b[0;32m    520\u001b[0m             json\u001b[39m=\u001b[39;49mjson,\n\u001b[0;32m    521\u001b[0m             method\u001b[39m=\u001b[39;49mmethod,\n\u001b[0;32m    522\u001b[0m             params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    523\u001b[0m             path\u001b[39m=\u001b[39;49mpath,\n\u001b[0;32m    524\u001b[0m         )\n\u001b[0;32m    525\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\logan\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\.venv\\lib\\site-packages\\praw\\util\\deprecate_args.py:43\u001b[0m, in \u001b[0;36m_deprecate_args.<locals>.wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m     arg_string \u001b[39m=\u001b[39m _generate_arg_string(_old_args[: \u001b[39mlen\u001b[39m(args)])\n\u001b[0;32m     37\u001b[0m     warn(\n\u001b[0;32m     38\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPositional arguments for \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m!r}\u001b[39;00m\u001b[39m will no longer be\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     39\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m supported in PRAW 8.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mCall this function with \u001b[39m\u001b[39m{\u001b[39;00marg_string\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     40\u001b[0m         \u001b[39mDeprecationWarning\u001b[39;00m,\n\u001b[0;32m     41\u001b[0m         stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m     42\u001b[0m     )\n\u001b[1;32m---> 43\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(_old_args, args)), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\logan\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\.venv\\lib\\site-packages\\praw\\reddit.py:941\u001b[0m, in \u001b[0;36mReddit.request\u001b[1;34m(self, data, files, json, method, params, path)\u001b[0m\n\u001b[0;32m    939\u001b[0m     \u001b[39mraise\u001b[39;00m ClientException(\u001b[39m\"\u001b[39m\u001b[39mAt most one of \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mjson\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is supported.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    940\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 941\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_core\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    942\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[0;32m    943\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[0;32m    944\u001b[0m         json\u001b[39m=\u001b[39;49mjson,\n\u001b[0;32m    945\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[0;32m    946\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    947\u001b[0m         path\u001b[39m=\u001b[39;49mpath,\n\u001b[0;32m    948\u001b[0m     )\n\u001b[0;32m    949\u001b[0m \u001b[39mexcept\u001b[39;00m BadRequest \u001b[39mas\u001b[39;00m exception:\n\u001b[0;32m    950\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\logan\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\.venv\\lib\\site-packages\\prawcore\\sessions.py:328\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, path, data, files, json, params, timeout)\u001b[0m\n\u001b[0;32m    326\u001b[0m     json[\u001b[39m\"\u001b[39m\u001b[39mapi_type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mjson\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    327\u001b[0m url \u001b[39m=\u001b[39m urljoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_requestor\u001b[39m.\u001b[39moauth_url, path)\n\u001b[1;32m--> 328\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request_with_retries(\n\u001b[0;32m    329\u001b[0m     data\u001b[39m=\u001b[39;49mdata,\n\u001b[0;32m    330\u001b[0m     files\u001b[39m=\u001b[39;49mfiles,\n\u001b[0;32m    331\u001b[0m     json\u001b[39m=\u001b[39;49mjson,\n\u001b[0;32m    332\u001b[0m     method\u001b[39m=\u001b[39;49mmethod,\n\u001b[0;32m    333\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    334\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    335\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    336\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\logan\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\.venv\\lib\\site-packages\\prawcore\\sessions.py:234\u001b[0m, in \u001b[0;36mSession._request_with_retries\u001b[1;34m(self, data, files, json, method, params, timeout, url, retry_strategy_state)\u001b[0m\n\u001b[0;32m    232\u001b[0m retry_strategy_state\u001b[39m.\u001b[39msleep()\n\u001b[0;32m    233\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_request(data, method, params, url)\n\u001b[1;32m--> 234\u001b[0m response, saved_exception \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    235\u001b[0m     data,\n\u001b[0;32m    236\u001b[0m     files,\n\u001b[0;32m    237\u001b[0m     json,\n\u001b[0;32m    238\u001b[0m     method,\n\u001b[0;32m    239\u001b[0m     params,\n\u001b[0;32m    240\u001b[0m     retry_strategy_state,\n\u001b[0;32m    241\u001b[0m     timeout,\n\u001b[0;32m    242\u001b[0m     url,\n\u001b[0;32m    243\u001b[0m )\n\u001b[0;32m    245\u001b[0m do_retry \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m response \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m codes[\u001b[39m\"\u001b[39m\u001b[39munauthorized\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\logan\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\.venv\\lib\\site-packages\\prawcore\\sessions.py:186\u001b[0m, in \u001b[0;36mSession._make_request\u001b[1;34m(self, data, files, json, method, params, retry_strategy_state, timeout, url)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_make_request\u001b[39m(\n\u001b[0;32m    175\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    176\u001b[0m     data: \u001b[39mlist\u001b[39m[\u001b[39mtuple\u001b[39m[\u001b[39mstr\u001b[39m, Any]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    183\u001b[0m     url: \u001b[39mstr\u001b[39m,\n\u001b[0;32m    184\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[Response, \u001b[39mNone\u001b[39;00m] \u001b[39m|\u001b[39m \u001b[39mtuple\u001b[39m[\u001b[39mNone\u001b[39;00m, \u001b[39mException\u001b[39;00m]:\n\u001b[0;32m    185\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 186\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_rate_limiter\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m    187\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_requestor\u001b[39m.\u001b[39;49mrequest,\n\u001b[0;32m    188\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_header_callback,\n\u001b[0;32m    189\u001b[0m             method,\n\u001b[0;32m    190\u001b[0m             url,\n\u001b[0;32m    191\u001b[0m             allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    192\u001b[0m             data\u001b[39m=\u001b[39;49mdata,\n\u001b[0;32m    193\u001b[0m             files\u001b[39m=\u001b[39;49mfiles,\n\u001b[0;32m    194\u001b[0m             json\u001b[39m=\u001b[39;49mjson,\n\u001b[0;32m    195\u001b[0m             params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    196\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    197\u001b[0m         )\n\u001b[0;32m    198\u001b[0m         log\u001b[39m.\u001b[39mdebug(\n\u001b[0;32m    199\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mResponse: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m bytes) (rst-\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m:rem-\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m:used-\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m ratelimit) at \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    200\u001b[0m             response\u001b[39m.\u001b[39mstatus_code,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m             time\u001b[39m.\u001b[39mtime(),\n\u001b[0;32m    206\u001b[0m         )\n\u001b[0;32m    207\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\logan\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\.venv\\lib\\site-packages\\prawcore\\rate_limit.py:47\u001b[0m, in \u001b[0;36mRateLimiter.call\u001b[1;34m(self, request_function, set_header_callback, *args, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelay()\n\u001b[0;32m     46\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m set_header_callback()\n\u001b[1;32m---> 47\u001b[0m response \u001b[39m=\u001b[39m request_function(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     48\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate(response\u001b[39m.\u001b[39mheaders)\n\u001b[0;32m     49\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\logan\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\.venv\\lib\\site-packages\\prawcore\\requestor.py:68\u001b[0m, in \u001b[0;36mRequestor.request\u001b[1;34m(self, timeout, *args, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Issue the HTTP request capturing any errors that may occur.\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 68\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_http\u001b[39m.\u001b[39mrequest(\u001b[39m*\u001b[39margs, timeout\u001b[39m=\u001b[39mtimeout \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     69\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:  \u001b[39m# noqa: BLE001\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     \u001b[39mraise\u001b[39;00m RequestException(exc, args, kwargs) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\logan\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\.venv\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\logan\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\.venv\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39msend(request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\logan\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\.venv\\lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\logan\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\.venv\\lib\\site-packages\\urllib3\\connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    787\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    789\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_request(\n\u001b[0;32m    791\u001b[0m     conn,\n\u001b[0;32m    792\u001b[0m     method,\n\u001b[0;32m    793\u001b[0m     url,\n\u001b[0;32m    794\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout_obj,\n\u001b[0;32m    795\u001b[0m     body\u001b[39m=\u001b[39mbody,\n\u001b[0;32m    796\u001b[0m     headers\u001b[39m=\u001b[39mheaders,\n\u001b[0;32m    797\u001b[0m     chunked\u001b[39m=\u001b[39mchunked,\n\u001b[0;32m    798\u001b[0m     retries\u001b[39m=\u001b[39mretries,\n\u001b[0;32m    799\u001b[0m     response_conn\u001b[39m=\u001b[39mresponse_conn,\n\u001b[0;32m    800\u001b[0m     preload_content\u001b[39m=\u001b[39mpreload_content,\n\u001b[0;32m    801\u001b[0m     decode_content\u001b[39m=\u001b[39mdecode_content,\n\u001b[0;32m    802\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mresponse_kw,\n\u001b[0;32m    803\u001b[0m )\n\u001b[0;32m    805\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n\u001b[0;32m    806\u001b[0m clean_exit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\logan\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\.venv\\lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[39m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    537\u001b[0m \u001b[39mexcept\u001b[39;00m (BaseSSLError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    538\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Users\\logan\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\.venv\\lib\\site-packages\\urllib3\\connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mresponse\u001b[39;00m \u001b[39mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    460\u001b[0m \u001b[39m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 461\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    463\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    464\u001b[0m     assert_header_parsing(httplib_response\u001b[39m.\u001b[39mmsg)\n",
      "File \u001b[1;32mC:\\Python310\\lib\\http\\client.py:1368\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1366\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1367\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1368\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[0;32m   1369\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[0;32m   1370\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mC:\\Python310\\lib\\http\\client.py:317\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 317\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[0;32m    318\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[0;32m    319\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python310\\lib\\http\\client.py:278\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 278\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    279\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[0;32m    280\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Python310\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python310\\lib\\ssl.py:1273\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1269\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1270\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1271\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1272\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1273\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1274\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1275\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mC:\\Python310\\lib\\ssl.py:1129\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1127\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1128\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1129\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1130\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1131\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cnt = 0 \n",
    "for submission in reddit.info(list_of_subs):\n",
    "    submission_dict[submission.id] = {\n",
    "            \"created_utc\": int(submission.created_utc),\n",
    "            \"distinguished\": submission.distinguished,\n",
    "            \"is_original_content\" : submission.is_original_content,\n",
    "            \"over_18\": submission.over_18,\n",
    "            \"score\": submission.score, \n",
    "            \"title\": submission.title,\n",
    "            \"edited\": submission.edited, \n",
    "            \"selftext\": submission.selftext,\n",
    "            \"upvote_ratio\": submission.upvote_ratio,\n",
    "            \"users_commented_on\": None,\n",
    "            \"comment_data\":None\n",
    "        }\n",
    "    cnt += 1\n",
    "    if (cnt % 20 == 0): print(cnt)\n",
    "    if (cnt % 500 == 50):\n",
    "        with DelayedKeyboardInterrupt():\n",
    "            with atomic_write(SUBMISSION_FILE, overwrite=True) as submission_f:\n",
    "                json.dump(submission_dict, submission_f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "options\n",
    "- sleep\n",
    "- batching \n",
    "- multiple user agents \n",
    "- scraping subseciton of comments\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Redditor:\n",
    "{iteration, date of update, all labels below}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Reader - Read the Data, Process It and Save Under processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TODO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets define what we want a redditor node to look like\n",
    "\n",
    "\n",
    "\n",
    "#property - just need to return it\n",
    "#label - if need to return it\n",
    "class Redditor:\n",
    "    \"\"\"\n",
    "    class to instantiate a Redditor node in our db\n",
    "\n",
    "    properties:\n",
    "        ITERATION_ITERATION_NUM\n",
    "        \n",
    "        iteration=ITERATION\n",
    "        dateUpdated=TODAY()\n",
    "        group=UNKNOWN (can update later to control vs treat)\n",
    "        #do you return kind of id? I'd say no and we can do group search\n",
    "\n",
    "    label:\n",
    "        (copied over from user base data)\n",
    "            \n",
    "            to_ret[\"username\"] = redditor.name\n",
    "            to_ret[\"comment_karma\"] = redditor.comment_karma\n",
    "            to_ret[\"num_comments\"] = len(redditor.comments)\n",
    "            to_ret[\"time_creation\"] = redditor.created_utc\n",
    "            to_ret[\"verified_email\"] = redditor.has_verified_email\n",
    "            to_ret[\"is_employee\"] = redditor.is_employee\n",
    "            to_ret[\"is_mod\"] = redditor.is_mod\n",
    "            to_ret[\"is_gold\"] = redditor.is_gold \n",
    "            to_ret[\"is_suspended\"] = False\n",
    "            to_ret[\"link_karma\"] = redditor.link_karma\n",
    "            to_ret[\"num_moderated\"] = len(redditor.moderated())\n",
    "            to_ret[\"num_multireddits\"] = len(redditor.multireddits())\n",
    "            to_ret[\"num_trophies\"] = len(redditor.trophies())\n",
    "\n",
    "            if (redditor.subreddit):\n",
    "                to_ret[\"has_subreddit\"] = True\n",
    "                to_ret[\"over_18\"] = redditor.subreddit.over_18#redditor.subreddit[\"over_18\"]\n",
    "                to_ret[\"num_subscribers\"] = redditor.subreddit.subscribers#redditor.subreddit[\"subscribers\"]\n",
    "\n",
    "                #this is the only one added that rlly shouldn't be\n",
    "                to_ret[\"public_description\"] = redditor.subreddit[\"public_description\"]\n",
    "            else: to_ret[\"has_subreddit\"] = False        \n",
    "\n",
    "    relationships: type: COMMENTED_ON (connects to a POST class we still have to create)\n",
    "\n",
    "            \"is_edited\": comment.edited,\n",
    "            \"link_title\": comment.link_title, \n",
    "            \"num_replies\": len(comment.replies.list()),\n",
    "            \"score\": comment.score, \n",
    "            \"score_is_hidden\": comment.score_hidden,\n",
    "            \"total_awards\": comment.total_awards_received, \n",
    "            \"num_ups\": comment.ups, \n",
    "            \"num_downs\": comment.downs, \n",
    "            \"body\": comment.body,\n",
    "            \"date\": comment.created_utc,\n",
    "            \"is_submitter\": comment.is_submitter, \n",
    "            \"stickied\": comment.stickied\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
