{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nUnfortunately, we often don’t know the true causal graph so it can be hard to know when another feature is redundant with our feature of interest because of observed confounding vs. non-confounding redundancy. If it is because of confounding then we should control for that feature using a method like double ML, whereas if it is a downstream consequence then we should drop the feature from our model if we want full causal effects rather than only direct effects. Controlling for a feature we shouldn’t tends to hide or split up causal effects, while failing to control for a feature we should have controlled for tends to infer causal effects that do not exist. This generally makes controlling for a feature the safer option when you are uncertain.\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#there's lots of options for models including agnostic but we'll start with NN\n",
    "\n",
    "#https://shap.readthedocs.io/en/latest/\n",
    "\n",
    "\n",
    "#SHAPE MAKES THE COEFFICIENT CORRELATION evident\n",
    "#causal when feature analyzing is strongly independent from others (i.e. why we matched + hope get unobserved)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Look into double ML (CausalML)\n",
    "- will fail to capture indirect casual effects if control for downstream features caused by feature of interest\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Unfortunately, we often don’t know the true causal graph so it can be hard to know when another feature is redundant with our feature of interest because of observed confounding vs. non-confounding redundancy. If it is because of confounding then we should control for that feature using a method like double ML, whereas if it is a downstream consequence then we should drop the feature from our model if we want full causal effects rather than only direct effects. Controlling for a feature we shouldn’t tends to hide or split up causal effects, while failing to control for a feature we should have controlled for tends to infer causal effects that do not exist. This generally makes controlling for a feature the safer option when you are uncertain.\n",
    "\n",
    "\"\"\"\n",
    "#controlling for feature is safer; confounding -> control, downstream conseqeunce -> drop\n",
    "#this is actually good for us to know. Our base data is all downstream which could be a problem (meaning we should also try dropping that info from dataset)\n",
    "#this means we should run it both with and without base data\n",
    "\n",
    "###DoubleMLIRM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Data Loading (TODO - move into one file they can all use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_path = \"../../Dataset Expansion/raw_data\"\n",
    "import os \n",
    "import json \n",
    "\n",
    "REDDITOR_FILE = os.path.join(old_path, \"redditor_dict.json\")\n",
    "SUBMISSION_FILE = os.path.join(old_path, \"submission_dict.json\")\n",
    "COMMENT_FILE = os.path.join(old_path, \"comment_dict.json\")\n",
    "\n",
    "def save_get_json(file_name):\n",
    "    if (os.path.exists(file_name)):\n",
    "        with open(file_name, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return dict()\n",
    "\n",
    "#TODO - comment back in - it's just easier this way\n",
    "\n",
    "redditor_dict = save_get_json(REDDITOR_FILE)\n",
    "submission_dict = save_get_json(SUBMISSION_FILE)\n",
    "comment_dict = save_get_json(COMMENT_FILE)\n",
    "assert(all([(len(i) !=0 for i in [redditor_dict, submission_dict, comment_dict])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# import CONTROL_DATE, FIRST_CONSIDER_DATE from \"../Dataset Expansion/feature_creation_corrected.ipynb\"\n",
    "# from ..DatasetExpansion/feature_creation_corrected import CONTROL_DATE, FIRST_CONSIDER_DATE\n",
    "CONTROL_DATE = datetime(2023,11,22)\n",
    "FIRST_CONSIDER_DATE = datetime(2023, 11-3,22)\n",
    "\n",
    "# CONTROL_DATE = datetime(2023,11,22)\n",
    "\n",
    "treatment_group = ['MoosieGoose', 'JollyK9', 'Southern_Ad3032', 'bduwowy272habbw', 'Late_Introduction203', 'kapster68', 'TheApertureMonkey', 'talemoon22', 'sebagolindenwald', 'spicyranchplzz', 'TheFloorMayBeLava_02', 'rxtten_flesh', 'greenblooded395', 'greenblooded395', 'DrakenJosh98', 'WhichUsernameIsBest', 'FStahp2', 'Pongpianskul', 'Kanashimi515', 'eviuwu', 'Kattheloner_22', 'Reeze2911', 'Sac20000', 'RanpoWasTaken', 'jlynny1811', 'Playful-Fail4778', 'GarageOk8109', 'katandcats', 'holyredemption', 'jifpeanutbutter420', 'Timely_Inflation1000', 'Erica_Peanut']\n",
    "\n",
    "# control_group = ['utroi', 'Evening-Management-7', 'kendragibs', 'throwaway8194122', 'Intelligent-Risk9885', 'jojaques', 'control_burn', 'MetroL7', 'abcdmetalhead', 'AdPsychological9510', 'lola4274', 'iuanaj', 'Wide-Bridge-6461', 'Wide-Bridge-6461', '14Boogie', 'ElthonJzohn_997', 'Am3l1a____', 'Oz_a_day', 'OkWrangler1500', 'prettyp0thead', 'miamihausjunkie', 'ElthonJzohn_997', 'Awkward_Reindeer_788', 'SpendLate5545', 'ace2212', 'Maggiethedogie', 'CommunicationDue6753', 'whale_omelette94', 'IndependentInternet3', 'Yeetenanny', 'EARTHISLIFENOMARS', 'ElizabethWolf1214']\n",
    "control_group = ['utroi', 'xtheboard', 'redman334', 'Evening-Management-7', 'Althe456', 'Super-Breadfruit2894', 'kendragibs', 'wutangis4thedaisy', 'chaoscoateddisaster', 'throwaway8194122', 'PolymerLilac', 'Mysterious_Writer655', 'Intelligent-Risk9885', 'Dull_Check8184', 'Hoommann', 'jojaques', 'jorjacw', 'phravalmom', 'control_burn', 'PeteHealy', 'No-Kaleidoscope9305', 'MetroL7', 'stonks-n-concrete', 'lipglossgirl18', 'abcdmetalhead', 'ConversationUseful', 'ImThatOneStudent', 'AdPsychological9510', 'GibbyTime24', 'cremehoneysky', 'lola4274', 'amoore1501', 'DuckLoveless', 'iuanaj', 'Lisalis9', 'Nanimon22', 'Wide-Bridge-6461', 'achlys19', 'weimaranercollie', 'Wide-Bridge-6461', 'achlys19', 'weimaranercollie', '14Boogie', 'WanderingWindow', 'wigwamtree', 'ElthonJzohn_997', 'CulturalMarsupial89', 'Cosplay_bird168', 'Am3l1a____', 'Ok-Pianist-6844', 'Fun_Sir5658', 'Oz_a_day', 'OMG2Reddit', 'jt2424', 'OkWrangler1500', 'Illustrious-Onion-42', 'Commades', 'prettyp0thead', 'niversescribbles', 'Willofthesouth', 'miamihausjunkie', '8109NZ814', 'sikalpi', 'ElthonJzohn_997', 'Awkward_Reindeer_788', 'CulturalMarsupial89', 'Awkward_Reindeer_788', 'Remote_Membership241', 'ElthonJzohn_997', 'SpendLate5545', 'vampirocitadino', 'Uncle-Thor', 'ace2212', 'SuperNintdoChalmers', 'Timpelgrim', 'Maggiethedogie', 'iuanaj', 'Lisalis9', 'CommunicationDue6753', 'MoekindoSama', 'Temporary_Stay1666', 'whale_omelette94', 'gaypornalt2174', 'anonheartthrob', 'IndependentInternet3', 'Evening-Management-7', 'Mean_Ideal_7504', 'Yeetenanny', 'Electric_Spongebob', 'TangledGoatsucker', 'EARTHISLIFENOMARS', 'Anthony__Alexander', 'RogueRedditerr', 'ElizabethWolf1214', 'weimaranercollie', 'achlys19']\n",
    "# control_group =  ['Theghostofsabotage', 'ThrowRAcottoncandyy', 'sikubis', 'catiegirl74', 'No_Break_4303', 'MyWhatBigEyesIHave', 'JusticeBeevr', 'ElGranTocho', 'Apuksl', 'Freddybear480', 'Charlie2905', 'FuzzySign278', 'bitteroldbird', 'earlylife_crisis_', 'Notaprumber', 'phuckme2', 'Keiichan_', 'TheSeraphman', 'YearOfTheMoose', 'Alishabrooks29', 'Similar-Lab64', '-doves-nest-', 'outlier37', 'Sudden-Manner-7027', 'SIRDumbDumb', 'Maintenanceman368', '_Dreamy-Rose_', 'aesmith1291', 'partial_birth', 'Tight_Ad_4459', 'NothofagusMacrocarpa', 'Stella2662']\n",
    "\"\"\"\n",
    "pca + propensity: ['Adam_718_702', 'Majestic-Fig-524', 'sun_madness', 'jolielu', 'ShamingShoegaze', 'KekMio', 'HollowPomegranate', 'morameat', 'sleepyvibes', 'amdetermined', 'MyWhatBigEyesIHave', 'yandererecon', 'loveForParanormal', 'ieat_tortas', 'GeneralSab', 'Fair_Bowler_4913', 'Key-Philosophy-2877', 'vigilantfox85', 'EisWarren', 'Unclelexx999', 'Dm_me9596', 'fruitsaladqueen', 'No-Application-4971', 'GRRAVEYARDD', 'sixsix6', 'FuzzySign278', 'a1ayy', 'Smooth-Noise-5836', 'ResponsibleEnd2058', 'JusticeBeevr', 'cutebutcrazy91', 'hollyelms']\n",
    "propensity: ['Theghostofsabotage', 'ThrowRAcottoncandyy', 'sikubis', 'catiegirl74', 'No_Break_4303', 'MyWhatBigEyesIHave', 'JusticeBeevr', 'ElGranTocho', 'Apuksl', 'Freddybear480', 'Charlie2905', 'FuzzySign278', 'bitteroldbird', 'earlylife_crisis_', 'Notaprumber', 'phuckme2', 'Keiichan_', 'TheSeraphman', 'YearOfTheMoose', 'Alishabrooks29', 'Similar-Lab64', '-doves-nest-', 'outlier37', 'Sudden-Manner-7027', 'SIRDumbDumb', 'Maintenanceman368', '_Dreamy-Rose_', 'aesmith1291', 'partial_birth', 'Tight_Ad_4459', 'NothofagusMacrocarpa', 'Stella2662']\n",
    "KD: ['utroi', 'Evening-Management-7', 'kendragibs', 'throwaway8194122', 'Intelligent-Risk9885', 'jojaques', 'control_burn', 'MetroL7', 'abcdmetalhead', 'AdPsychological9510', 'lola4274', 'iuanaj', 'Wide-Bridge-6461', 'Wide-Bridge-6461', '14Boogie', 'ElthonJzohn_997', 'Am3l1a____', 'Oz_a_day', 'OkWrangler1500', 'prettyp0thead', 'miamihausjunkie', 'ElthonJzohn_997', 'Awkward_Reindeer_788', 'SpendLate5545', 'ace2212', 'Maggiethedogie', 'CommunicationDue6753', 'whale_omelette94', 'IndependentInternet3', 'Yeetenanny', 'EARTHISLIFENOMARS', 'ElizabethWolf1214']\n",
    "\"\"\"\n",
    "\n",
    "assert(all([c not in treatment_group for c in control_group]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting empath\n",
      "  Using cached empath-0.89-py3-none-any.whl\n",
      "Requirement already satisfied: requests in c:\\python310\\lib\\site-packages (from empath) (2.28.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from requests->empath) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python310\\lib\\site-packages (from requests->empath) (1.24.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\logan\\appdata\\roaming\\python\\python310\\site-packages (from requests->empath) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\logan\\appdata\\roaming\\python\\python310\\site-packages (from requests->empath) (3.3)\n",
      "Installing collected packages: empath\n",
      "Successfully installed empath-0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: You are using pip version 22.0.4; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the 'C:\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install empath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"angry\", \"furious\", \"frustrated\", \"frightened\", \"disgusted\", \"outraged\", \"upset\", \"irritated\", \"resentful\", \"annoyed\", \"embarrassed\", \"unhappy\", \"exasperated\", \"incensed\", \"indignant\", \"fearful\", \"distressed\", \"scared\", \"terrified\", \"enraged\", \"confused\", \"afraid\", \"anxious\", \"irate\", \"insulted\", \"agitated\", \"distraught\", \"nervous\", \"bitter\", \"bewildered\", \"impatient\", \"offended\", \"shocked\", \"disgusted\", \"humiliated\", \"ashamed\", \"apprehensive\", \"puzzled\", \"incredulous\", \"perplexed\", \"tired\", \"worried\", \"disgruntled\", \"mystified\", \"dispirited\", \"despondent\", \"sad\", \"aghast\", \"alarmed\", \"exasperated\", \"mad\", \"terrified\", \"dismayed\", \"aggrieved\", \"irritated\", \"uneasy\", \"apologetic\", \"pained\", \"angered\", \"uncomfortable\", \"annoyed\", \"frightened\", \"bewildered\", \"horrified\", \"dissatisfied\", \"sorry\", \"appalled\", \"disappointed\", \"insecure\", \"defiant\", \"jealous\", \"relieved\", \"shocked\", \"bullied\", \"abusive\", \"intimidated\", \"disturbed\", \"suspicious\", \"complaining\", \"embittered\", \"elated\", \"angrier\", \"appalled\", \"stunned\", \"disillusioned\", \"outraged\", \"disoriented\", \"hysterical\", \"dumbfounded\", \"testy\", \"angered\", \"embarrassing\", \"embarrassed\", \"visibly\"]\n",
      "[\"lonely\", \"solitary\", \"sullen\", \"cranky\", \"homesick\", \"insecure\", \"bored\", \"restless\", \"neurotic\", \"spoiled\", \"terrified\", \"unloved\", \"miserable\", \"morose\", \"disoriented\", \"sad\", \"frightened\", \"bewildered\", \"orphaned\", \"grumpy\", \"frail\", \"jealous\", \"forlorn\", \"irritable\", \"possessive\", \"unbearably\", \"rootless\", \"alienated\", \"rebellious\", \"carefree\", \"introverted\", \"vain\", \"jaded\", \"repressed\", \"sickly\", \"contented\", \"pining\", \"outcast\", \"lovable\", \"loner\", \"adolescent\", \"wistful\", \"perpetually\", \"pampered\", \"surly\", \"adorable\", \"exhausted\", \"smug\", \"pathetic\", \"moody\", \"charming\", \"disheveled\", \"demented\", \"distracted\", \"spinster\", \"recluse\", \"heartbroken\", \"humble\", \"comforting\", \"hysterical\", \"depressed\", \"romantic\", \"stoic\", \"sleepy\", \"housewife\", \"resentful\", \"embittered\", \"pushy\", \"anguished\", \"forgetful\", \"despondent\", \"virgin\", \"withdrawn\", \"depressing\", \"brave\", \"narcissistic\", \"grown\", \"cheerful\", \"selfish\", \"tormented\", \"destitute\", \"breathless\", \"lonesome\", \"bossy\", \"weepy\", \"paranoid\", \"talkative\", \"cute\", \"spiritually\", \"exhilarated\", \"adrift\", \"beautiful\", \"feisty\", \"harried\", \"haggard\", \"unhappily\", \"mischievous\", \"rambunctious\", \"witless\", \"immature\"]\n",
      "[\"sad\", \"heartbreaking\", \"horrible\", \"depressing\", \"strange\", \"terrible\", \"awful\", \"sorry\", \"pathetic\", \"lonely\", \"upsetting\", \"pained\", \"ashamed\", \"miserable\", \"scary\", \"wonderful\", \"tragic\", \"weird\", \"frightening\", \"terrified\", \"shocking\", \"disgusted\", \"heartbroken\", \"pitiful\", \"frightened\", \"disheartening\", \"painful\", \"blessed\", \"ironic\", \"ugly\", \"comforting\", \"regretful\", \"beautiful\", \"sadly\", \"hysterical\", \"speechless\", \"terribly\", \"funny\", \"insecure\", \"appalled\", \"angry\", \"disgusting\", \"horrified\", \"relieved\", \"bewildered\", \"whole_experience\", \"comforting\", \"terrifying\", \"inspiring\", \"jaded\", \"smug\", \"scared\", \"dumbfounded\", \"unbelievable\", \"liberating\", \"fantastic\", \"offended\", \"joyful\", \"unfortunate\", \"unpleasant\", \"saddened\", \"happy\", \"numb\", \"moved\", \"stupid\", \"saddens\", \"unnerving\", \"strange_way\", \"exhilarated\", \"distressing\", \"embarrassing\", \"thankful\", \"gratifying\", \"exhausted\", \"bored\", \"hurtful\", \"horrifying\", \"ecstatic\", \"poignant\", \"perplexed\", \"remorseful\", \"selfish\", \"feel\", \"distressed\", \"shocked\", \"embarrassed\", \"uplifting\", \"humiliating\", \"jealous\", \"brave\", \"boring\", \"unbearable\", \"moving\", \"frustrating\", \"unbearably\", \"resentful\", \"cruel\"]\n",
      "[\"worried\", \"concerned\", \"worried\", \"worry\", \"concerned\", \"apprehensive\", \"nervous\", \"alarmed\", \"anxious\", \"fearful\", \"worrying\", \"afraid\", \"leery\", \"unhappy\", \"upset\", \"worries\", \"scared\", \"pessimistic\", \"uneasy\", \"wary\", \"skittish\", \"sanguine\", \"cautious\", \"skeptical\", \"angry\", \"frightened\", \"optimistic\", \"excited\", \"frustrated\", \"worries\", \"fret\", \"adamant\", \"irritated\", \"fear\", \"hesitant\", \"reassured\", \"worry\", \"second_thoughts\", \"annoyed\", \"distressed\", \"complacent\", \"unsure\", \"ambivalent\", \"convinced\", \"scared\", \"jittery\"]\n",
      "[\"frustrated\", \"angry\", \"frustrated\", \"irritated\", \"unhappy\", \"impatient\", \"annoyed\", \"disgusted\", \"dissatisfied\", \"exasperated\", \"confused\", \"upset\", \"anxious\", \"overwhelmed\", \"embarrassed\", \"apprehensive\", \"nervous\", \"distressed\", \"furious\", \"resentful\", \"frightened\", \"mystified\", \"disappointed\", \"tired\", \"exasperated\", \"alarmed\", \"fearful\", \"frustrating\", \"relieved\", \"despondent\", \"incensed\", \"agitated\", \"perplexed\", \"distracted\", \"worried\", \"dismayed\", \"disgruntled\", \"distraught\", \"bewildered\", \"demoralized\", \"bored\", \"leery\", \"fatigued\", \"scared\", \"intimidated\", \"baffled\", \"preoccupied\", \"testy\", \"complacent\", \"irritated\", \"uncomfortable\", \"dispirited\", \"uneasy\", \"elated\", \"irate\", \"disenchanted\", \"puzzled\", \"outraged\", \"disillusioned\", \"weary\", \"exhausted\", \"wary\", \"disenchanted\", \"stressed\", \"concerned\", \"perturbed\", \"disheartened\", \"disturbed\", \"annoyed\"]\n",
      "[\"anxious\", \"hesitant\", \"eager\", \"worried\", \"apprehensive\", \"nervous\", \"fearful\", \"frustrated\", \"impatient\", \"desperate\", \"frightened\", \"determined\", \"afraid\", \"irritated\", \"angry\", \"unhappy\", \"uneasy\", \"reluctant\", \"wanting\", \"hopeful\", \"happy\", \"concerned\", \"confused\", \"relieved\", \"inclined\", \"preoccupied\", \"wary\", \"confident\", \"resentful\", \"dissatisfied\", \"alarmed\", \"uncomfortable\", \"ambivalent\", \"excited\", \"cautious\", \"scared\", \"annoyed\", \"distressed\", \"upset\", \"embarrassed\", \"skittish\", \"insecure\", \"leery\", \"exasperated\", \"jittery\", \"distracted\", \"disgusted\", \"puzzled\"]\n",
      "[\"nervous\", \"apprehensive\", \"worried\", \"excited\", \"scared\", \"irritated\", \"annoyed\", \"anxious\", \"jittery\", \"uncomfortable\", \"frightened\", \"upset\", \"unnerved\", \"frustrated\", \"uneasy\", \"skittish\", \"confused\", \"embarrassed\", \"angry\", \"cautious\", \"queasy\", \"puzzled\", \"exasperated\", \"relieved\", \"agitated\", \"antsy\", \"hesitant\", \"alarmed\", \"perplexed\", \"euphoric\", \"mystified\", \"panicky\", \"edgy\", \"fatigued\", \"tired\", \"flustered\", \"disgusted\", \"leery\", \"unhappy\", \"impatient\", \"distracted\", \"surprised\", \"terrified\", \"afraid\", \"enthusiastic\", \"concerned\", \"elated\", \"perturbed\", \"bewildered\", \"scared\", \"wary\", \"panicking\", \"complacent\", \"uptight\", \"happy\", \"enthused\", \"insecure\", \"bored\", \"depressed\", \"stressed\", \"cold_feet\"]\n",
      "[\"disappointed\", \"disappointed\", \"pleased\", \"surprised\", \"gratified\", \"delighted\", \"elated\", \"relieved\", \"thrilled\", \"shocked\", \"upset\", \"dismayed\", \"distressed\", \"glad\", \"sorry\", \"embarrassed\", \"unhappy\", \"displeased\", \"disgusted\", \"satisfied\", \"happy\", \"annoyed\", \"excited\", \"flattered\", \"pleased\", \"frustrated\", \"impressed\", \"saddened\", \"puzzled\", \"outraged\", \"incensed\", \"gratified\", \"surprised\", \"stunned\", \"amazed\", \"encouraged\", \"astounded\", \"ecstatic\", \"regret\", \"chagrined\", \"angry\", \"mystified\", \"shocked\", \"flabbergasted\", \"grateful\", \"disappointing\", \"apprehensive\", \"optimistic\", \"dissatisfied\", \"thrilled\", \"miffed\", \"thankful\", \"concerned\"]\n",
      "[\"gloomy\", \"bleak\", \"grim\", \"buoyant\", \"euphoric\", \"rosy\", \"edgy\", \"subdued\", \"dim\", \"upbeat\", \"muted\", \"glum\", \"bleaker\", \"somber\", \"grimmer\", \"dreary\", \"unnerving\", \"cloudy\", \"pessimistic\", \"fatalistic\", \"reassuring\", \"unsettling\", \"ominous\", \"tepid\", \"unfocused\", \"downbeat\", \"jittery\", \"giddy\", \"tame\", \"dour\", \"placid\", \"blase\", \"gloomier\", \"predictable\", \"sanguine\", \"abrupt\", \"overdone\", \"prescient\", \"slack\", \"muddled\", \"unsettled\", \"jarring\", \"chilly\", \"disquieting\", \"dismal\", \"hazy\", \"serene\", \"chilly\", \"cheerful\", \"tranquil\", \"chaotic\", \"dull\", \"unperturbed\", \"untroubled\", \"depressing\", \"robust\", \"restrained\", \"temperate\", \"sobering\", \"depressing\", \"unpredictable\", \"gleeful\", \"stormy\", \"overheated\", \"palpably\", \"tentative\", \"benign\", \"sunny\", \"listless\", \"unconcerned\", \"cheery\", \"fuzzy\", \"decidedly\", \"perplexed\", \"apocalyptic\", \"wistful\", \"reflective\", \"dispiriting\", \"unruffled\", \"dispirited\", \"calmer\", \"comforting\", \"puzzling\", \"disappointingly\", \"smug\", \"prosaic\", \"dire\", \"nostalgic\", \"prophetic\", \"uplifting\", \"jaded\", \"unforgiving\", \"frenetic\", \"directionless\", \"sluggish\", \"depressingly\", \"lifeless\", \"contrived\", \"murky\"]\n",
      "[\"miserable\", \"exhausted\", \"lonely\", \"unbearable\", \"awful\", \"horrible\", \"bearable\", \"hopeless\", \"insecure\", \"terrible\", \"sad\", \"terrified\", \"unpleasant\", \"cranky\", \"depressing\", \"helpless\", \"selfish\", \"lousy\", \"numb\", \"bored\", \"stressed\", \"pathetic\", \"frightened\", \"depressed\", \"dizzy\", \"homesick\", \"humiliating\", \"blessed\", \"disoriented\", \"smug\", \"stressful\", \"fatigued\", \"carefree\", \"tiring\", \"wretched\", \"uptight\", \"irritable\", \"lazy\", \"jaded\", \"comforting\", \"dejected\", \"Emotionally\", \"boring\", \"resentful\", \"sick\", \"pitiful\", \"bewildered\", \"disgusted\", \"dreadful\", \"scary\", \"mad\", \"grumpy\", \"envious\", \"distracted\", \"overwhelmed\", \"bad\", \"hysterical\", \"tired\", \"queasy\", \"horribly\", \"worst_part\", \"upsetting\", \"heartbreaking\", \"sadder\", \"despondent\", \"paranoid\"]\n",
      "[\"lonely\", \"solitary\", \"sullen\", \"cranky\", \"homesick\", \"insecure\", \"bored\", \"restless\", \"neurotic\", \"spoiled\", \"terrified\", \"unloved\", \"miserable\", \"morose\", \"disoriented\", \"sad\", \"frightened\", \"bewildered\", \"orphaned\", \"grumpy\", \"frail\", \"jealous\", \"forlorn\", \"irritable\", \"possessive\", \"unbearably\", \"rootless\", \"alienated\", \"rebellious\", \"carefree\", \"introverted\", \"vain\", \"jaded\", \"repressed\", \"sickly\", \"contented\", \"pining\", \"outcast\", \"lovable\", \"loner\", \"adolescent\", \"wistful\", \"perpetually\", \"pampered\", \"surly\", \"adorable\", \"exhausted\", \"smug\", \"pathetic\", \"moody\", \"charming\", \"disheveled\", \"demented\", \"distracted\", \"spinster\", \"recluse\", \"heartbroken\", \"humble\", \"comforting\", \"hysterical\", \"depressed\", \"romantic\", \"stoic\", \"sleepy\", \"housewife\", \"resentful\", \"embittered\", \"pushy\", \"anguished\", \"forgetful\", \"despondent\", \"virgin\", \"withdrawn\", \"depressing\", \"brave\", \"narcissistic\", \"grown\", \"cheerful\", \"selfish\", \"tormented\", \"destitute\", \"breathless\", \"lonesome\", \"bossy\", \"weepy\", \"paranoid\", \"talkative\", \"cute\", \"spiritually\", \"exhilarated\", \"adrift\", \"beautiful\", \"feisty\", \"harried\", \"haggard\", \"unhappily\", \"mischievous\", \"rambunctious\", \"witless\", \"immature\"]\n",
      "[\"happy\", \"glad\", \"thrilled\", \"delighted\", \"pleased\", \"nice\", \"excited\", \"thankful\", \"like\", \"proud\", \"satisfied\", \"grateful\", \"sorry\", \"love\", \"lucky\", \"relieved\", \"sure\", \"thrilled\", \"want\", \"surprised\", \"disappointed\", \"unhappy\", \"anxious\", \"afraid\", \"good\", \"content\", \"confident\", \"sure\", \"fortunate\", \"elated\", \"comfortable\", \"wish\", \"flattered\", \"scared\", \"overjoyed\", \"know\", \"'m\", \"sad\", \"ashamed\", \"embarrassed\", \"going\", \"think\", \"good_time\", \"nervous\", \"O.K.\", \"guess\"]\n",
      "[\"loved\", \"hated\", \"liked\", \"love\", \"adored\", \"admired\", \"loves\", \"treasured\", \"longed\", \"great_love\", \"loved\", \"cared\", \"blessed\", \"adore\", \"happiest\", \"yearned\", \"remembered\", \"wonderful\", \"loathed\", \"good_friends\", \"treasure\", \"fondly\"]\n",
      "[\"joyful\", \"joyous\", \"solemn\", \"uplifting\", \"wistful\", \"mournful\", \"touching\", \"contemplative\", \"sensual\", \"cathartic\", \"lighthearted\", \"exuberant\", \"merry\", \"endearing\", \"invigorating\", \"celebratory\", \"wondrous\", \"radiant\", \"dreamy\", \"dutiful\", \"exhilarating\", \"affectionate\", \"carefree\", \"soulful\", \"seductive\", \"gloriously\", \"childlike\", \"anguished\", \"unbearably\", \"heartbreaking\", \"melancholy\", \"mischievous\", \"playful\", \"beguiling\", \"redemptive\", \"pensive\", \"introspective\", \"rhapsodic\", \"thrilling\", \"soothing\", \"effervescent\", \"sentimental\", \"morbid\", \"meditative\", \"inspiring\", \"poetic\", \"rueful\", \"thrilling\", \"hypnotic\", \"poignant\", \"exhilarating\", \"mesmerizing\", \"lugubrious\", \"hauntingly\", \"wry\", \"plaintive\", \"mawkish\", \"sardonic\", \"heartwarming\", \"sublime\", \"brooding\", \"otherworldly\", \"dignified\", \"ethereal\", \"unforgettable\", \"sorrowful\", \"maudlin\", \"serene\", \"elegiac\", \"reverent\", \"touchingly\", \"portentous\", \"lyrical\", \"histrionic\", \"jolly\", \"palpably\", \"endearingly\", \"antic\", \"girlish\", \"decorous\", \"purposeful\", \"droll\", \"haunting\", \"blithe\", \"moody\", \"marvelous\", \"rambunctious\", \"liberating\", \"romantic\", \"magical\", \"lyrically\", \"sensuous\", \"enchanting\", \"uninhibited\", \"ponderous\", \"campy\", \"charming\", \"sly\", \"cloying\", \"exalted\"]\n",
      "[\"content\", \"format\", \"subject_matter\", \"electronic_media\", \"interactivity\", \"digital_technology\", \"new_medium\", \"written_word\", \"programming\", \"editorial_content\", \"medium\", \"formulas\", \"lowest_common_denominator\", \"current_events\", \"broadcasters\", \"mass_audience\", \"final_product\", \"sound_quality\", \"function\", \"blogs\", \"mass_media\", \"visuals\", \"disclaimers\", \"other_media\", \"meanings\"]\n",
      "[\"pleased\", \"delighted\", \"disappointed\", \"gratified\", \"disappointed\", \"thrilled\", \"happy\", \"satisfied\", \"surprised\", \"pleased\", \"glad\", \"impressed\", \"relieved\", \"confident\", \"encouraged\", \"gratified\", \"elated\", \"thrilled\", \"proud\", \"grateful\", \"hopeful\", \"excited\", \"dismayed\", \"unhappy\", \"optimistic\", \"surprised\", \"displeased\", \"shocked\", \"heartened\", \"amazed\"]\n",
      "[\"grateful\", \"thankful\", \"appreciative\", \"blessed\", \"fortunate\", \"glad\", \"thrilled\", \"delighted\", \"thank\", \"proud\", \"happy\", \"relieved\", \"gratified\", \"cherish\", \"pleased\", \"sorry\", \"gratitude\", \"overjoyed\", \"thrilled\", \"treasure\", \"comforted\", \"ashamed\", \"disappointed\", \"appreciate\", \"elated\"]\n",
      "[\"relieved\", \"relieved\"]\n",
      "[\"optimistic\", \"pessimistic\", \"hopeful\", \"sanguine\", \"confident\", \"cautious\", \"skeptical\", \"upbeat\", \"encouraging\", \"bullish\", \"concerned\", \"enthusiastic\", \"worried\", \"satisfied\", \"apprehensive\", \"convinced\", \"pleased\", \"enthused\", \"disappointed\", \"unconvinced\"]\n",
      "[\"satisfied\", \"pleased\", \"confident\", \"happy\", \"disappointed\", \"disappointed\", \"dissatisfied\", \"satisfied\", \"convinced\", \"optimistic\", \"pleased\", \"relieved\", \"assured\", \"unhappy\", \"concerned\", \"adamant\", \"reassured\", \"dissatisfied\", \"encouraged\", \"impressed\"]\n",
      "[\"excited\", \"thrilled\", \"excited\", \"nervous\", \"happy\", \"enthusiastic\", \"enthused\", \"exciting\", \"thrilled\", \"glad\", \"ecstatic\", \"disappointed\", \"apprehensive\", \"surprised\", \"delighted\", \"worried\", \"scared\", \"pleased\", \"elated\", \"impressed\", \"upset\", \"antsy\", \"anxious\", \"great_time\", \"flattered\", \"overjoyed\", \"relieved\", \"thinking\", \"curious\", \"thankful\", \"nice\", \"amazed\", \"concerned\", \"gratifying\"]\n",
      "[\"loved\", \"hated\", \"liked\", \"love\", \"adored\", \"admired\", \"loves\", \"treasured\", \"longed\", \"great_love\", \"loved\", \"cared\", \"blessed\", \"adore\", \"happiest\", \"yearned\", \"remembered\", \"wonderful\", \"loathed\", \"good_friends\", \"treasure\", \"fondly\"]\n"
     ]
    }
   ],
   "source": [
    "#define input text model (as will be analyzed) here\n",
    "#define empath\n",
    "\n",
    "from empath import Empath\n",
    "\n",
    "global lexicon\n",
    "\n",
    "lexicon = Empath()\n",
    "\n",
    "#negative\n",
    "lexicon.create_category(\"angry\", [\"angry\"], model=\"nytimes\")\n",
    "lexicon.create_category(\"lonely\", [\"lonely\"], model=\"nytimes\")\n",
    "lexicon.create_category(\"sad\", [\"sad\"], model=\"nytimes\")\n",
    "lexicon.create_category(\"worried\", [\"worried\"], model=\"nytimes\")\n",
    "lexicon.create_category(\"frustrated\", [\"frustrated\"], model=\"nytimes\")\n",
    "lexicon.create_category(\"anxious\", [\"anxious\"], model=\"nytimes\")\n",
    "lexicon.create_category(\"nervous\", [\"nervous\"], model=\"nytimes\")\n",
    "lexicon.create_category(\"disappointed\", [\"disappointed\"], model=\"nytimes\")\n",
    "lexicon.create_category(\"gloomy\", [\"gloomy\"], model=\"nytimes\")\n",
    "lexicon.create_category(\"miserable\", [\"miserable\"], model=\"nytimes\")\n",
    "lexicon.create_category(\"lonely\", [\"lonely\"], model=\"nytimes\")\n",
    "\n",
    "#positive\n",
    "lexicon.create_category(\"happy\", [\"happy\"], model=\"nytimes\")\n",
    "lexicon.create_category(\"loved\", [\"loved\"], model=\"nytimes\")\n",
    "lexicon.create_category(\"joyful\", [\"joyful\"], model=\"nytimes\")\n",
    "lexicon.create_category(\"content\", [\"content\"], model=\"nytimes\")\n",
    "lexicon.create_category(\"pleased\", [\"pleased\"], model=\"nytimes\")\n",
    "lexicon.create_category(\"grateful\", [\"grateful\"], model=\"nytimes\")\n",
    "lexicon.create_category(\"relieved\", [\"relieved\"], model=\"nytimes\")\n",
    "lexicon.create_category(\"optimistic\", [\"optimistic\"], model=\"nytimes\")\n",
    "lexicon.create_category(\"satisfied\", [\"satisfied\"], model=\"nytimes\")\n",
    "lexicon.create_category(\"excited\", [\"excited\"], model=\"nytimes\")\n",
    "lexicon.create_category(\"loved\", [\"loved\"], model=\"nytimes\")\n",
    "\n",
    "\n",
    "def get_data_from_txt(txt:str):\n",
    "    global lexicon\n",
    "\n",
    "    if (not len(txt)):\n",
    "        return {k: 0 for k in[\"angry\",\"sad\",\"worried\",\n",
    "       \"frustrated\",\"anxious\",\n",
    "        \"nervous\",\"disappointed\",\n",
    "      \"gloomy\",\"miserable\",\"longly\",\n",
    "       \"happy\", \"loved\", \"joyful\",\n",
    "        \"content\", \"pleased\",\n",
    "      \"grateful\", \"relieved\",\n",
    "      \"optimistic\", \"satisfied\",\n",
    "        \"excited\"]}\n",
    "\n",
    "    unnormalized = lexicon.analyze(txt, categories=[\"angry\",\"sad\",\"worried\",\n",
    "       \"frustrated\",\"anxious\",\n",
    "        \"nervous\",\"disappointed\",\n",
    "      \"gloomy\",\"miserable\",\"longly\",\n",
    "       \"happy\", \"loved\", \"joyful\",\n",
    "        \"content\", \"pleased\",\n",
    "      \"grateful\", \"relieved\",\n",
    "      \"optimistic\", \"satisfied\",\n",
    "        \"excited\"],normalize = False)\n",
    "  \n",
    "    normalized = lexicon.analyze(txt, categories=[\"angry\",\"sad\",\"worried\",\n",
    "       \"frustrated\",\"anxious\",\n",
    "        \"nervous\",\"disappointed\",\n",
    "      \"gloomy\",\"miserable\",\"longly\",\n",
    "       \"happy\", \"loved\", \"joyful\",\n",
    "        \"content\", \"pleased\",\n",
    "      \"grateful\", \"relieved\",\n",
    "      \"optimistic\", \"satisfied\",\n",
    "        \"excited\"],normalize = True)\n",
    "    \n",
    "\n",
    "    #we want to make these average to 0 \n",
    "    summed = 0\n",
    "    for v in normalized.values(): summed += v \n",
    "    to_add = -summed/len(normalized)\n",
    "\n",
    "    for k in normalized.keys():\n",
    "        normalized[k] += to_add\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "global txt_keys\n",
    "txt_keys = [\"angry\",\"sad\",\"worried\",\n",
    "       \"frustrated\",\"anxious\",\n",
    "        \"nervous\",\"disappointed\",\n",
    "      \"gloomy\",\"miserable\",\"longly\",\n",
    "       \"happy\", \"loved\", \"joyful\",\n",
    "        \"content\", \"pleased\",\n",
    "      \"grateful\", \"relieved\",\n",
    "      \"optimistic\", \"satisfied\",\n",
    "        \"excited\"]\n",
    "\n",
    "global submission_keys_order\n",
    "submission_keys_order = [\n",
    "        # \"distinguished\" ,\n",
    "        \"is_original_content\" ,\n",
    "        \"over_18\" ,\n",
    "        \"score\" ,\n",
    "        \"title\",\n",
    "        \"upvote_ratio\" ]\n",
    "\n",
    "global comment_key_order\n",
    "comment_key_order =  [\n",
    "      \"is_edited\",\n",
    "      \"num_replies\", \n",
    "      \"score\",\n",
    "      \"score_is_hidden\",\n",
    "      \"total_awards\",\n",
    "      \"num_ups\",\n",
    "      \"num_downs\",\n",
    "      \"body\",\n",
    "      \"is_submitter\", \n",
    "      \"stickied\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "# import binary_search\n",
    "\n",
    "class Redditor:\n",
    "    \"\"\"this is basically a class that created a data array and column label array so we can use that for KD Tree and/or Propensity Score\"\"\"\n",
    "\n",
    "    comment_keys = []\n",
    "    sub_data_keys = []\n",
    "\n",
    "    def __init__(self, redditor_dict, comment_dict, submission_dict, redditor_name, start_date, control_date):\n",
    "        \"\"\"im lazy rn so first one passed in must have at least some comments\"\"\"\n",
    "        global txt_keys\n",
    "        global submission_keys_order\n",
    "        global comment_key_order\n",
    "\n",
    "        self.all_data = None\n",
    "        self.all_data_keys = None\n",
    "        #yield stats related to this\n",
    "        redditor =  redditor_dict[redditor_name] \n",
    "        base_redditor_data = redditor[\"data\"]\n",
    "        if (not base_redditor_data[\"has_subreddit\"]):\n",
    "            #set values to -1\n",
    "            base_redditor_data[\"over_18\"] = -1#redditor.subreddit[\"over_18\"]\n",
    "            base_redditor_data[\"num_subscribers\"] = -1#redditor.subreddit[\"subscribers\"]\n",
    "\n",
    "            #this is the only one added that rlly shouldn't be\n",
    "            base_redditor_data[\"public_description\"] = \"\"\n",
    "\n",
    "        \n",
    "        description_data = get_data_from_txt(base_redditor_data[\"public_description\"])\n",
    "\n",
    "        _keys = list(base_redditor_data.keys())\n",
    "        _keys.sort()\n",
    "\n",
    "        base_data = list()\n",
    "        base_data_keys = list()\n",
    "        for k in _keys:\n",
    "            v = base_redditor_data[k]\n",
    "            if (isinstance(v, type(\"\"))): continue\n",
    "            base_data.append(v)\n",
    "            base_data_keys.append(k)\n",
    "\n",
    "        base_data += [description_data[i] for i in txt_keys] #= [v for k,v in base_redditor_data.items() if not isinstance(v, type(\"\"))] +\n",
    "        base_data_keys += [f'public_description_{i}' for i in txt_keys]\n",
    "\n",
    "        #get sequential data for comments\n",
    "        sequential_redditor_data = redditor[\"comments\"]\n",
    "\n",
    "        \n",
    "        #sequential data -> 2 sorts\n",
    "        k_segs = 2\n",
    "        segments = [start_date + (1+i)*(control_date-start_date)/k_segs for i in range(k_segs)]\n",
    "        cnts = [0 for _ in range(k_segs)]\n",
    "        suicide_cnts = [0 for _ in range(k_segs)]\n",
    "        segment_data = [np.zeros((53,),dtype='float') for _ in range(k_segs)]\n",
    "        comment_keys = [] #None, just setting each time even tho inefficient\n",
    "        sub_data_keys = []\n",
    "        current_seg_idx = 0 \n",
    "\n",
    "        try:\n",
    "            sequential_redditor_data.sort()\n",
    "        except:\n",
    "            print(\"sequential data is \", sequential_redditor_data) #okay they have the exact same time which is why there are problems bc its not letting me sort bc dict not comparable\n",
    "            return #TODO - I actually need to correct this one - this doesn't enforce types we wanted to \n",
    "        \n",
    "        #we need to make sure we have data covering for all this time NOTE must add this\n",
    "        is_valid = sequential_redditor_data[-1][0] < start_date if len(sequential_redditor_data) else True\n",
    "\n",
    "        for comment_date, comment_data in sequential_redditor_data:\n",
    "\n",
    "            #can do binary search for efficiency in future - TODO\n",
    "            if (comment_date > control_date):\n",
    "                break \n",
    "            if (comment_date < start_date): continue \n",
    "\n",
    "            sub_id = comment_data[\"submission_id\"] \n",
    "            comment_key = sub_id + \"--------\" + comment_data[\"comment_id\"]\n",
    "            try:\n",
    "                comment_data_dict = comment_dict[comment_key][\"data\"]\n",
    "            except:\n",
    "                return\n",
    "\n",
    "            #getting comment data \n",
    "            comment_body_data = get_data_from_txt(comment_data_dict[\"body\"])\n",
    "            comment_data = [comment_data_dict[k] for k in comment_key_order]\n",
    "\n",
    "            #make comment keys if they don't alr exist\n",
    "            if (not len(Redditor.comment_keys)): Redditor.comment_keys = [comment_key_order[i] for i in range(len(comment_key_order)) if not isinstance(comment_data[i], type(\"\"))] + [f'comment_body_{t}' for t in txt_keys]\n",
    "            \n",
    "            #concat numeric data and txt data\n",
    "            comment_data = [v for v in comment_data if not isinstance(v, type(\"\"))] + [comment_body_data[i] for i in txt_keys]\n",
    "\n",
    "            #getting submission data \n",
    "            try:\n",
    "                sub_data_dict = submission_dict[sub_id]\n",
    "            except:\n",
    "                return\n",
    "            sub_data = [sub_data_dict[k] for k in submission_keys_order]\n",
    "\n",
    "            #TODO - make lookup to avoid repeated computation\n",
    "            sub_title_data = get_data_from_txt(sub_data_dict[\"title\"])\n",
    "\n",
    "            if (not len(Redditor.sub_data_keys)): Redditor.sub_data_keys = [submission_keys_order[i] for i in range(len(submission_keys_order)) if not isinstance(sub_data[i], type(\"\"))] + [f'submission_title_{t}' for t in txt_keys]\n",
    "            sub_data = [v for v in sub_data if not isinstance(v, type(\"\"))] + [sub_title_data[i] for i in txt_keys]  \n",
    "            #we have a valid comment date \n",
    "\n",
    "\n",
    "            #aggregate\n",
    "            all_time_info = np.array(comment_data + sub_data, dtype='float')\n",
    "\n",
    "            while (segments[current_seg_idx] < comment_date):\n",
    "                current_seg_idx += 1\n",
    "            \n",
    "            cnts[current_seg_idx] += 1\n",
    "\n",
    "            segment_data[current_seg_idx] += all_time_info\n",
    "\n",
    "            if (any(kw in sub_data_dict[\"title\"] for kw in [\"suicide\", \"depress\"]) or any(kw in comment_data_dict[\"body\"] for kw in [\"suicide\", \"depress\"])):\n",
    "                suicide_cnts[current_seg_idx] += 1\n",
    "\n",
    "\n",
    "        #make my segment keys \n",
    "        segment_keys = [\"comment_\"+ i for i in Redditor.comment_keys] + [\"submission_\" + i for i in Redditor.sub_data_keys ]\n",
    "\n",
    "        all_data = base_data*1\n",
    "        all_data_keys = base_data_keys * 1\n",
    "\n",
    "        #check lengths match up\n",
    "        assert len(all_data) == len(all_data_keys), \"(0) all_data is of len \" + str(len(all_data)) + \" all data keys is of len \" + str(len(all_data_keys))\n",
    "        assert len(segment_keys) == len(segment_data[0].tolist()), \"segment keys is len \" + str(len(segment_keys)) + \" while segment_data is len \" +  str(len(segment_data[0].tolist()))\n",
    "        \n",
    "        \n",
    "        #add segment data in \n",
    "        segment_data = [segment_data[i]/max(1,cnts[i]) for i in range(k_segs)]\n",
    "        for i in range(k_segs):\n",
    "            all_data += segment_data[i].tolist()\n",
    "            all_data_keys += [f'seg_{i}_{key_name}' for key_name in segment_keys]\n",
    "\n",
    "        for i in range(1): \n",
    "            all_data.append(is_valid)\n",
    "            all_data_keys.append(\"is_valid\")\n",
    "        for i in range(1): \n",
    "            all_data += cnts\n",
    "            all_data_keys += [f'num_comments_in_seg_{i}' for i in range(len(cnts))]\n",
    "        for i in range(1):\n",
    "            all_data += suicide_cnts\n",
    "            all_data_keys += [f'num_suicide_mentions_{i}' for i in range(len(suicide_cnts))]\n",
    "\n",
    "        assert len(all_data) == len(all_data_keys), \"all_data is of len \" + str(len(all_data)) + \" all data keys is of len \" + str(len(all_data_keys))\n",
    "        self.all_data_keys = all_data_keys\n",
    "        self.all_data = np.array(all_data, dtype='float')\n",
    "            #add it to the correct one \n",
    "\n",
    "\n",
    "        #okay time to get sequential data\n",
    "        \n",
    "\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.all_data\n",
    "r = Redditor(redditor_dict, comment_dict, submission_dict, treatment_group[0], int(time.mktime(FIRST_CONSIDER_DATE.timetuple())), int(time.mktime(CONTROL_DATE.timetuple())) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(145,)\n",
      "145\n"
     ]
    }
   ],
   "source": [
    "print(r.all_data.shape)\n",
    "print(len(r.all_data_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "control = [Redditor(redditor_dict, comment_dict, submission_dict, c, int(time.mktime(FIRST_CONSIDER_DATE.timetuple())), int(time.mktime(CONTROL_DATE.timetuple())) ) for c in control_group]\n",
    "treat = [Redditor(redditor_dict, comment_dict, submission_dict, t, int(time.mktime(FIRST_CONSIDER_DATE.timetuple())), int(time.mktime(CONTROL_DATE.timetuple())) )for t in treatment_group]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.000000001\n",
    "control_data = np.array([c.all_data for c in control])\n",
    "treatment_data = np.array([t.all_data for t in treat])\n",
    "\n",
    "c_std = np.std(control_data, axis =0)\n",
    "t_std = np.std(treatment_data, axis=0)\n",
    "\n",
    "treat_mean = np.mean(treatment_data, axis=0) \n",
    "control_mean= np.mean(control_data, axis=0)\n",
    "\n",
    "control_data = (control_data - treat_mean)/(epsilon + t_std)\n",
    "treatment_data = (treatment_data - treat_mean)/(epsilon + t_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 145)\n",
      "(128,)\n"
     ]
    }
   ],
   "source": [
    "#for now, we're doing a NN for perplexity score, will turn into actual later\n",
    "treatments = np.array([0 for i in range(len(control))] + [1 for i in range(len(treat))])\n",
    "# ys = np.array([randint(1,20) for i in range(len(control) + len(treat))])\n",
    "ys = treatments\n",
    "Xs = np.concatenate((control_data, treatment_data))\n",
    "print(Xs.shape)\n",
    "print(ys.shape)\n",
    "assert(Xs.shape[0] == ys.shape[0])\n",
    "assert(treatments.shape[0] == ys.shape[0])\n",
    "assert False, \"ys shold not be treatment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting econml\n",
      "  Downloading econml-0.14.1-cp310-cp310-win_amd64.whl (929 kB)\n",
      "     -------------------------------------- 929.6/929.6 KB 2.9 MB/s eta 0:00:00\n",
      "Collecting sparse\n",
      "  Downloading sparse-0.14.0-py2.py3-none-any.whl (80 kB)\n",
      "     ---------------------------------------- 81.0/81.0 KB ? eta 0:00:00\n",
      "Collecting lightgbm\n",
      "  Using cached lightgbm-4.1.0-py3-none-win_amd64.whl (1.3 MB)\n",
      "Requirement already satisfied: numpy in c:\\python310\\lib\\site-packages (from econml) (1.22.3)\n",
      "Requirement already satisfied: scikit-learn<1.3,>0.22.0 in c:\\python310\\lib\\site-packages (from econml) (1.1.1)\n",
      "Requirement already satisfied: pandas in c:\\python310\\lib\\site-packages (from econml) (1.4.2)\n",
      "Requirement already satisfied: joblib>=0.13.0 in c:\\python310\\lib\\site-packages (from econml) (1.1.0)\n",
      "Requirement already satisfied: scipy>1.4.0 in c:\\python310\\lib\\site-packages (from econml) (1.8.0)\n",
      "Collecting shap<0.42.0,>=0.38.1\n",
      "  Downloading shap-0.41.0-cp310-cp310-win_amd64.whl (435 kB)\n",
      "     -------------------------------------- 435.6/435.6 KB 3.0 MB/s eta 0:00:00\n",
      "Collecting statsmodels>=0.10\n",
      "  Using cached statsmodels-0.14.1-cp310-cp310-win_amd64.whl (9.8 MB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python310\\lib\\site-packages (from scikit-learn<1.3,>0.22.0->econml) (3.1.0)\n",
      "Requirement already satisfied: slicer==0.0.7 in c:\\python310\\lib\\site-packages (from shap<0.42.0,>=0.38.1->econml) (0.0.7)\n",
      "Requirement already satisfied: tqdm>4.25.0 in c:\\python310\\lib\\site-packages (from shap<0.42.0,>=0.38.1->econml) (4.64.1)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\users\\logan\\appdata\\roaming\\python\\python310\\site-packages (from shap<0.42.0,>=0.38.1->econml) (21.3)\n",
      "Collecting cloudpickle\n",
      "  Using cached cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: numba in c:\\python310\\lib\\site-packages (from shap<0.42.0,>=0.38.1->econml) (0.55.2)\n",
      "Collecting patsy>=0.5.4\n",
      "  Using cached patsy-0.5.4-py2.py3-none-any.whl (233 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python310\\lib\\site-packages (from pandas->econml) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\python310\\lib\\site-packages (from pandas->econml) (2.8.2)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in c:\\python310\\lib\\site-packages (from numba->shap<0.42.0,>=0.38.1->econml) (0.38.1)\n",
      "Requirement already satisfied: setuptools in c:\\python310\\lib\\site-packages (from numba->shap<0.42.0,>=0.38.1->econml) (57.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\python310\\lib\\site-packages (from packaging>20.9->shap<0.42.0,>=0.38.1->econml) (3.0.7)\n",
      "Requirement already satisfied: six in c:\\python310\\lib\\site-packages (from patsy>=0.5.4->statsmodels>=0.10->econml) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\python310\\lib\\site-packages (from tqdm>4.25.0->shap<0.42.0,>=0.38.1->econml) (0.4.4)\n",
      "Installing collected packages: patsy, cloudpickle, sparse, lightgbm, statsmodels, shap, econml\n",
      "Successfully installed cloudpickle-3.0.0 econml-0.14.1 lightgbm-4.1.0 patsy-0.5.4 shap-0.41.0 sparse-0.14.0 statsmodels-0.14.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: You are using pip version 22.0.4; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the 'C:\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install econml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data One (Double ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Using cached xgboost-2.0.2-py3-none-win_amd64.whl (99.8 MB)\n",
      "Requirement already satisfied: scipy in c:\\python310\\lib\\site-packages (from xgboost) (1.8.0)\n",
      "Requirement already satisfied: numpy in c:\\python310\\lib\\site-packages (from xgboost) (1.22.3)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: You are using pip version 22.0.4; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the 'C:\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://shap.readthedocs.io/en/latest/example_notebooks/overviews/Be%20careful%20when%20interpreting%20predictive%20models%20in%20search%20of%20causal%C2%A0insights.html\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from econml.dml import LinearDML\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "import xgboost\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Run Double ML, controlling for all the other features\n",
    "def double_ml(y, causal_feature, control_features):\n",
    "    \"\"\"Use doubleML from econML to estimate the slope of the causal effect of a feature.\"\"\"\n",
    "\n",
    "    est = LinearDML(model_y=LinearRegression())\n",
    "    est.fit(y, causal_feature, W=control_features)\n",
    "    return est.effect_inference()\n",
    "\n",
    "\n",
    "effect = double_ml(ys, treatments, Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>point_estimate</th>\n",
       "      <th>stderr</th>\n",
       "      <th>zstat</th>\n",
       "      <th>pvalue</th>\n",
       "      <th>ci_lower</th>\n",
       "      <th>ci_upper</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.047</td>\n",
       "      <td>0.034</td>\n",
       "      <td>30.783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   point_estimate  stderr   zstat  pvalue  ci_lower  ci_upper\n",
       "X                                                            \n",
       "0           1.047   0.034  30.783     0.0      0.98     1.114"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effect.summary_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting DoubleML\n",
      "  Using cached DoubleML-0.7.0-py3-none-any.whl (234 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\python310\\lib\\site-packages (from DoubleML) (1.1.1)\n",
      "Requirement already satisfied: statsmodels in c:\\python310\\lib\\site-packages (from DoubleML) (0.14.1)\n",
      "Requirement already satisfied: plotly in c:\\python310\\lib\\site-packages (from DoubleML) (5.18.0)\n",
      "Requirement already satisfied: pandas in c:\\python310\\lib\\site-packages (from DoubleML) (1.4.2)\n",
      "Requirement already satisfied: scipy in c:\\python310\\lib\\site-packages (from DoubleML) (1.8.0)\n",
      "Requirement already satisfied: numpy in c:\\python310\\lib\\site-packages (from DoubleML) (1.22.3)\n",
      "Requirement already satisfied: joblib in c:\\python310\\lib\\site-packages (from DoubleML) (1.1.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python310\\lib\\site-packages (from pandas->DoubleML) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\python310\\lib\\site-packages (from pandas->DoubleML) (2.8.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\python310\\lib\\site-packages (from plotly->DoubleML) (8.2.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\logan\\appdata\\roaming\\python\\python310\\site-packages (from plotly->DoubleML) (21.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python310\\lib\\site-packages (from scikit-learn->DoubleML) (3.1.0)\n",
      "Requirement already satisfied: patsy>=0.5.4 in c:\\python310\\lib\\site-packages (from statsmodels->DoubleML) (0.5.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\python310\\lib\\site-packages (from packaging->plotly->DoubleML) (3.0.7)\n",
      "Requirement already satisfied: six in c:\\python310\\lib\\site-packages (from patsy>=0.5.4->statsmodels->DoubleML) (1.16.0)\n",
      "Installing collected packages: DoubleML\n",
      "Successfully installed DoubleML-0.7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: You are using pip version 22.0.4; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the 'C:\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install DoubleML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Z1        Z2        Z3        Z4           y    d\n",
      "0    0.280397 -0.161563  0.374582  0.868365  234.141188  1.0\n",
      "1   -0.436909 -0.307469 -0.416819  0.256420  191.432100  0.0\n",
      "2   -0.617402  0.556987  0.250681 -0.061405  203.014252  0.0\n",
      "3    0.000140 -1.630138 -0.468617 -1.706889  159.419151  1.0\n",
      "4   -0.961297  0.364768  1.119084 -0.846581  192.933615  0.0\n",
      "..        ...       ...       ...       ...         ...  ...\n",
      "495 -0.938382 -1.926729  0.725102 -0.464327  160.438105  1.0\n",
      "496  0.881096  0.201463  0.827761  0.609844  257.372425  0.0\n",
      "497 -0.736528  0.462325 -0.572178  0.723444  200.440252  1.0\n",
      "498  1.249469  0.243495  0.490437  1.159711  270.203470  0.0\n",
      "499 -0.259027 -0.895659  0.002256 -1.184481  173.738908  1.0\n",
      "\n",
      "[500 rows x 6 columns]\n",
      "================== DoubleMLData Object ==================\n",
      "\n",
      "------------------ Data summary      ------------------\n",
      "Outcome variable: y\n",
      "Treatment variable(s): ['d']\n",
      "Covariates: ['Z1', 'Z2', 'Z3', 'Z4']\n",
      "Instrument variable(s): None\n",
      "No. Observations: 500\n",
      "\n",
      "------------------ DataFrame info    ------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Columns: 6 entries, Z1 to d\n",
      "dtypes: float64(6)\n",
      "memory usage: 23.6 KB\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>std err</th>\n",
       "      <th>t</th>\n",
       "      <th>P&gt;|t|</th>\n",
       "      <th>2.5 %</th>\n",
       "      <th>97.5 %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>-3.116501</td>\n",
       "      <td>2.029539</td>\n",
       "      <td>-1.535571</td>\n",
       "      <td>0.124644</td>\n",
       "      <td>-7.094325</td>\n",
       "      <td>0.861322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       coef   std err         t     P>|t|     2.5 %    97.5 %\n",
       "d -3.116501  2.029539 -1.535571  0.124644 -7.094325  0.861322"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import doubleml as dml\n",
    "from doubleml.datasets import make_did_SZ2020\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "np.random.seed(42)\n",
    "ml_g = RandomForestRegressor(n_estimators=100, max_depth=5, min_samples_leaf=5)\n",
    "ml_m = RandomForestClassifier(n_estimators=100, max_depth=5, min_samples_leaf=5)\n",
    "data = make_did_SZ2020(n_obs=500, return_type='DataFrame')\n",
    "print(data)\n",
    "obj_dml_data = dml.DoubleMLData(data, 'y', 'd')\n",
    "print(obj_dml_data)\n",
    "dml_did_obj = dml.DoubleMLDID(obj_dml_data, ml_g, ml_m)\n",
    "dml_did_obj.fit().summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "================== DoubleMLDID Object ==================\n",
      "\n",
      "------------------ Data summary      ------------------\n",
      "Outcome variable: the_outcome\n",
      "Treatment variable(s): ['is_treatment']\n",
      "Covariates: ['comment_karma', 'has_subreddit', 'is_employee', 'is_gold', 'is_mod', 'is_suspended', 'link_karma', 'num_moderated', 'num_multireddits', 'num_subscribers', 'num_trophies', 'over_18', 'time_creation', 'verified_email', 'public_description_angry', 'public_description_sad', 'public_description_worried', 'public_description_frustrated', 'public_description_anxious', 'public_description_nervous', 'public_description_disappointed', 'public_description_gloomy', 'public_description_miserable', 'public_description_longly', 'public_description_happy', 'public_description_loved', 'public_description_joyful', 'public_description_content', 'public_description_pleased', 'public_description_grateful', 'public_description_relieved', 'public_description_optimistic', 'public_description_satisfied', 'public_description_excited', 'seg_0_comment_is_edited', 'seg_0_comment_num_replies', 'seg_0_comment_score', 'seg_0_comment_score_is_hidden', 'seg_0_comment_total_awards', 'seg_0_comment_num_ups', 'seg_0_comment_num_downs', 'seg_0_comment_is_submitter', 'seg_0_comment_stickied', 'seg_0_comment_comment_body_angry', 'seg_0_comment_comment_body_sad', 'seg_0_comment_comment_body_worried', 'seg_0_comment_comment_body_frustrated', 'seg_0_comment_comment_body_anxious', 'seg_0_comment_comment_body_nervous', 'seg_0_comment_comment_body_disappointed', 'seg_0_comment_comment_body_gloomy', 'seg_0_comment_comment_body_miserable', 'seg_0_comment_comment_body_longly', 'seg_0_comment_comment_body_happy', 'seg_0_comment_comment_body_loved', 'seg_0_comment_comment_body_joyful', 'seg_0_comment_comment_body_content', 'seg_0_comment_comment_body_pleased', 'seg_0_comment_comment_body_grateful', 'seg_0_comment_comment_body_relieved', 'seg_0_comment_comment_body_optimistic', 'seg_0_comment_comment_body_satisfied', 'seg_0_comment_comment_body_excited', 'seg_0_submission_is_original_content', 'seg_0_submission_over_18', 'seg_0_submission_score', 'seg_0_submission_upvote_ratio', 'seg_0_submission_submission_title_angry', 'seg_0_submission_submission_title_sad', 'seg_0_submission_submission_title_worried', 'seg_0_submission_submission_title_frustrated', 'seg_0_submission_submission_title_anxious', 'seg_0_submission_submission_title_nervous', 'seg_0_submission_submission_title_disappointed', 'seg_0_submission_submission_title_gloomy', 'seg_0_submission_submission_title_miserable', 'seg_0_submission_submission_title_longly', 'seg_0_submission_submission_title_happy', 'seg_0_submission_submission_title_loved', 'seg_0_submission_submission_title_joyful', 'seg_0_submission_submission_title_content', 'seg_0_submission_submission_title_pleased', 'seg_0_submission_submission_title_grateful', 'seg_0_submission_submission_title_relieved', 'seg_0_submission_submission_title_optimistic', 'seg_0_submission_submission_title_satisfied', 'seg_0_submission_submission_title_excited', 'seg_1_comment_is_edited', 'seg_1_comment_num_replies', 'seg_1_comment_score', 'seg_1_comment_score_is_hidden', 'seg_1_comment_total_awards', 'seg_1_comment_num_ups', 'seg_1_comment_num_downs', 'seg_1_comment_is_submitter', 'seg_1_comment_stickied', 'seg_1_comment_comment_body_angry', 'seg_1_comment_comment_body_sad', 'seg_1_comment_comment_body_worried', 'seg_1_comment_comment_body_frustrated', 'seg_1_comment_comment_body_anxious', 'seg_1_comment_comment_body_nervous', 'seg_1_comment_comment_body_disappointed', 'seg_1_comment_comment_body_gloomy', 'seg_1_comment_comment_body_miserable', 'seg_1_comment_comment_body_longly', 'seg_1_comment_comment_body_happy', 'seg_1_comment_comment_body_loved', 'seg_1_comment_comment_body_joyful', 'seg_1_comment_comment_body_content', 'seg_1_comment_comment_body_pleased', 'seg_1_comment_comment_body_grateful', 'seg_1_comment_comment_body_relieved', 'seg_1_comment_comment_body_optimistic', 'seg_1_comment_comment_body_satisfied', 'seg_1_comment_comment_body_excited', 'seg_1_submission_is_original_content', 'seg_1_submission_over_18', 'seg_1_submission_score', 'seg_1_submission_upvote_ratio', 'seg_1_submission_submission_title_angry', 'seg_1_submission_submission_title_sad', 'seg_1_submission_submission_title_worried', 'seg_1_submission_submission_title_frustrated', 'seg_1_submission_submission_title_anxious', 'seg_1_submission_submission_title_nervous', 'seg_1_submission_submission_title_disappointed', 'seg_1_submission_submission_title_gloomy', 'seg_1_submission_submission_title_miserable', 'seg_1_submission_submission_title_longly', 'seg_1_submission_submission_title_happy', 'seg_1_submission_submission_title_loved', 'seg_1_submission_submission_title_joyful', 'seg_1_submission_submission_title_content', 'seg_1_submission_submission_title_pleased', 'seg_1_submission_submission_title_grateful', 'seg_1_submission_submission_title_relieved', 'seg_1_submission_submission_title_optimistic', 'seg_1_submission_submission_title_satisfied', 'seg_1_submission_submission_title_excited', 'is_valid', 'num_comments_in_seg_0', 'num_comments_in_seg_1', 'num_suicide_mentions_0', 'num_suicide_mentions_1']\n",
      "Instrument variable(s): None\n",
      "No. Observations: 128\n",
      "\n",
      "------------------ Score & algorithm ------------------\n",
      "Score function: observational\n",
      "DML algorithm: dml2\n",
      "\n",
      "------------------ Machine learner   ------------------\n",
      "Learner ml_g: LinearRegression()\n",
      "Learner ml_m: RandomForestClassifier()\n",
      "Out-of-sample Performance:\n",
      "Learner ml_g0 RMSE: [[0.]]\n",
      "Learner ml_g1 RMSE: [[10.99682311]]\n",
      "Learner ml_m RMSE: [[0.48038233]]\n",
      "\n",
      "------------------ Resampling        ------------------\n",
      "No. folds: 5\n",
      "No. repeated sample splits: 1\n",
      "Apply cross-fitting: True\n",
      "\n",
      "------------------ Fit summary       ------------------\n",
      "                 coef   std err         t         P>|t|     2.5 %    97.5 %\n",
      "is_treatment -0.59375  0.086821 -6.838803  7.985744e-12 -0.763916 -0.423584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Propensity predictions from learner RandomForestClassifier() for ml_m are close to zero or one (eps=1e-12).\n"
     ]
    }
   ],
   "source": [
    "_keys = list(r.all_data_keys)\n",
    "import collections\n",
    "print([item for item, count in collections.Counter(_keys).items() if count > 1])\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(Xs)\n",
    "df.columns = _keys\n",
    "df[\"is_treatment\"] = treatments \n",
    "df[\"the_outcome\"] = [randint(0,1) if i else 1 for i in treatments] \n",
    "new_data = dml.DoubleMLData(df, 'the_outcome', 'is_treatment')\n",
    "df.head()\n",
    "dml_did_obj = dml.DoubleMLDID(new_data, LinearRegression(),RandomForestClassifier())\n",
    "\n",
    "\n",
    "#https://docs.doubleml.org/stable/examples/py_double_ml_did.html\n",
    "\n",
    "# https://econml.azurewebsites.net/_autosummary/econml.dml.NonParamDML.html#econml.dml.NonParamDML\n",
    "\n",
    "# to try\n",
    "\"\"\" \n",
    "DoubleMLIRM\n",
    "DoubleMLIIVM \n",
    "DoubleMLDID\n",
    "\"\"\"\n",
    "\n",
    "print(dml_did_obj.fit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'causalml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\logan\\OneDrive\\Documents\\Research\\Mental_Health\\Reddit_Data_Scraping\\Reddit-Crawler-Data\\DatasetPrediction\\SHAP\\shap_model.ipynb Cell 16\u001b[0m line \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/logan/OneDrive/Documents/Research/Mental_Health/Reddit_Data_Scraping/Reddit-Crawler-Data/DatasetPrediction/SHAP/shap_model.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcausalml\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/logan/OneDrive/Documents/Research/Mental_Health/Reddit_Data_Scraping/Reddit-Crawler-Data/DatasetPrediction/SHAP/shap_model.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcausalml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minference\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmeta\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseSRegressor, BaseTRegressor, BaseXRegressor, BaseRRegressor\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/logan/OneDrive/Documents/Research/Mental_Health/Reddit_Data_Scraping/Reddit-Crawler-Data/DatasetPrediction/SHAP/shap_model.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcausalml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataset\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mregression\u001b[39;00m \u001b[39mimport\u001b[39;00m synthetic_data\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'causalml'"
     ]
    }
   ],
   "source": [
    "import causalml\n",
    "from causalml.inference.meta import BaseSRegressor, BaseTRegressor, BaseXRegressor, BaseRRegressor\n",
    "from causalml.dataset.regression import synthetic_data\n",
    "\n",
    "# Load synthetic data\n",
    "y, X, treatment, tau, b, e = synthetic_data(mode=1, n=10000, p=25, sigma=0.5)\n",
    "w_multi = np.array(['treatment_A' if x==1 else 'control' for x in treatment]) # customize treatment/control names\n",
    "\n",
    "slearner = BaseSRegressor(LGBMRegressor(), control_name='control')\n",
    "slearner.estimate_ate(X, w_multi, y)\n",
    "slearner_tau = slearner.fit_predict(X, w_multi, y)\n",
    "\n",
    "model_tau_feature = RandomForestRegressor()  # specify model for model_tau_feature\n",
    "\n",
    "slearner.get_importance(X=X, tau=slearner_tau, model_tau_feature=model_tau_feature,\n",
    "                        normalize=True, method='auto', features=feature_names)\n",
    "\n",
    "# Using the feature_importances_ method in the base learner (LGBMRegressor() in this example)\n",
    "slearner.plot_importance(X=X, tau=slearner_tau, normalize=True, method='auto')\n",
    "\n",
    "# Using eli5's PermutationImportance\n",
    "slearner.plot_importance(X=X, tau=slearner_tau, normalize=True, method='permutation')\n",
    "\n",
    "# Using SHAP\n",
    "shap_slearner = slearner.get_shap_values(X=X, tau=slearner_tau)\n",
    "\n",
    "# Plot shap values without specifying shap_dict\n",
    "slearner.plot_shap_values(X=X, tau=slearner_tau)\n",
    "\n",
    "# Plot shap values WITH specifying shap_dict\n",
    "slearner.plot_shap_values(X=X, shap_dict=shap_slearner)\n",
    "\n",
    "# interaction_idx set to 'auto' (searches for feature with greatest approximate interaction)\n",
    "slearner.plot_shap_dependence(treatment_group='treatment_A',\n",
    "                              feature_idx=1,\n",
    "                              X=X,\n",
    "                              tau=slearner_tau,\n",
    "                              interaction_idx='auto')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
