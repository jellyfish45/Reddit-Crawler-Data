{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "973bcada-79c8-4c78-a373-9a3039d722e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import praw  #Python Reddit API Wrappe\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cde2cc4-878f-4dbf-b0c5-f0cce1a8c32f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=\"54u8-WeUDusPlUoFQDZu4w\",\n",
    "    client_secret=\"lGhkiE6scCkSMYsj0P9AXzyYVSLBag\",\n",
    "    user_agent=\"jellyfish\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd03ee5f-f225-42c9-a7e6-968ef4867d26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Suicide= reddit.subreddit(\"SuicideWatch\")\n",
    "awfuleverything= reddit. subreddit(\"awfuleverything\")\n",
    "lonely= reddit.subreddit(\"lonely\")\n",
    "depression= reddit.subreddit(\"depression\")\n",
    "therewasanattempt= reddit.subreddit(\"therewasanattempt\")\n",
    "depression_memes= reddit.subreddit(\"depression_memes\")\n",
    "dankmemes= reddit.subreddit(\"dankmemes\")\n",
    "fakedisordercringe=reddit.subreddit(\"fakedisordercringe\")\n",
    "all=[Suicide,awfuleverything,lonely,depression,therewasanattempt,dankmemes,fakedisordercringe]\n",
    "posts = []\n",
    "for subreddit in all:\n",
    "    for post in subreddit.search(\"suicide\", limit=1000): \n",
    "        posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created,post.comments,'suicide'])\n",
    "    for post in subreddit.search(\"depression\", limit=1000): \n",
    "        posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created,'depression'])\n",
    "    for post in subreddit.search(\"mental disorder\", limit=1000): \n",
    "        posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created,'mental_disorder'])\n",
    "    for post in subreddit.search(\"despair\", limit=1000): \n",
    "        posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created,'despair'])\n",
    "    for post in subreddit.search(\"suicidal\", limit=1000): \n",
    "        posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created,'suicidal'])\n",
    "    for post in subreddit.search(\"anxiety\", limit=1000): \n",
    "        posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created,'anxiety'])\n",
    "    for post in subreddit.search(\"trauma\", limit=1000): \n",
    "        posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created,'trauma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcf9e43f-0f2f-4946-8e96-889c9171d6a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'drop_duplicates'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#posts = pd.DataFrame(posts,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body','created','key_words'])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m posts \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m posts\u001b[38;5;241m=\u001b[39mposts\u001b[38;5;241m.\u001b[39mdrop_duplicates()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'drop_duplicates'"
     ]
    }
   ],
   "source": [
    "posts = pd.DataFrame(posts,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body','created','key_words'])\n",
    "posts=posts.drop_duplicates()\n",
    "#posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd90ce12-e40f-4430-909a-74c0faf3ddd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#posts=posts.to_csv('suicide.csv')\n",
    "posts=pd.read_csv('suicide.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e7011d80-9822-4f4d-ba88-7646b9b5875c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SuicideWatch             1556\n",
       "depression               1517\n",
       "lonely                   1486\n",
       "dankmemes                1222\n",
       "fakedisordercringe        998\n",
       "awfuleverything           802\n",
       "therewasanattempt         512\n",
       "u_RedditCareResources      12\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit=posts['subreddit'].value_counts().sort_values(ascending=False)\n",
    "subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e375f3f9-2dcf-4223-9258-96414ad72cd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "sw_nltk = stopwords.words('english')\n",
    "\n",
    "text_data = posts['body'].str.cat(sep=' ')\n",
    "words = re.findall(r'\\w+', text_data.lower())\n",
    "words = [word for word in words if word not in sw_nltk]\n",
    "tags = pos_tag(words)\n",
    "nouns = [word for word, tag in tags if tag in ['NN', 'NNS', 'NNP', 'NNPS']]\n",
    "verbs = [word for word, tag in tags if tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']]\n",
    "adjectives = [word for word, tag in tags if tag in ['JJ', 'JJR', 'JJS']]\n",
    "\n",
    "\n",
    "text_data2 = posts['title'].str.cat(sep=' ')\n",
    "words2 = re.findall(r'\\w+', text_data2.lower())\n",
    "words2 = [word for word in words2 if word not in sw_nltk]\n",
    "tags2 = pos_tag(words2)\n",
    "nouns2 = [word for word, tag in tags2 if tag in ['NN', 'NNS', 'NNP', 'NNPS']]\n",
    "verbs2 = [word for word, tag in tags2 if tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']]\n",
    "adjectives2 = [word for word, tag in tags2 if tag in ['JJ', 'JJR', 'JJS']]\n",
    "\n",
    "\n",
    "\n",
    "noun_count = collections.Counter(nouns)\n",
    "noun_count2 = collections.Counter(nouns2)\n",
    "noun_count.update(noun_count2)\n",
    "verb_count = collections.Counter(verbs)\n",
    "verb_count2 = collections.Counter(verbs2)\n",
    "verb_count.update(verb_count2)\n",
    "adj_count = collections.Counter(adjectives)\n",
    "adj_count2 = collections.Counter(adjectives2)\n",
    "adj_count.update(adj_count2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6513866f-ee71-4461-a8e5-1e3dced3386d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people: 4742\n",
      "life: 4197\n",
      "time: 3101\n",
      "anxiety: 2596\n",
      "depression: 2588\n",
      "feel: 2252\n",
      "years: 2053\n",
      "someone: 1923\n",
      "things: 1918\n",
      "friends: 1883\n",
      "day: 1826\n",
      "way: 1576\n",
      "anyone: 1470\n",
      "something: 1456\n",
      "anything: 1364\n",
      "person: 1326\n",
      "everything: 1318\n",
      "help: 1286\n",
      "family: 1276\n",
      "thoughts: 1230\n",
      "everyone: 1176\n",
      "year: 1156\n",
      "school: 1151\n",
      "disorder: 1144\n",
      "nothing: 1138\n",
      "talk: 1110\n",
      "work: 1099\n",
      "thing: 1053\n",
      "suicide: 1043\n",
      "health: 957\n",
      "trauma: 947\n",
      "job: 860\n",
      "lot: 851\n",
      "friend: 832\n",
      "parents: 828\n",
      "point: 792\n",
      "despair: 786\n",
      "get: 755\n",
      "months: 726\n",
      "disorders: 723\n",
      "days: 714\n",
      "world: 711\n",
      "times: 686\n",
      "end: 672\n",
      "im: 662\n",
      "others: 633\n",
      "feels: 630\n",
      "care: 628\n",
      "pain: 613\n",
      "relationship: 599\n",
      "post: 590\n",
      "reason: 571\n",
      "mom: 562\n",
      "home: 555\n",
      "issues: 551\n",
      "suicidal: 533\n",
      "kind: 532\n",
      "today: 522\n",
      "week: 505\n",
      "mind: 504\n",
      "night: 503\n",
      "part: 481\n",
      "hate: 468\n",
      "self: 446\n",
      "support: 424\n",
      "death: 408\n",
      "try: 405\n",
      "college: 402\n",
      "feeling: 401\n",
      "feelings: 401\n",
      "place: 400\n",
      "head: 387\n",
      "house: 386\n",
      "love: 376\n",
      "problems: 375\n",
      "brain: 373\n",
      "die: 372\n",
      "fact: 371\n",
      "please: 367\n",
      "man: 365\n",
      "age: 362\n",
      "therapy: 358\n",
      "money: 353\n",
      "dont: 352\n",
      "problem: 340\n",
      "experience: 336\n",
      "attention: 333\n",
      "body: 328\n",
      "think: 327\n",
      "hope: 323\n",
      "group: 323\n",
      "nobody: 322\n",
      "illness: 321\n",
      "weeks: 318\n",
      "situation: 318\n",
      "room: 316\n",
      "see: 315\n",
      "tell: 314\n",
      "hours: 312\n",
      "sense: 309\n",
      "childhood: 306\n",
      "mother: 303\n",
      "dad: 303\n",
      "kids: 296\n",
      "matter: 296\n",
      "need: 294\n",
      "https: 292\n",
      "advice: 292\n",
      "heart: 289\n",
      "idea: 288\n",
      "system: 287\n",
      "month: 285\n",
      "diagnosis: 284\n",
      "shit: 282\n",
      "stuff: 279\n",
      "bit: 279\n",
      "kill: 274\n",
      "personality: 273\n",
      "moment: 270\n",
      "state: 270\n",
      "autism: 270\n",
      "reddit: 267\n",
      "story: 266\n",
      "share: 265\n",
      "guess: 264\n",
      "guys: 262\n",
      "girl: 262\n",
      "symptoms: 262\n",
      "look: 259\n",
      "deal: 259\n",
      "hell: 258\n",
      "cause: 257\n",
      "change: 255\n",
      "let: 255\n",
      "relationships: 254\n",
      "alters: 252\n",
      "thanks: 251\n",
      "medication: 250\n",
      "com: 248\n",
      "therapist: 248\n",
      "words: 246\n",
      "www: 243\n",
      "face: 236\n",
      "hospital: 235\n",
      "guy: 233\n",
      "ones: 226\n",
      "partner: 226\n",
      "loneliness: 225\n",
      "phone: 222\n",
      "gonna: 220\n",
      "emotions: 219\n",
      "call: 218\n",
      "stop: 218\n",
      "abuse: 218\n",
      "meds: 215\n",
      "r: 214\n",
      "car: 207\n",
      "course: 207\n",
      "living: 203\n",
      "cry: 198\n",
      "child: 197\n",
      "woman: 197\n",
      "energy: 195\n",
      "doctor: 194\n",
      "women: 194\n",
      "morning: 192\n",
      "start: 192\n",
      "commit: 192\n",
      "comments: 191\n",
      "know: 190\n",
      "none: 188\n",
      "treatment: 185\n",
      "attempt: 185\n",
      "sorry: 184\n",
      "ways: 182\n",
      "wanna: 182\n",
      "conversation: 180\n",
      "understand: 179\n",
      "fear: 179\n",
      "media: 179\n",
      "cannot: 178\n",
      "doctors: 174\n",
      "god: 173\n",
      "chance: 171\n",
      "kid: 171\n",
      "fakers: 170\n",
      "yesterday: 169\n",
      "sex: 168\n",
      "community: 167\n",
      "reasons: 166\n",
      "father: 165\n",
      "reality: 165\n",
      "edit: 163\n",
      "fuck: 163\n",
      "idk: 163\n",
      "want: 162\n",
      "children: 161\n",
      "thank: 160\n",
      "girls: 160\n",
      "move: 160\n"
     ]
    }
   ],
   "source": [
    "most_common_nouns = noun_count.most_common(200)\n",
    "for word, count in most_common_nouns:\n",
    "    print(f'{word}: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cbc330-854e-4fe3-b908-b3ebc933e41a",
   "metadata": {
    "tags": []
   },
   "source": [
    "most_common_verbs =verb_count.most_common(200)\n",
    "for word, count in most_common_verbs:\n",
    "    print(f'{word}: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e6f6b0-a3de-47ce-851e-5094ed936775",
   "metadata": {
    "tags": []
   },
   "source": [
    "most_common_adjs =adj_count.most_common(200)\n",
    "for word, count in most_common_adjs:\n",
    "    print(f'{word}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1e89d94-72cd-43f8-8bdd-76ebfbb43e70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#1.black\n",
    "Result_black = []\n",
    "keywords = ['black people','African', 'Race', 'Racial', 'Racism', 'Minority', 'Slavery', 'Afro', 'Segregation', 'Afrodescendiente', 'Negro']\n",
    "for i in range(posts.shape[0]):\n",
    "    title = posts.iloc[i]['title']\n",
    "    body = posts.iloc[i]['body']\n",
    "    if (not pd.isna(title) and any(keyword in title for keyword in keywords)) or (not pd.isna(body) and any(keyword in body for keyword in keywords)):\n",
    "        Result_black.append(posts.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ab48f9c-fc9c-41b9-9836-e7da40dd9182",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Result_black = pd.DataFrame(Result_black,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body','created','key_words'])\n",
    "Result_black=Result_black.drop_duplicates()\n",
    "#Result_black.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c42626f-c3d3-45dd-8f5d-c2135b762d92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Result_black=Result_black.to_csv('Result_black.csv')\n",
    "Result_black=pd.read_csv('Result_black.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "325ec96c-a1c1-48a5-bc64-0d95543beed4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "depression            28\n",
       "SuicideWatch          21\n",
       "lonely                10\n",
       "fakedisordercringe     7\n",
       "awfuleverything        5\n",
       "therewasanattempt      2\n",
       "dankmemes              1\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit=Result_black['subreddit'].value_counts().sort_values(ascending=False)\n",
    "subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8489c05b-87d7-4aa9-b56f-3cb031b4563b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#2.teenager\n",
    "Result_teen = []\n",
    "keywords = ['adolescence', 'youth', 'young', 'puberty', 'peer pressure', 'teen', 'adolescent', 'teenage', 'rebellion']\n",
    "for i in range(posts.shape[0]):\n",
    "    title = posts.iloc[i]['title']\n",
    "    body = posts.iloc[i]['body']\n",
    "    if (not pd.isna(title) and any(keyword in title for keyword in keywords)) or (not pd.isna(body) and any(keyword in body for keyword in keywords)):\n",
    "        Result_teen.append(posts.iloc[i])\n",
    "Result_teen = pd.DataFrame(Result_teen,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body','created','key_words'])\n",
    "Result_teen=Result_teen.drop_duplicates()\n",
    "#Result_teen\n",
    "Result_teen=Result_teen.to_csv('Result_teen.csv')\n",
    "Result_teen=pd.read_csv('Result_teen.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "26b8344f-bc9b-4bf8-98b2-81c58433d7d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SuicideWatch          139\n",
       "lonely                129\n",
       "depression            118\n",
       "fakedisordercringe     91\n",
       "awfuleverything        27\n",
       "dankmemes               3\n",
       "therewasanattempt       1\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit=Result_teen['subreddit'].value_counts().sort_values(ascending=False)\n",
    "subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9ec635ab-9eb4-4c13-9224-6b56cc36c7e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#3.parents\n",
    "Result_parent = []\n",
    "keywords = ['mother', 'father', 'parent', 'family', 'children', 'childcare', 'guardians', 'upbringing', 'nurturing', 'mom','dad']\n",
    "for i in range(posts.shape[0]):\n",
    "    title = posts.iloc[i]['title']\n",
    "    body = posts.iloc[i]['body']\n",
    "    if (not pd.isna(title) and any(keyword in title for keyword in keywords)) or (not pd.isna(body) and any(keyword in body for keyword in keywords)):\n",
    "        Result_parent.append(posts.iloc[i])\n",
    "Result_parent = pd.DataFrame(Result_parent,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body','created','key_words'])\n",
    "Result_parent=Result_parent.drop_duplicates()\n",
    "#Result_parent\n",
    "Result_parent=Result_parent.to_csv('Result_parent.csv')\n",
    "Result_parent=pd.read_csv('Result_parent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "23e878b1-6b33-4519-9e44-f69fae75a531",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SuicideWatch          629\n",
       "depression            547\n",
       "lonely                472\n",
       "fakedisordercringe    138\n",
       "awfuleverything       105\n",
       "dankmemes              16\n",
       "therewasanattempt      10\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit=Result_parent['subreddit'].value_counts().sort_values(ascending=False)\n",
    "subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "05903422-7106-4005-9c63-c80769ef3374",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#4.date\n",
    "Result_date = []\n",
    "keywords = ['romance', 'relationship', 'love', 'partner', 'courtship', 'date', 'affection', 'attraction', 'compatibility', 'intimacy', 'commitment', 'connection', 'chemistry', 'dating', 'romantic', 'matchmaking', 'flirting', 'compatibility', 'single']\n",
    "for i in range(posts.shape[0]):\n",
    "    title = posts.iloc[i]['title']\n",
    "    body = posts.iloc[i]['body']\n",
    "    if (not pd.isna(title) and any(keyword in title for keyword in keywords)) or (not pd.isna(body) and any(keyword in body for keyword in keywords)):\n",
    "        Result_date.append(posts.iloc[i])\n",
    "Result_date = pd.DataFrame(Result_date,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body','created','key_words'])\n",
    "Result_date=Result_date.drop_duplicates()\n",
    "#Result_date\n",
    "Result_date=Result_date.to_csv('Result_date.csv')\n",
    "Result_date=pd.read_csv('Result_date.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "07c5d9c8-bccf-49c3-9e40-74a6a9b7cf3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#5.college\n",
    "Result_college = []\n",
    "keywords = ['education', 'university', 'campus', 'student', 'professor', 'degree', 'course', 'study', 'academics', 'tuition', 'scholarship', 'exams', 'classroom', 'lecture',  'library', 'extracurricular', 'admissions', 'graduation', 'dormitory', 'alumni']\n",
    "for i in range(posts.shape[0]):\n",
    "    title = posts.iloc[i]['title']\n",
    "    body = posts.iloc[i]['body']\n",
    "    if (not pd.isna(title) and any(keyword in title for keyword in keywords)) or (not pd.isna(body) and any(keyword in body for keyword in keywords)):\n",
    "        Result_college.append(posts.iloc[i])\n",
    "Result_college = pd.DataFrame(Result_college,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body','created','key_words'])\n",
    "Result_college=Result_college.drop_duplicates()\n",
    "#Result_college\n",
    "Result_college=Result_college.to_csv('Result_college.csv')\n",
    "Result_college=pd.read_csv('Result_college.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e7cc84-9f20-4484-897e-5caa0d6af08d",
   "metadata": {},
   "source": [
    "1.influence(beautiful soup,trace)\n",
    "2.sum and analyze more detailed\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
