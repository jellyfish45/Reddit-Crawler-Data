{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86b1e655-7039-442e-adb5-4bc8b7aeda89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import praw\n",
    "from datetime import date\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a38ccbfa-c341-4068-ad9a-40be96bf051c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Result=pd.read_csv('mental_illness.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43ae0359-1dd8-494d-a8c3-e49aeda6dfc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=\"54u8-WeUDusPlUoFQDZu4w\",\n",
    "    client_secret=\"lGhkiE6scCkSMYsj0P9AXzyYVSLBag\",\n",
    "    user_agent=\"jellyfish\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5a281c4-ed56-45e6-82f3-33c56ea0743c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "science               104\n",
       "bipolar                86\n",
       "memes                  78\n",
       "AskReddit              58\n",
       "OCD                    53\n",
       "worldnews              52\n",
       "Antipsychiatry         40\n",
       "funny                  40\n",
       "shitposting            38\n",
       "WhitePeopleTwitter     32\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10_values = Result['subreddit'].value_counts().sort_values(ascending=False).head(10)\n",
    "top_10_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c2f2c90-91e0-46bf-885d-9723186febe2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Crawler(object):\n",
    "    '''\n",
    "        basic_url is the reddit site.\n",
    "        headers is for requests.get method\n",
    "        REX is to find submission ids.\n",
    "    '''\n",
    "    def __init__(self, subreddit=\"apple\"):\n",
    "        '''\n",
    "            Initialize a Crawler object.\n",
    "                subreddit is the topic you want to parse. default is r\"apple\"\n",
    "            basic_url is the reddit site.\n",
    "            headers is for requests.get method\n",
    "            REX is to find submission ids.\n",
    "            submission_ids save all the ids of submission you will parse.\n",
    "            reddit is an object created using praw API. Please check it before you use.\n",
    "        '''\n",
    "        self.basic_url = \"https://www.reddit.com\"\n",
    "        #self.headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36'} \n",
    "        self.headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.4 Safari/605.1.15'}\n",
    "        self.REX = re.compile(r\"<div class=\\\" thing id-t3_[\\w]+\")\n",
    "        self.subreddit = subreddit\n",
    "        self.submission_ids = []\n",
    "        self.reddit = praw.Reddit(client_id=\"54u8-WeUDusPlUoFQDZu4w\", client_secret=\"lGhkiE6scCkSMYsj0P9AXzyYVSLBag\", user_agent=\"jellyfish\")\n",
    "\n",
    "    def get_submission_ids(self, pages=2):\n",
    "        '''\n",
    "            Collect all ids of submissions..\n",
    "            One page has 25 submissions.\n",
    "            page url: https://www.reddit.com/r/subreddit/?count25&after=t3_id\n",
    "                id(after) is the last submission from last page.\n",
    "        '''\n",
    "#         This is page url.\n",
    "        url = self.basic_url + \"/r/\" + self.subreddit\n",
    "\n",
    "        if pages <= 0:\n",
    "            return []\n",
    "\n",
    "        text = requests.get(url, headers=self.headers).text\n",
    "        ids = self.REX.findall(text)\n",
    "        ids = list(map(lambda x: x[-6:], ids))\n",
    "        if pages == 1:\n",
    "            self.submission_ids = ids\n",
    "            return ids\n",
    "\n",
    "        count = 0\n",
    "        for i in range(1, pages):\n",
    "            count += 25\n",
    "            temp_url = self.basic_url + \"/r/\" + self.subreddit + \"?count=\" + str(count) + \"&after=t3_\" + ids[-1]\n",
    "            text = requests.get(temp_url, headers=self.headers).text\n",
    "            temp_list = self.REX.findall(text)\n",
    "            temp_list = list(map(lambda x: x[-6:], temp_list))\n",
    "            ids += temp_list\n",
    "            if count % 100 == 0:\n",
    "                time.sleep(60)\n",
    "        self.submission_ids = ids\n",
    "        return ids\n",
    "\n",
    "    def get_comments(self, submission):\n",
    "        '''\n",
    "            Submission is an object created using praw API.\n",
    "        '''\n",
    "#         Remove all \"more comments\".\n",
    "        submission.comments.replace_more(limit=None)\n",
    "        comments = []\n",
    "        for each in submission.comments.list():\n",
    "            try:\n",
    "                comments.append((each.id, each.link_id[3:], each.author.name, date.fromtimestamp(each.created_utc).isoformat(), each.score, each.body) )\n",
    "            except AttributeError as e: # Some comments are deleted, we cannot access them.\n",
    "#                 print(each.link_id, e)\n",
    "                continue\n",
    "        return comments\n",
    "\n",
    "    def save_comments_submissions(self, pages):\n",
    "        '''\n",
    "            1. Save all the ids of submissions.\n",
    "            2. For each submission, save information of this submission. (submission_id, #comments, score, subreddit, date, title, body_text)\n",
    "            3. Save comments in this submission. (comment_id, submission_id, author, date, score, body_text)\n",
    "            4. Separately, save them to two csv file.\n",
    "\n",
    "            Note: You can link them with submission_id.\n",
    "            Warning: According to the rule of Reddit API, the get action should not be too frequent. Safely, use the defalut time span in this crawler.\n",
    "        '''\n",
    "\n",
    "        print(\"Start to collect all submission ids...\")\n",
    "        self.get_submission_ids(pages)\n",
    "        print(\"Start to collect comments...This may cost a long time depending on # of pages.\")\n",
    "        submission_url = self.basic_url + \"/r/\" + self.subreddit + \"/comments/\"\n",
    "        comments = []\n",
    "        submissions = []\n",
    "        count = 0\n",
    "        for idx in self.submission_ids:\n",
    "            temp_url = submission_url + idx\n",
    "            submission = self.reddit.submission(url=temp_url)\n",
    "            submissions.append((submission.name[3:], submission.num_comments, submission.score, submission.subreddit_name_prefixed, date.fromtimestamp(submission.created_utc).isoformat(), submission.title, submission.selftext))\n",
    "            temp_comments = self.get_comments(submission)\n",
    "            comments += temp_comments\n",
    "            count += 1\n",
    "            print(str(count) + \" submissions have got...\")\n",
    "            if count % 50 == 0:\n",
    "                time.sleep(60)\n",
    "        comments_fieldnames = [\"comment_id\", \"submission_id\", \"author_name\", \"post_time\", \"comment_score\", \"text\"]\n",
    "        df_comments = pd.DataFrame(comments, columns=comments_fieldnames)\n",
    "        df_comments.to_csv(\"comments.csv\")\n",
    "        submissions_fieldnames = [\"submission_id\", \"num_of_comments\", \"submission_score\", \"submission_subreddit\", \"post_date\", \"submission_title\", \"text\"]\n",
    "        df_submission = pd.DataFrame(submissions, columns=submissions_fieldnames)\n",
    "        df_submission.to_csv(\"submissions.csv\")\n",
    "        return df_comments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86a0970a-2299-4a34-8b3b-4025fe106404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to collect all submission ids...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m subreddit, pages \u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscience\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m      2\u001b[0m c \u001b[38;5;241m=\u001b[39m Crawler(subreddit)\n\u001b[0;32m----> 3\u001b[0m c\u001b[38;5;241m.\u001b[39msave_comments_submissions(pages)\n",
      "Cell \u001b[0;32mIn[6], line 85\u001b[0m, in \u001b[0;36mCrawler.save_comments_submissions\u001b[0;34m(self, pages)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m    1. Save all the ids of submissions.\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03m    2. For each submission, save information of this submission. (submission_id, #comments, score, subreddit, date, title, body_text)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m    Warning: According to the rule of Reddit API, the get action should not be too frequent. Safely, use the defalut time span in this crawler.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart to collect all submission ids...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_submission_ids(pages)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart to collect comments...This may cost a long time depending on # of pages.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     87\u001b[0m submission_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasic_url \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/r/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubreddit \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/comments/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[6], line 48\u001b[0m, in \u001b[0;36mCrawler.get_submission_ids\u001b[0;34m(self, pages)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, pages):\n\u001b[1;32m     47\u001b[0m     count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m25\u001b[39m\n\u001b[0;32m---> 48\u001b[0m     temp_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasic_url \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/r/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubreddit \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?count=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(count) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m&after=t3_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m ids[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     49\u001b[0m     text \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(temp_url, headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders)\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m     50\u001b[0m     temp_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREX\u001b[38;5;241m.\u001b[39mfindall(text)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "subreddit, pages =[\"science\",2]\n",
    "c = Crawler(subreddit)\n",
    "c.save_comments_submissions(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997ebb7c-2f79-4973-82b2-7c6635f4d268",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
